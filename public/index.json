[{"authors":["admin"],"categories":null,"content":"My background is in cognitive psychology, and for my dissertation research, I examined how writing helps (or doesn\u0026rsquo;t help) people learn new information. I have worked in higher education since 2011, and in 2019, I became certified by RStudio to teach the tidyverse. I am available to train groups and individuals to learn R and the tidyverse, either in-person or virtually.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"My background is in cognitive psychology, and for my dissertation research, I examined how writing helps (or doesn\u0026rsquo;t help) people learn new information. I have worked in higher education since 2011, and in 2019, I became certified by RStudio to teach the tidyverse. I am available to train groups and individuals to learn R and the tidyverse, either in-person or virtually.","tags":null,"title":"Arie Spirgel","type":"authors"},{"authors":["arie"],"categories":["education"],"content":" If you’ve downloaded enough data from the IPEDS Data Center using the “Compare Institutions” interface, you’ve probably realized that, depending on what you’re downloading, the data provided is rarely in a format ready for analysis. Here, via a specific example, I describe what makes the IPEDS data format impractical, and how to use R to resolve that.\nReading in the Data I first downloaded Fall 2012 to Fall 2018 distance education headcounts for every college and university in the IPEDS Data Center. In this first section, I read in the data, and display a subset of what the full data set looks like.\nlibrary(tidyverse) library(scales) theme_set(theme_light()) distance \u0026lt;- read_csv(\u0026quot;raw-data/distance-fall-12-18.csv\u0026quot;) The data set contains 6,800 rows and 43 columns, and ignoring the Institution Name column, each of the remaining columns is some version of the following: Students enrolled exclusively in distance education courses (EF2018A_DIST Undergraduate total). Under that specific column, for each of the 6,800 institutions that reported data, are headcounts for exclusively distance undergraduate students in the fall term of 2018. The problem, thus, is that this column (and all the other ones like it) actually contains three pieces of information:\nLevel, which can take on the values undergraduate or graduate. Modality, which can take on the values exlusively distance, some distance, or no distance. Year, which can take on any integer value from 2012 to 2018.  This untidy format is exactly what makes IPEDS data tricky to work with. In contrast, tidy data - which means each variable is in its own column, each observation is in its own row, and each value is in its own cell1 - is advantageous not just for working with data in R, but other software as well (e.g., pivot tables in Excel).\n Tidying the Data The first step to tidying this data is to pivot it so that all of the column names that contain the type of headcount are in one column, and the actual headcounts are in a different column. To do that, I use the gather()2 function. I first provide gather() with the names of the two new variables that are being created - I call them variable and headcount, but they can be called anything you want - and then which columns I want pivoted from wide to long; here, I pivot everything from the 2nd column to the last column of the data set.\ndistance \u0026lt;- distance %\u0026gt;% gather(variable, headcount, 2:ncol(.)) distance ## # A tibble: 285,600 x 3 ## `Institution Name` variable headcount ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Educational Technical College… Students enrolled exclusively in di… NA ## 2 A T Still University of Healt… Students enrolled exclusively in di… NA ## 3 Aaniiih Nakoda College Students enrolled exclusively in di… NA ## 4 ABC Adult School Students enrolled exclusively in di… NA ## 5 ABC Beauty Academy Students enrolled exclusively in di… NA ## 6 ABCO Technology Students enrolled exclusively in di… NA ## 7 Abcott Institute Students enrolled exclusively in di… NA ## 8 Abdill Career College Inc Students enrolled exclusively in di… NA ## 9 Abilene Christian University Students enrolled exclusively in di… 32 ## 10 Abraham Baldwin Agricultural … Students enrolled exclusively in di… 377 ## # … with 285,590 more rows As you can see above, the data set now only has three columns, not 43. Same data, different layout. Looks better already, right?!?\nWe’re not done though. Remember, each row of the variable column contains three pieces of information: level, modality, and year. So for the next three steps I split that column apart so each of these variables are in their own column. First, I’ll make a new column for level.\nThere are countless ways of reaching the same endpoint in R, and in this instance, I use str_detect() to tell R to put “Undergraduate” in the level column if it detects the string “Undergraduate” in the variable column, and then perform the analogous task for “Graduate”.\ndistance \u0026lt;- distance %\u0026gt;% mutate(level = case_when( str_detect(variable, \u0026quot;Undergraduate\u0026quot;) ~ \u0026quot;Undergraduate\u0026quot;, str_detect(variable, \u0026quot;Graduate\u0026quot;) ~ \u0026quot;Graduate\u0026quot;)) distance ## # A tibble: 285,600 x 4 ## `Institution Name` variable headcount level ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Educational Technical Coll… Students enrolled exclusively … NA Underg… ## 2 A T Still University of He… Students enrolled exclusively … NA Underg… ## 3 Aaniiih Nakoda College Students enrolled exclusively … NA Underg… ## 4 ABC Adult School Students enrolled exclusively … NA Underg… ## 5 ABC Beauty Academy Students enrolled exclusively … NA Underg… ## 6 ABCO Technology Students enrolled exclusively … NA Underg… ## 7 Abcott Institute Students enrolled exclusively … NA Underg… ## 8 Abdill Career College Inc Students enrolled exclusively … NA Underg… ## 9 Abilene Christian Universi… Students enrolled exclusively … 32 Underg… ## 10 Abraham Baldwin Agricultur… Students enrolled exclusively … 377 Underg… ## # … with 285,590 more rows See the new column on the end with level?\nNext I do the same thing for modality: I tell R to look for specific strings, and make a new column based on those strings.\ndistance \u0026lt;- distance %\u0026gt;% mutate(modality = case_when( str_detect(variable, \u0026quot;not enrolled in any\u0026quot;) ~ \u0026quot;No Distance\u0026quot;, str_detect(variable, \u0026quot;in some\u0026quot;) ~ \u0026quot;Some Distance\u0026quot;, str_detect(variable, \u0026quot;exclusively\u0026quot;) ~ \u0026quot;Exclusively Distance\u0026quot;)) distance ## # A tibble: 285,600 x 5 ## `Institution Name` variable headcount level modality ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Educational Technical C… Students enrolled exclu… NA Under… Exclusive… ## 2 A T Still University of… Students enrolled exclu… NA Under… Exclusive… ## 3 Aaniiih Nakoda College Students enrolled exclu… NA Under… Exclusive… ## 4 ABC Adult School Students enrolled exclu… NA Under… Exclusive… ## 5 ABC Beauty Academy Students enrolled exclu… NA Under… Exclusive… ## 6 ABCO Technology Students enrolled exclu… NA Under… Exclusive… ## 7 Abcott Institute Students enrolled exclu… NA Under… Exclusive… ## 8 Abdill Career College I… Students enrolled exclu… NA Under… Exclusive… ## 9 Abilene Christian Unive… Students enrolled exclu… 32 Under… Exclusive… ## 10 Abraham Baldwin Agricul… Students enrolled exclu… 377 Under… Exclusive… ## # … with 285,590 more rows The last step of tidying is to get year in its own column. I could tell R to make a new variable and put “2012” if it detects “2012”, “2013” if it detects “2013”, and so-on, but there is a much simpler way: the parse_number() function, which drops any non-numeric characters from a string.\ndistance \u0026lt;- distance %\u0026gt;% mutate(year = parse_number(variable)) The tidying is now done, and so although this next step isn’t necessary, renaming and reordering the variables and factor levels will make the data easier to work with.\n# rename columns, reorder factor levels (e.g., Undergraduate before Graduate) distance \u0026lt;- distance %\u0026gt;% select(institution_name = `Institution Name`, level, modality, year, headcount) %\u0026gt;% mutate(level = fct_relevel(level, \u0026quot;Undergraduate\u0026quot;), modality = fct_relevel(modality, \u0026quot;Some Distance\u0026quot;)) distance ## # A tibble: 285,600 x 5 ## institution_name level modality year headcount ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Educational Technical College-Reci… Undergrad… Exclusively D… 2018 NA ## 2 A T Still University of Health Sci… Undergrad… Exclusively D… 2018 NA ## 3 Aaniiih Nakoda College Undergrad… Exclusively D… 2018 NA ## 4 ABC Adult School Undergrad… Exclusively D… 2018 NA ## 5 ABC Beauty Academy Undergrad… Exclusively D… 2018 NA ## 6 ABCO Technology Undergrad… Exclusively D… 2018 NA ## 7 Abcott Institute Undergrad… Exclusively D… 2018 NA ## 8 Abdill Career College Inc Undergrad… Exclusively D… 2018 NA ## 9 Abilene Christian University Undergrad… Exclusively D… 2018 32 ## 10 Abraham Baldwin Agricultural Colle… Undergrad… Exclusively D… 2018 377 ## # … with 285,590 more rows Behold, tidy data!\n Visualizing the Data With the data in a tidy format you can now do…pretty much whatever you want with it! In the examples below, I chose to visualize it, which demonstrates how - thanks to tidy data(!) - you can recycle the same code with slight alterations to make different plots. First, here are overall trends in distance education.\ndistance %\u0026gt;% mutate(headcount = replace_na(headcount, 0)) %\u0026gt;% group_by(year, modality) %\u0026gt;% summarise(total = sum(headcount)) %\u0026gt;% group_by(year) %\u0026gt;% mutate(prop = total / sum(total)) %\u0026gt;% ungroup() %\u0026gt;% filter(modality != \u0026quot;No Distance\u0026quot;) %\u0026gt;% ggplot(aes(x = factor(year), y = prop, fill = modality)) + geom_col() + scale_y_continuous(label = percent_format()) + theme(legend.position = \u0026quot;top\u0026quot;) + labs(x = \u0026quot;Year\u0026quot;, y = \u0026quot;% of Students\u0026quot;, title = \u0026quot;Percentage of Students Enrolled in Distance Education\u0026quot;, fill = NULL, subtitle = \u0026quot;Fall 2012 to Fall 2018\u0026quot;, caption = \u0026quot;Source: IPEDS Data Center\u0026quot;) Next, I change the grouping variables to repeat the same chart except here I partition the data by level.\ndistance %\u0026gt;% mutate(headcount = replace_na(headcount, 0)) %\u0026gt;% group_by(year, modality, level) %\u0026gt;% summarise(total = sum(headcount)) %\u0026gt;% group_by(year, level) %\u0026gt;% mutate(prop = total / sum(total)) %\u0026gt;% ungroup() %\u0026gt;% filter(modality != \u0026quot;No Distance\u0026quot;) %\u0026gt;% ggplot(aes(x = factor(year), y = prop, fill = modality)) + geom_col() + facet_wrap(~level) + scale_y_continuous(label = percent_format()) + theme(legend.position = \u0026quot;top\u0026quot;) + labs(x = \u0026quot;Year\u0026quot;, y = \u0026quot;% of Students\u0026quot;, title = \u0026quot;Percentage of Students Enrolled in Distance Education\u0026quot;, fill = NULL, subtitle = \u0026quot;Fall 2012 to Fall 2018\u0026quot;, caption = \u0026quot;Source: IPEDS Data Center\u0026quot;) And once more, limiting the results to a single institution: Florida State University.\ndistance %\u0026gt;% filter(institution_name == \u0026quot;Florida State University\u0026quot;) %\u0026gt;% mutate(headcount = replace_na(headcount, 0)) %\u0026gt;% group_by(year, modality, level) %\u0026gt;% summarise(total = sum(headcount)) %\u0026gt;% group_by(year, level) %\u0026gt;% mutate(prop = total / sum(total)) %\u0026gt;% ungroup() %\u0026gt;% filter(modality != \u0026quot;No Distance\u0026quot;) %\u0026gt;% ggplot(aes(x = factor(year), y = prop, fill = modality)) + geom_col() + facet_wrap(~level) + scale_y_continuous(label = percent_format()) + theme(legend.position = \u0026quot;top\u0026quot;) + labs(x = \u0026quot;Year\u0026quot;, y = \u0026quot;% of Students\u0026quot;, title = \u0026quot;Percentage of Florida State U. Students Enrolled in Distance Education\u0026quot;, fill = NULL, subtitle = \u0026quot;Fall 2012 to Fall 2018\u0026quot;, caption = \u0026quot;Source: IPEDS Data Center\u0026quot;)  Conclusion Among its many benefits, tidy data lets you devote more attention to what you want to do rather than how you want to do it. Yes, tidying data takes longer at the start, but in the long-run, it will save you time. In that way, it’s just like learning R!\n  https://r4ds.had.co.nz/tidy-data.html↩\n pivot_longer() is an updated version of gather().↩\n   ","date":1586217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586217600,"objectID":"192c5e91c87a2cb378e6b554f60a74e1","permalink":"/post/online-education/distance-education/","publishdate":"2020-04-07T00:00:00Z","relpermalink":"/post/online-education/distance-education/","section":"post","summary":"If you’ve downloaded enough data from the IPEDS Data Center using the “Compare Institutions” interface, you’ve probably realized that, depending on what you’re downloading, the data provided is rarely in a format ready for analysis. Here, via a specific example, I describe what makes the IPEDS data format impractical, and how to use R to resolve that.\nReading in the Data I first downloaded Fall 2012 to Fall 2018 distance education headcounts for every college and university in the IPEDS Data Center.","tags":["r","ipeds"],"title":"Tidying IPEDS Data in R","type":"post"},{"authors":["arie"],"categories":["workshop"],"content":" Learn how to visualize data using R and ggplot2!\nWhether you’re interested in data science, business, or working on a dissertation or research project, knowing how to visualize data is a vital skill. In this free, 2.5 hour workshop, you’ll be introduced to the R programming language and learn to visualize data using ggplot2. In the last 30 minutes of the workshop, Janine Morris from NSU’s Writing and Communication Center will talk about the features of effective data visualizations.\n","date":1584100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584100800,"objectID":"82742011de3a1cf9277098f94dea4f28","permalink":"/talk/2020-wcc-ggplot2/","publishdate":"2020-03-07T00:00:00Z","relpermalink":"/talk/2020-wcc-ggplot2/","section":"talk","summary":"Introduction to R and ggplot2","tags":["wcc","ggplot2"],"title":"POSTPONED DUE TO CORONAVIRUS Introduction to R and ggplot2","type":"talk"},{"authors":["arie"],"categories":["institutional research"],"content":" Introduction I first heard about R when I was in graduate school in 2008 and fellow students used it to analyze their data. I didn’t bother to learn it at the time because, one, I didn’t see the benefit of it, and two, I assumed that without any programming experience, it was too difficult. So I continued with my same workflow: Clean data and make charts in Excel, import data into SPSS to analyze it, and then paste my output into a Word document and write up the results.\nI started working in institutional research in 2013 and I still hadn’t made the switch to R, but was beginning to see the drawbacks of my workflow and the upside of coding. I often had to generate the same reports on a regular basis where the only thing that would change was the data. Or I’d have to generate the same charts or tables for each of the 15 colleges at the university, and on bad days, each of the 150-something majors. This quickly became unsustainable when I would, for example, get one of these requests late on a Friday afternoon and had to have it ready for a Board meeting on Monday. R increasingly seemed like a preferable alternative.\nFast-forward 7 years and my SPSS license has long since expired, I don’t recall the last time I made a chart in Excel, and the only thing I use Word for is making grocery lists. Today, my entire workflow exists inside of R.\nIn the intervening years, I have frequently met other institutional researchers who are stuck in the same mindset I was in 2008: For people who have never coded, R seems too overwhelming to learn, and even if they were to learn it, they do not see the benefits of doing so. In future posts I plan to address the former, but in this series of posts I want to address the latter: What’s the point of learning R for institutional research? Rather than list all of the reasons why R is an excellent choice for doing institutional research, I want to show examples of how I use it. In this post, I’ll demonstrate the scenario of using R to run many models.\nIf you are not an R user, do not worry about the details of the code below, but instead, pay attention to what the code is capable of producing.\n Running One Model Whether you want to predict future enrollment or explain why some students do not graduate, modeling is an important skill in institutional research. To show how to run a linear model in R, for all colleges and universities in the IPEDS Data Center, I downloaded their state, one year retention rates (i.e., the percent of first-time in college students who re-enroll their second fall term), student-faculty ratios, and the number of undergraduate applications they received for a given year. Here is the code for reading in the data and what the first five rows of data look like:\nlibrary(tidyverse) library(broom) library(drlib) ipeds \u0026lt;- read_rds(\u0026quot;processed-data/ipeds-sfr.rds\u0026quot;) head(ipeds, 5) ## # A tibble: 5 x 5 ## name state undergrad_applic… retention_rate student_faculty_… ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Educational Techni… Puerto… NA 11 21 ## 2 A T Still Universi… Missou… NA NA NA ## 3 Aaniiih Nakoda Col… Montana NA 34 10 ## 4 ABC Adult School Califo… NA NA 4 ## 5 ABC Beauty Academy Texas NA 25 10 In this contrived example, to build a linear model with retention rate as the outcome and student-faculty ratio and number of undergraduate applications as the predictors, I took the ipeds data frame, piped it (%\u0026gt;%) to the lm function, and then cleaned up the results with the tidy() function from the broom package. This gives us the model results:\nipeds %\u0026gt;% lm(retention_rate ~ student_faculty_ratio + undergrad_applicants, data = .) %\u0026gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.868 87.8 0. ## 2 student_faculty_ratio -0.326 0.0589 -5.54 3.45e- 8 ## 3 undergrad_applicants 0.000504 0.0000315 16.0 3.03e-54 At this point you may be thinking, “So what? I can just easily do the same thing in SPSS, or even Excel”. That is true, but what if instead of running one model, you had to run 150?\n Running Many Models As part of our university’s strategic business plan, I recently had to create separate models for each of the 150-something majors at the school. If I were still using SPSS, this would mean:\ndays of pointing and clicking and copying and pasting. doing the same thing over and over again each time the project requirements changed, which is an inevitability. having no documentation about the decisions I made because everything was done by pointing and clicking.  Returning to the original data set, let’s say I wanted repeat the same model above, but separately for each state. Using R, I first filter the data to only include states with at least 50 schools (an arbitrarily chosen cutoff point):\nipeds \u0026lt;- ipeds %\u0026gt;% add_count(state) %\u0026gt;% filter(n \u0026gt;= 50) Next, I turn the model into a function:\nstate_regression \u0026lt;- function(df) { df %\u0026gt;% lm(retention_rate ~ student_faculty_ratio + undergrad_applicants, data = .) } From there, I can apply the function to each state in the data set, which returns a data frame with the model results for each state:\nipeds_model \u0026lt;- ipeds %\u0026gt;% group_by(state) %\u0026gt;% nest() %\u0026gt;% mutate(model = map(data, state_regression), tidy_model = map(model, tidy)) %\u0026gt;% unnest(tidy_model) head(ipeds_model, 5) ## # A tibble: 5 x 8 ## # Groups: state [2] ## state data model term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;list\u0026lt;df[,5\u0026gt; \u0026lt;lis\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Puerto … [146 × 5] \u0026lt;lm\u0026gt; (Intercept) 87.5 5.80 15.1 2.48e-18 ## 2 Puerto … [146 × 5] \u0026lt;lm\u0026gt; student_fac… -0.869 0.315 -2.76 8.68e- 3 ## 3 Puerto … [146 × 5] \u0026lt;lm\u0026gt; undergrad_a… 0.00208 0.00140 1.49 1.45e- 1 ## 4 Missouri [171 × 5] \u0026lt;lm\u0026gt; (Intercept) 84.5 5.46 15.5 1.91e-22 ## 5 Missouri [171 × 5] \u0026lt;lm\u0026gt; student_fac… -1.07 0.458 -2.33 2.30e- 2 Now, with a separate model for each state all in a data frame, I can treat the model output like I would any other data. For example, here, I visualize the model results for each state:\nipeds_model %\u0026gt;% filter(term != \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% mutate(term = if_else(term == \u0026quot;student_faculty_ratio\u0026quot;, \u0026quot;Student/Faculty Ratio\u0026quot;, \u0026quot;# of Undergraduate Applications\u0026quot;)) %\u0026gt;% ggplot(aes(x = reorder_within(state, -estimate, term), y = estimate, ymin = estimate - (2 * std.error), ymax = estimate + (2 * std.error))) + geom_pointrange(color = \u0026quot;grey60\u0026quot;) + coord_flip() + guides(color = FALSE) + facet_wrap(~term, scales = \u0026quot;free\u0026quot;, ncol = 2) + theme_classic() + scale_x_reordered() + geom_hline(yintercept = 0, linetype = 2) + labs( title = str_wrap(\u0026quot;Is First-Year Retention Associated with Student-Faulty Ratio and/or Undergraduate Applications?\u0026quot;, 75), subtitle = \u0026quot;Limited to states with at least 50 schools\u0026quot;, caption = \u0026quot;Source: IPEDS Data Center\u0026quot;, x = NULL, y = \u0026quot;Estimate\u0026quot;)  Conclusion Claiming that there is more friction to learning R than there is to learning menu-driven tools is like saying learning to microwave TV dinners is easier than learning to cook the same meal from scratch. Both points might be true, but they obscure the ultimate goals of each: R, like cooking, unconstrains you, giving you the freedom to create whatever fills your imagination. And whether it’s running models for 150 majors or making soup for a large dinner party, learning to code and learning to cook can make your work not only more tenable, but more enjoyable, and in the long-run, simpler.\n ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"63a339f6dc11d9c30374d3378a4280ed","permalink":"/post/ipeds-many-models/01-r-for-institutional-research/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/ipeds-many-models/01-r-for-institutional-research/","section":"post","summary":"Introduction I first heard about R when I was in graduate school in 2008 and fellow students used it to analyze their data. I didn’t bother to learn it at the time because, one, I didn’t see the benefit of it, and two, I assumed that without any programming experience, it was too difficult. So I continued with my same workflow: Clean data and make charts in Excel, import data into SPSS to analyze it, and then paste my output into a Word document and write up the results.","tags":["modeling","ipeds"],"title":"Why Use R for Institutional Research? Part 1, Many Models","type":"post"},{"authors":["arie"],"categories":["education"],"content":" The Curse of Knowledge in Everyday Life Several years ago my friend Lauren asked me for my recipe for BBQ seitan. I love food-related conversation, so I wasted no time. “Start by sauteing some chopped onion in oil…”, and as quickly as I began, she cut me off. “Hold on,” she interjected. “What kind of oil do you use? How much? How high do you turn the heat?”\nDissecting the conversation, what happened was that I implicitly made the absurd assumption that knowledge that is in my head must be in hers (i.e., “Use however much of whatever oil you’d like to at whatever heat you normally saute”). In other words, I fell victim to the curse of knowledge. I’m not an expert cook - just ask my wife who always keeps the salt and pepper shaker within arm’s reach when I prepare a meal - but I did naively explain the recipe to Lauren as if she possessed my idiosyncratic definition of “saute”.\n The Curse of Knowledge When Teaching R Scenarios like this are universal, and most of the time, they are harmless. However, they can be frustrating when people have invested time and money to learn R from you. Even if you yourself are relatively to new to R, it is easy to take for granted all that you know and what it’s like to be a true beginner. Consider the following questions and confusion that a new R user might have when you ask them to do something as seemingly innocuous as running a line of read_csv() code you’ve provided:\n “I just bought a book that says to use read.csv(), but you use read_csv(). They are so similar they must do exactly the same thing, right?” “Excuse me, but are you saying tibble? Do you mean table?” “I tried running read_csv() but I got an error saying the function couldn’t be found. How does that make sense?” “I thought you said colons aren’t allowed in function names, so why did you write readr::read_csv()?” “The code you shared says read_csv(\u0026quot;raw-data/survey-results.csv\u0026quot;) but I changed the / to \\ because that’s what the folders look like on my computer and now it doesn’t work. WTF, right?!”  Not all of your students will tell you when they’re stuck, and because you can’t read their minds, what are you to do? Ask them! Whether it’s a one day workshop or a semester long course, giving frequent, brief, assessments will help you identify areas of confusion and guide your lessons.\nYou might be thinking that when you have a large group of people, resolving every question that every student has is unrealistic. That may be true, but it is a shame when a student falls behind because an instructor misses an opportunity for a simple clarification. Consider the following (intentionally confusing) passage from Bransford and Johnson (1972):\n The procedure is actually quite simple. First you arrange things into different groups… Of course, one pile may be sufficient depending on how much there is to do. If you have to go somewhere else due to lack of facilities that is the next step, otherwise you are pretty well set. It is important not to overdo any particular endeavor. That is, it is better to do too few things at once than too many. In the short run this may not seem important, but complications from doing too many can easily arise. A mistake can be expensive as well… At first the whole procedure will seem complicated. Soon, however, it will become just another facet of life. It is difficult to foresee any end to the necessity for this task in the immediate future, but then one never can tell. After the procedure is completed one arranges the materials into different groups again. Then they can be put into their appropriate places. Eventually they will be used once more and the whole cycle will have to be repeated. However, that is part of life. (Bransford and Johnson 1972 p. 722)\n If you’ve never seen this passage before, it probably makes little sense to you and its details are unlikely to stick in your mind. But if before you read it I gave you the passage’s title - Washing Clothes - it would suddenly become much clearer. There are plenty of “washing clothes” examples in R, and as the instructor, it’s your job to construct an environment that helps you identify them.\n Conclusion Frequent assessments will alert you when you succumb to the curse of knowledge and help you to correct your biases. If you’re teaching R - or for that matter, anything - and you’re not regularly checking in on what your students know and what their misconceptions are, it’s worth asking yourself what your goals are, because maximizing student understanding may not be one of them.\n ","date":1581638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581638400,"objectID":"38d650ffa71cd3fd39244dcd1bb1e282","permalink":"/post/curse-of-knowledge/curse-of-knowledge/","publishdate":"2020-02-14T00:00:00Z","relpermalink":"/post/curse-of-knowledge/curse-of-knowledge/","section":"post","summary":"The Curse of Knowledge in Everyday Life Several years ago my friend Lauren asked me for my recipe for BBQ seitan. I love food-related conversation, so I wasted no time. “Start by sauteing some chopped onion in oil…”, and as quickly as I began, she cut me off. “Hold on,” she interjected. “What kind of oil do you use? How much? How high do you turn the heat?”\nDissecting the conversation, what happened was that I implicitly made the absurd assumption that knowledge that is in my head must be in hers (i.","tags":["teaching","R","cognitive bias"],"title":"Fellow R Instructors: Beware of the Curse of Knowledge!","type":"post"},{"authors":["arie"],"categories":["workshop"],"content":" Part 2 of “Welcome to the Tidyverse”, which covers the following topics:\n Modeling with modelr and broom. Reporting with rmarkdown.  ","date":1579258800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579258800,"objectID":"dd0d24515469149eb65e7a6829fbd930","permalink":"/talk/colorado-ed-part-2/","publishdate":"2020-01-17T11:00:00Z","relpermalink":"/talk/colorado-ed-part-2/","section":"talk","summary":"Modeling with modelr and broom, and reporting with rmarkdown.","tags":["tidyverse"],"title":"Welcome to the Tidyverse: Part 2","type":"talk"},{"authors":["arie"],"categories":["workshop"],"content":" Part 1 of “Welcome to the Tidyverse”, which covers the following topics:\n Introduction to R. Visualization with ggplot2. Tranformation with dplyr.  ","date":1578999600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578999600,"objectID":"051cd82e7362543c54bce223d86fb3aa","permalink":"/talk/colorado-ed-part-1/","publishdate":"2020-01-14T11:00:00Z","relpermalink":"/talk/colorado-ed-part-1/","section":"talk","summary":"R basics, ggplot, and dplyr.","tags":["tidyverse"],"title":"Welcome to the Tidyverse: Part 1","type":"talk"},{"authors":["arie"],"categories":["visualization"],"content":" With ggplot2 - the ubiquitous tool for making plots in R - you can create beautiful data visualizations without doing much to the defaults. By applying the template below (see R for Data Science), adding a theme (e.g., theme_light()), and giving your chart custom labels, you can have a publication-ready visualization.\nggplot(data = \u0026lt;DATA\u0026gt;) + \u0026lt;GEOM_FUNCTION\u0026gt;( mapping = aes(\u0026lt;MAPPINGS\u0026gt;), stat = \u0026lt;STAT\u0026gt;, position = \u0026lt;POSITION\u0026gt; ) + \u0026lt;COORDINATE_FUNCTION\u0026gt; + \u0026lt;FACET_FUNCTION\u0026gt; But the more I pay attention to how people respond to visualizations, the more I realize how minor improvements can make a major difference. Like cooking, where adding a little bit of salt or giving a dish a few extra minutes in the oven can transform a meal from acceptable to outstanding, relatively small changes to a chart can do the same.\nTake the example below, in which one of Stephanie Evergreen’s clients started with the slide on the top, and she helped them create the one below it:\nBoth charts contain the same information, and even if you can’t express why, you just know the one on the bottom is better. Continuing with the cooking analogy, the charts are like two Mexican restaurants that use the same ingredients, but at one, the guacamole is fresher, the rice is more flavorful, and the tortillas are made in house.\nggplot2 was designed so users can build any plot that they can imagine, so as attractive as its defaults are, my goal with this series of posts is to venture beyond the minor design adjustments I typically make and learn to tweak charts from ordinary to outstanding. My first goal was to use ggplot2 to reproduce the chart that Dr. Evergreen created for her client, which based on her post, I think she did in Excel.\nlibrary(tidyverse) library(scales) library(cowplot) Step 0: Create the Dataset In addition to creating the data set, I used fct_reorder() so the bars will appear in order from highest to lowest growth in the chart.\ngrowth \u0026lt;- tribble( ~region, ~growth, ~group, \u0026quot;Large city, Midwest\u0026quot;, .2, \u0026quot;Midwest\u0026quot;, \u0026quot;Large city, East Coast\u0026quot;, .175, \u0026quot;Other\u0026quot;, \u0026quot;Medium city, South\u0026quot;, .165, \u0026quot;Other\u0026quot;, \u0026quot;Medium city, Midwest\u0026quot;, .165, \u0026quot;Midwest\u0026quot;, \u0026quot;Small city, Midwest\u0026quot;, .14, \u0026quot;Midwest\u0026quot;, \u0026quot;Large city, West coast\u0026quot;, .10, \u0026quot;Other\u0026quot; ) %\u0026gt;% mutate(region = fct_reorder(region, growth))   Step 1: Make the Foundational Chart This chart contains all of the information that the final chart contains, but it’s like the decent restaurant you’ll never return to: fine, but unmemorable. In this step, I also narrowed the width of the bars, a subtle alteration that improves the feel of the chart as it gets closer to the finished product.\n(p \u0026lt;- growth %\u0026gt;% ggplot(aes(x = region, y = growth, fill = group)) + geom_col(width = .7) + coord_flip() + labs(x = NULL, y = NULL, title = \u0026quot;Geographic growth dominated by Midwest.\u0026quot;, subtitle = \u0026quot;5 year growth\u0026quot;, caption = \u0026quot;Source: Our Smart Source 2015\u0026quot;))  Step 2: Update the Colors and Remove the Legend Even if you like the default ggplot2 colors - which I very much do - putting a dark color next to a muted color does a better job of highlighting a conclusion you might want to draw attention to. And by juxtaposing the green and gray, in combination with the chart’s title, the legend becomes extraneous and can be removed.\n(p \u0026lt;- p + scale_fill_manual(values = c(\u0026quot;#4D643D\u0026quot;, \u0026quot;#D7DBDD\u0026quot;)) + guides(fill = FALSE))  Step 3: White Background and Minimal Gridlines The white background can be achieved using theme_minimal(), and removing all of the grid lines other than the major-x ones cleans up the look of the chart.\n(p \u0026lt;- p + theme_minimal() + theme(panel.grid.minor.x = element_blank(), panel.grid.major.y = element_blank()))  Step 4: Change Font and Convert Axis to % Taken together, using a lighter color for the axis text and bolding the labels, make the chart look more professional. It’s so easy to rely on ggplot2’s default font choices that it’s also easy to forget that changing them can give your chart a completely different look. In this step, I also expressed the axis as percents, which is more consistent with how people think about growth (and happens to look better).\n(p \u0026lt;- p + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;), plot.subtitle = element_text(face = \u0026quot;bold\u0026quot;), plot.caption = element_text(color = \u0026quot;gray53\u0026quot;, hjust = -.15, face = \u0026quot;bold\u0026quot;), axis.text.y = element_text(color = \u0026quot;gray30\u0026quot;, face = \u0026quot;bold\u0026quot;), axis.text.x = element_text(color = \u0026quot;gray30\u0026quot;, face = \u0026quot;bold\u0026quot;)) + scale_y_continuous(label = percent_format(), limits = c(0, .25)))   Step 5: Add an Icon The logo - which I added using the cowplot package - is functionally unnecessary but aesthetically powerful. It is the cilantro garnish on top of your beans and rice: Doesn’t add much flavor, but makes you all the more eager to dig in!\nggdraw() + draw_image(\u0026quot;us-map.png\u0026quot;, y = .85, x = 0.1, width = .15, height = .15 ) + draw_plot(p)  Conclusion ggplot2 has become so popular that I rarely, if ever, search online for a question about it that hasn’t already been asked and answered. This task was no different, with each question I had (e.g., how do I add an icon?) quickly resolved with a StackOverflow solution.\nSmall adjustments make a big difference, and the beauty of ggplot2 is that those adjustments are not only possible, but the knowledge to accomplish them is accessible (RStudio Community, R for Data Science, etc.). Stay tuned for more tricks on how you can make your ggplots more interpretable, compelling, and visually satisfying!\n ","date":1578182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578182400,"objectID":"207a70b6862cfab3d34453cccc015a34","permalink":"/post/ggplotredo1/geographic-growth-midwest/","publishdate":"2020-01-05T00:00:00Z","relpermalink":"/post/ggplotredo1/geographic-growth-midwest/","section":"post","summary":"With ggplot2 - the ubiquitous tool for making plots in R - you can create beautiful data visualizations without doing much to the defaults. By applying the template below (see R for Data Science), adding a theme (e.g., theme_light()), and giving your chart custom labels, you can have a publication-ready visualization.\nggplot(data = \u0026lt;DATA\u0026gt;) + \u0026lt;GEOM_FUNCTION\u0026gt;( mapping = aes(\u0026lt;MAPPINGS\u0026gt;), stat = \u0026lt;STAT\u0026gt;, position = \u0026lt;POSITION\u0026gt; ) + \u0026lt;COORDINATE_FUNCTION\u0026gt; + \u0026lt;FACET_FUNCTION\u0026gt; But the more I pay attention to how people respond to visualizations, the more I realize how minor improvements can make a major difference.","tags":["ggplot2"],"title":"ggplot2: Going Beyond the Defaults","type":"post"},{"authors":["Arie Spirgel"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["arie"],"categories":["nba"],"content":" On October 29, 2018, in a game against the Chicago Bulls, Klay Thompson attempted 24 three-pointers, making 14 of them; both marks set NBA records. A couple months after that his teammate Stephen Curry attempted 14 three pointers in a single half of a single NBA game, which tied an NBA record. That same game, the Golden State Warriors and Sacramento Kings made a combined 41 threes, a total that had never been reached before. Just this morning when I checked ESPN.com I learned that the previous night James Harden scored 61 points in Madison Square Garden, but maybe more shocking, he did it while attempting 20 three-point shots (he only made five of them).\nFor anyone who has been paying attention to the NBA for more than a few years, these numbers look like typos. The NBA began using the three-point line in 1979, and on October 12 of that year - almost 40 years ago - Chris Ford made the first one. For most of its history the three-point shot has been like that kitchen gadget you’ve always had but rarely used until someone showed you what a powerful tool it could be (immersion blender maybe?).\nIn the case of the NBA, that someone would be Houston Rockets General Manager Daryl Morey, who wisely drew attention to math and the reality that three is worth more than two. More specifically, his insight was that the shots most worth taking are higher percentage attempts near the basket (layups and dunks, ideally), or lower percentage ones that are further from the basket but are worth three points (free throws are also an important part of the equation in so-called Morey-Ball). This means that if a team’s goal is to get the most points out of every position (and that is every team’s goal), mid-range shots should be discouraged: They are harder to make than layups and dunks and still worth only two points.\nThis has had a profound effect on how games are played and which players are valued. It wasn’t long ago that big men were expected to be able to post up near the basket and wouldn’t have to think about life outside the three-point arc. In 2019, a few games past the midpoint of the current season, Brook Lopez has already attempted 304 threes, Karl-Anthony Towns attempted 217, Marc Gasol attempted 193, Joel Embid attempted 178. Each of these players is listed as a center and is at least 7’0 feet tall. And the list goes on.\nAnd it’s not just who is shooting threes, it’s how they’re being shot. James Harden is setting new records for the number of unassisted threes made, meaning he doesn’t just shoot from deep as a compromise when better shots are not available, but it’s as if any shot that is not a three is a compromise. Or consider Curry, for whom the three-point line itself is merely a suggestion; he has made 45 of 94 threes (47.9%!) launched from between 30 and 35 feet since 2014-2015 (the line is 23 ft 9 inches from the basket, and 22 feet if you are standing in the corner).\nIn 2019, you don’t have to watch games closely to notice the abundance of three pointers that are being shot. But still, I wanted to attach some numbers to the obvious, so below, I present a brief quantitative history of NBA three-point attempts.\nPercentage of All Shot Attempts That Were Threes It’s easy to forget how recent the explosion in three-point attempts has been. The chart below, which displays the percentage of all shot attempts that were threes, shows a gradual increase from 1998 to 2007. But from 2008 until 2012, the numbers leveled off, and it seemed peak-three had been reached. Then 2014 to 2018 happened, and the increase during that time period alone (8%) was greater than it was from all of 1998 to 2012 (7%)\nAlthough it’s a bit of an aside, despite the increase in the number of attempts, the percentage of three-pointers that have been made has remained relatively steady, as displayed below.\n Distribution of Threes As I mentioned above, three-point shots are no longer limited to certain positions, a point that is reflected in the histogram below. It shows players’ average number of three-point attempts per game along the x-axis, and the number of players falling within that range for the season is represented along the y-axis. Over time, notice how the distribution gets flatter. In 1998, the group who shot between zero and 0.5 threes per game appeared with the most frequency. But by 2018 that bar dropped significantly and the other bars lifted up. And pay attention to right side of the chart beginning in 2016.\n Top Attempters by Year Who are these gunners that are changing how basketball is played? The chart below displays the three players who averaged the most threes per game for each season from 1998 to 2018. Even the casual fan will not be surprised by the more recent appearances on the chart: Splash-Brothers Curry and Thompson; and Gordon and Harden, whose GM is the aforementioned Daryl Morey.\n Curry Explains it All Daryl Morey’s influence on the game and the state of the evolution of the three-point shot is encapsulated by the shot charts below, which I downloaded using Todd Schneider’s ballr package. The chart on the left is Stephen Curry’s shot chart from his 2009-2010 rookie season, and the shot chart on the right is from his 2017-2018 season. It’s almost as if during that time the in-between game disappeared, leaving only three point shots and shots at the basket. That, folks, is modern basketball.\n Conclusion The new style of play is not for everyone. Like any trend, this one can’t go on forever, but I do wonder how far it can go until it reaches a breaking point. Maybe the breaking point comes from the league if they decide to move the line further back or establish a four-point line. Or maybe it comes from the next Daryl Morey who discovers an inefficiency in the way the game is currently played. Either way, enjoy the show, because there is nothing quite like watching Stephen Curry casually make game winning shots from close to half court!\n ","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548288000,"objectID":"fad8752f3dec90a67908f0849183840c","permalink":"/post/history-3s/history-of-basketball/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/post/history-3s/history-of-basketball/","section":"post","summary":"On October 29, 2018, in a game against the Chicago Bulls, Klay Thompson attempted 24 three-pointers, making 14 of them; both marks set NBA records. A couple months after that his teammate Stephen Curry attempted 14 three pointers in a single half of a single NBA game, which tied an NBA record. That same game, the Golden State Warriors and Sacramento Kings made a combined 41 threes, a total that had never been reached before.","tags":["ggplot2","gganimate"],"title":"A Brief History of NBA Three-Point Attempts: 1998 to 2018 Seasons","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"About / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ba63ea3d55678df7fe7c8a43f1e2f005","permalink":"/list/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/list/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":["arie"],"categories":["education"],"content":" The chief administrator job of a higher education institution is, as the title implies, the pinnacle of careers in academic administration. The chief administrator is a school’s spokesperson and guides its vision, affecting the lives of the thousands of students who pass through those institutions. And for the chief administrators who don’t care about the idealism of educating future generations, I would imagine the high-six/low-seven figure salaries that many earn is incentive enough. Despite the prominent roles that these administrators fill, there is a dearth of publicly available data on them.\nEvery institution that participates in federal student financial aid is required to submit data to Integrated Postsecondary Education System, or IPEDS. Eventually, the submitted data is made publicly available in the IPEDS Data Center. This includes numbers on admissions, student enrollment, degree completions, graduation rates, financial aid, finances, human resources, and libraries. Much of this data has to be reported by gender and race/ethnicity. For example, how many Hispanic female undergraduates began at an institution last fall? What is the graduation rate of male American Indian Alaska Natives? What is the average salary of female instructional staff on a 9-month contract? But for one reason or another, as far as I can tell, the only things that must be reported about an institution’s chief administrator are her or his name and title.\nNames, though, are not completely devoid of meaningful information. If you live in the United States and hear the name Steven, you probably think of a male, and if you hear the name Mary, you probably think of a female. Yes, some names are more ambiguous than others (my own being a good example), and some people’s names might belie the gender they identify with, but there is a degree of reliability that a person’s name offers in determining whether they are female or male. Thus, using first names to make educated guesses about chief administrators’ gender, my goal here was to describe gender representation among this set of individuals.\nThe first step was to download the names of the chief administrators for every institution in the IPEDS Data Center (n = 7108) and clean up the data. This required putting years in a consistent format; removing titles preceding first names (e.g., Dr., Ms., Mrs.); extracting first names into their own column; and adding variable labels for college sector.\nlibrary(tidyverse) library(babynames) library(scales) admin \u0026lt;- read_csv(\u0026quot;data/chief-admin-names.csv\u0026quot;) # tidy year admin \u0026lt;- admin %\u0026gt;% gather(\u0026quot;year\u0026quot;, \u0026quot;name\u0026quot;, `Name of chief administrator (HD2016)`, `Name of chief administrator (HD2015)`:`Name of Chief Administrator (IC90HD)`) # extract year from names and put in consistent format admin \u0026lt;- admin %\u0026gt;% mutate(year = parse_number(year), year = ifelse(year \u0026gt;= 9596 \u0026amp; year \u0026lt;= 9798, str_sub(start = 1, end = 2, year), year), year = ifelse(nchar(year) == 2, paste0(\u0026quot;19\u0026quot;, year), year), year = as.integer(year)) # select and rename variables admin \u0026lt;- admin %\u0026gt;% select(unit_id = UnitID, sector_code = `Sector of institution (HD2016)`, institution_name = `Institution Name`, year, name, undergrad_enroll_2016 = `Grand total (EF2016 All students Undergraduate total)`, grad_enroll_2016 = `Grand total (EF2016 All students Graduate and First professional)`) # titles are in the first position for many names, so need to remove # those so can extract first posistion from names and have it reflect # first name admin \u0026lt;- admin %\u0026gt;% mutate(name = tolower(name), name = gsub(\u0026quot;\\\\.\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;dr\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;mr\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;ms\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;mrs\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;rev\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;reverend\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;very reverend\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;very\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;rabbi\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;msgr\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;dra\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = gsub(\u0026quot;\\\\\u0026lt;sr\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name), name = str_trim(name, side = \u0026quot;both\u0026quot;), full_name = tolower(name)) %\u0026gt;% separate(name, into = \u0026quot;first_name\u0026quot;, sep = \u0026quot; \u0026quot;) # add sector label labels \u0026lt;- read_csv(\u0026quot;data/sector-value-labels.csv\u0026quot;) %\u0026gt;% select(sector_code = Value, sector_label = ValueLabel) admin \u0026lt;- left_join(admin, labels, by = \u0026quot;sector_code\u0026quot;) Next, I assigned a gender to each chief administrator based on her or his first name. To do this, I used R’s babynames package, which contains the number of babies born every year from 1880 to 2015 for each combination of name and sex1. I took the babynames data and calculated how often each name was given to females vs. males, and then assigned gender based on the higher proportion. For example, in the babynames data, about 72% of all newborns named Jaydin were male, so I assigned the name Jaydin to male. Names like Jaydin, however, were the exception: Most of the time, names went overwhelmingly to one sex or the other, with the vast majority of names in the babynames dataset being associated with only one sex.\n# read in babynames data and assign a proportion to female and male for each name baby_names \u0026lt;- babynames %\u0026gt;% select(sex, name, n) %\u0026gt;% mutate(name = tolower(name)) baby_names \u0026lt;- baby_names %\u0026gt;% group_by(sex, name) %\u0026gt;% summarise(total = sum(n)) %\u0026gt;% ungroup() %\u0026gt;% group_by(name) %\u0026gt;% mutate(prop = total/sum(total)) %\u0026gt;% filter(prop == max(prop)) %\u0026gt;% ungroup() %\u0026gt;% select(sex, first_name = name, prop) # one chief admin officer has name gold and it happens to be one that is .5 prop, # so removed from dataset. (gold was given as full name, so i\u0026#39;m *guessing* this # is actually last name). baby_names \u0026lt;- baby_names %\u0026gt;% filter(first_name != \u0026quot;gold\u0026quot; | prop != .5) Historical Trends This gave me a “dictionary” containing the probabilistic sex of 97,430 first names, which I then linked to the chief administrator data2, making it possible to examine historical trends in gender differences among chief administrators. (The babynames data is based on sex, but once I link it to adults’ names (i.e., the administrators), I make the (often wrong) assumption that names reflect gender. Also, of course, with this data, it is not possible to account for gender non-binary administrators).\n# join ipeds and babynames, removing rows where there were no matches admin \u0026lt;- left_join(admin, baby_names, by = \u0026quot;first_name\u0026quot;) %\u0026gt;% mutate(institution_name = gsub(\u0026quot;-\u0026quot;, \u0026quot; \u0026quot;, institution_name)) %\u0026gt;% filter(!is.na(sex)) admin \u0026lt;- admin %\u0026gt;% select(unit_id, institution_name, sector_label, year, undergrad_enroll_2016, grad_enroll_2016, full_name, first_name, sex) # several variables are for 2016 only, so put those in their own data frame admin_2016 \u0026lt;- admin %\u0026gt;% filter(year == 2016) %\u0026gt;% select(-year) admin \u0026lt;- admin %\u0026gt;% select(-undergrad_enroll_2016, -grad_enroll_2016) # proportion female by year female_prop_sex \u0026lt;- admin %\u0026gt;% count(year, sex) %\u0026gt;% group_by(year) %\u0026gt;% mutate(year_total = sum(n)) %\u0026gt;% ungroup() %\u0026gt;% mutate(prop_sex = n/year_total) %\u0026gt;% filter(sex == \u0026quot;F\u0026quot;) female_prop_sex %\u0026gt;% ggplot(aes(x = factor(year), y = prop_sex, group = 1)) + geom_line() + geom_point() + theme_minimal() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_y_continuous(label = percent_format()) + labs(x = \u0026quot;Year\u0026quot;, y = \u0026quot;Percent Women\u0026quot;, title = \u0026quot;Percentage of Chief Administrators Who Are Women\u0026quot;, subtitle = \u0026quot;All IPEDS Institutions, Years 1990-2016\u0026quot;) There are three things to notice about the chart:\nThe increasing percentage of chief administrators who are women3. The still minority percentage of chief administrators who are women The 35% figure I came up with for 2016 is roughly consistent with a survey reporting that 30% of 2016 college presidents were women), and compatible with the trends reported here, lending some support to the approach I selected.   Institution Type School, as it’s used in IPEDS, is a broad term that covers vastly different types of institutions. It includes everything from a cosmetology school that enrolls a handful of students each year to flagship schools with billion-plus endowments and tens of thousands of students. That is to say, chief administrator positions vary in prestige, responsibility, salary, and a host of other intangibles. With that in mind, I calculated the percentage of female chief administrators, by sector.\n# proportion female by sector female_by_sector \u0026lt;- admin_2016 %\u0026gt;% count(sector_label, sex) %\u0026gt;% group_by(sector_label) %\u0026gt;% mutate(prop_sex = n/sum(n)) %\u0026gt;% ungroup() %\u0026gt;% filter(sex == \u0026quot;F\u0026quot;) female_by_sector %\u0026gt;% ggplot(aes(x = reorder(sector_label, prop_sex), y = prop_sex)) + geom_col() + coord_flip() + scale_y_continuous(label = percent_format()) + theme_minimal() + labs(x = NULL, y = \u0026quot;Percent Women\u0026quot;, title = \u0026quot;Percentage of Chief Aministrators Who Are Women,\\nby Sector\u0026quot;, subtitle = \u0026quot;All IPEDS Institutions, Year 2016\u0026quot;) Although they are a minority in every sector, women make up a higher percentage of chief administrators at 2-year schools than at 4-year schools. One institution type is not better than another, but they serve different functions (e.g., teaching vs. research), meaning the disparities by sector further exaggerate the existing imbalance. For example, in 2016, 35% of academic chief administrators were women, yet the institutions they led accounted for only 29% of all students enrolled at institutions of higher education.\n Conclusion The overall trend is moving in the right direction, but change is slow: From 1990 to 2016, the average yearly increase in the percentage of women chief administrators was 1%. These are prestigious jobs that aren’t vacated haphazardly, so for the near-term, the disparity is here to stay: If the same rate of change observed from 1990 to 2016 continues, it will take 23 years until gender parity among chief administrators is achieved.\nThis is of course not a problem unique to higher education, but a societal one that begins well before women submit job applications. We must be aware of our biases - stop always telling your niece that she is pretty and her brother that he is smart! - however subtle they are, and correct them. You don’t have to be a woman to care about this, after all, “human rights are women’s rights”.\n  The data is restricted to combinations of five for more.↩\n I removed rows where there were no matches between first names in the IPEDS data and first names in the babynames data.↩\n You may have noticed a slight dip from 2006 to 2009. Considering how abrupt it is, I’m skeptical it represents a real trend. My guess is that it is instead reflective of an unrelated change in the underlying data (e.g., different reporting requirements), but I’m not entirely sure.↩\n   ","date":1535068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535068800,"objectID":"12c4de63f37f591f7e194fe9ddaa5b1e","permalink":"/post/gender-college-presidents/gender-college-presidents/","publishdate":"2018-08-24T00:00:00Z","relpermalink":"/post/gender-college-presidents/gender-college-presidents/","section":"post","summary":"The chief administrator job of a higher education institution is, as the title implies, the pinnacle of careers in academic administration. The chief administrator is a school’s spokesperson and guides its vision, affecting the lives of the thousands of students who pass through those institutions. And for the chief administrators who don’t care about the idealism of educating future generations, I would imagine the high-six/low-seven figure salaries that many earn is incentive enough.","tags":["gender","ipeds"],"title":"An Analysis of Gender Disparity Among Higher Education Chief Administrators","type":"post"},{"authors":["arie"],"categories":["education"],"content":" Data: It’s become a cliche to say that it’s everywhere and in quantities that are unimaginable. But data in its raw form, whether in a structured database or on the internet, is of limited use until a human does something to it: Gather it, clean it, visualize it, model it, write about it, and so-on.\nThe amount of data that those in institutional research encounter requires powerful tools to work with. And those tools exist. Lots of them. Everything from free, open-source software, to software costing hundreds of thousands dollars offered by companies that won’t stop emailing you despite unsubscribing from their list on a weekly basis.\nOn the rare occasion someone asks me which software I’d recommend, I always say R. In my experience, its ability to do everything you’d want and need to do as an institutional researcher is unmatched (cut to five minutes later when that someone regrets having asked me and is looking for ways to exit the conversation). But rather than use this space to drone on about why I think R is amazing, my goal here is to reveal the software preferences of others in the field. (And obviously, I used R to do this!)\nIt’s not a perfect approach, but my thinking was I could see how often different tools were mentioned in AIR Forum program books and how that’s changed over time. My first step was to download the program books from the forum website going back six years. Next, I created a function to read each of the programs books into R and tidy them so every individual word within each book is contained on its own line in a single data frame.\n# load packages library(pdftools) library(tidyverse) library(tidytext) library(scales) # function to read in PDFs and get one word per line. text_prepr \u0026lt;- function(doc, forum_year){ reg \u0026lt;- \u0026quot;([^A-Za-z\\\\d#@\u0026#39;]|\u0026#39;(?![A-Za-z\\\\d#@]))\u0026quot; df \u0026lt;- pdf_text(doc) df \u0026lt;- data.frame(df) df \u0026lt;- df %\u0026gt;% rename(text = df) %\u0026gt;% unnest_tokens(word, text, token = \u0026quot;regex\u0026quot;, pattern = reg) %\u0026gt;% mutate(year = forum_year) return(df) } # apply function to programs books all_years \u0026lt;- bind_rows( text_prepr(\u0026quot;data/AIR-2018-Forum-Program-Book.pdf\u0026quot;, 2018), text_prepr(\u0026quot;data/2017-AIR-Forum-Program-Book.pdf\u0026quot;, 2017), text_prepr(\u0026quot;data/2016_AIR-Forum_Program-Book.pdf\u0026quot;, 2016), text_prepr(\u0026quot;data/2015-Forum-Program-Book-Web.pdf\u0026quot;, 2015), text_prepr(\u0026quot;data/2014ForumProgramBookFinal.pdf\u0026quot;, 2014), text_prepr(\u0026quot;data/2013finalprogram.pdf\u0026quot;, 2013) ) This resulted in a data frame with a total 447582 rows (one row for each word), a glimpse of which is printed below:\nhead(all_years) ## word year ## 1 association 2018 ## 2 for 2018 ## 3 institutional 2018 ## 4 research 2018 ## 5 2018 2018 ## 6 may 2018 Next, I decided on which words I would ask R to look for. Somewhat arbitrarily and somewhat based on my experience at AIR Forums, I chose the following: Excel, R, SAS, SPSS, and Tableau. The code below (1) searches for mentions of those tools in the list of words created above (2) counts the results by year (3) builds a chart of the results. To clarify, the resulting chart displays the number of times each software is mentioned in each of the AIR Forum program books for each of the respective years.\n# filter for keywords and count by word and year software \u0026lt;- all_years %\u0026gt;% mutate(word = tolower(word)) %\u0026gt;% filter(word %in% c(\u0026quot;excel\u0026quot;, \u0026quot;r\u0026quot;, \u0026quot;sas\u0026quot;, \u0026quot;spss\u0026quot;, \u0026quot;tableau\u0026quot;)) %\u0026gt;% count(word, year) %\u0026gt;% complete(word, year, fill = list(n = 0)) # make data frame of only 2018, so can include as labels at end of lines software_2018 \u0026lt;- software %\u0026gt;% filter(year == 2018) # create chart ggplot() + geom_line(data = software, aes(x = year, y = n, color = word, group = word), size = 2) + geom_text(data = software_2018, aes(x = year, y = n, label = word), nudge_y = 2) + geom_point(data = software, aes(x = year, y = n, color = word), size = 3) + theme_minimal() + labs(x = \u0026quot;Forum Year\u0026quot;, y = \u0026quot;# of Mentions\u0026quot;, title = \u0026quot;Number of Times Software is Mentioned in AIR Forum Program Book\u0026quot;, subtitle = \u0026quot;2013 to 2018 Forums\u0026quot;) + guides(color = FALSE) What stands out - and confirms what I’ve noticed at AIR Forums - is the rapid rise of Tableau. In 2014 I had never heard of it. In 2018, not knowing at least something about it seems unavoidable. Tableau is known for visual analytics, so it’s not surprising that its rise in populairty has coincided with an increasing interest in data visualization at AIR Forums.\n# filter for keyword and count by word and year visualize \u0026lt;- all_years %\u0026gt;% group_by(year) %\u0026gt;% mutate(total = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(visualiz = str_detect(word, \u0026quot;visualiz\u0026quot;)) %\u0026gt;% group_by(year) %\u0026gt;% summarise(prop = mean(visualiz)) %\u0026gt;% ungroup() # create chart ggplot(visualize, aes(x = year, y = prop)) + geom_line(size = 2) + geom_point(size = 3) + theme_minimal() + labs(x = \u0026quot;Forum Year\u0026quot;, y = \u0026quot;% of Mentions\u0026quot;, title = \u0026#39;Percent of All Words That Were \u0026quot;Visualiz*\u0026quot; in AIR Forum Program Book\u0026#39;, subtitle = \u0026quot;2013 to 2018 Forums\u0026quot;) + guides(color = FALSE) + scale_y_continuous(label = percent_format()) Returning to the first chart, perhaps it’s my background in psychology - a field which has historically been dominated by SPSS - but I was surprised how little SPSS is mentioned (although, I wouldn’t be surprised to see it decrease in the future). As for R, it seems to have a presence, but I’m curious to see what happens to its popularity in institutional research over the next few years. Judging by my experience at recent Forums and the development of R tools that decrease the barrier to entry, my prediction is that interest will only grow.\nI’m not sure if this need be said, but I will: This approach to evaluating software popularity is fraught with limitations. My goal, however, was not to get a precise estimate. Rather, I was interested in one, getting a general sense of broad trends, and two, sharing how just a little of bit R code can do so much!\n","date":1535068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535068800,"objectID":"4ae2c2f6beaa89f4f865762527bb6cb7","permalink":"/post/air-forum-text-software-frequency/air-forum-text-software-frequency/","publishdate":"2018-08-24T00:00:00Z","relpermalink":"/post/air-forum-text-software-frequency/air-forum-text-software-frequency/","section":"post","summary":"Data: It’s become a cliche to say that it’s everywhere and in quantities that are unimaginable. But data in its raw form, whether in a structured database or on the internet, is of limited use until a human does something to it: Gather it, clean it, visualize it, model it, write about it, and so-on.\nThe amount of data that those in institutional research encounter requires powerful tools to work with.","tags":["r","spss"],"title":"Popularity of Various Data Analytic Tools at AIR Forums","type":"post"},{"authors":["arie"],"categories":["conference presentation"],"content":" Although the concept of reproducibility is typically reserved for the sciences, the presenter will argue that by adopting its principles, IR offices would see immeasurable benefits in efficiency, accuracy, and transparency. Reproducible workflows preserve every decision made about data analyses (e.g., removing a student who withdrew) and allow users to quickly and accurately respond to requests for modifications (e.g., group tables by college instead of major). One barrier to reproducibility, however, is that it requires coding. Using examples from the free R programming language, the presenter will show that not only is R an ideal software for reproducibility, but that many of its modern features are designed to get novices quickly doing powerful things. The primary goals of the presentation are for audience members to leave convinced that they can learn R, and that if they do, they will become better at their jobs.\n","date":1527763500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527763500,"objectID":"136786ca731bb255e98898d725bb0902","permalink":"/talk/2018-air/","publishdate":"2018-05-31T10:45:00Z","relpermalink":"/talk/2018-air/","section":"talk","summary":"Demonstrating the value of reproducibility with R and R Markdown to IR institutional researchers.","tags":["air2018","rmarkdown"],"title":"Achieving Reproducibility in IR With R","type":"talk"},{"authors":["arie"],"categories":["conference presentation"],"content":" A multitude of tools and promising practices exist for visualizing data for informing and decision making. This session will feature a showcase of tools and techniques for utilizing data to inform and present. Presenters were all provided the same dataset and guiding questions and each used a different tool (Excel, R, Tableau, and SAS Visual Analytics) to create a visual display of the data. Each will present their work with the session moderator highlighting best practice in visualization throughout the presentation.\n","date":1496158200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496158200,"objectID":"438becd2c97e08124ced2511c9b9fc69","permalink":"/talk/2017-air/","publishdate":"2017-05-30T15:30:00Z","relpermalink":"/talk/2017-air/","section":"talk","summary":"This session features a showcase of tools and techniques for utilizing data to inform and present.","tags":["air2017","ggplot2"],"title":"Data Visualization Showcase","type":"talk"},{"authors":["arie"],"categories":["conference presentation"],"content":" Creating graphs is a central part of the IR workload, but it can be frustrating. If you make graphs using Excel, you might spend hours mindlessly pointing-and-clicking, which then has to be repeated if updates to the graphs are required. The presenter will begin by arguing in favor of two points: First, IR professionals would be better off using R (a free and open-source programming language), and second, even those without any programming experience can learn R well enough to reap its majesty. A demonstration will be given on how to create and modify a scatter plot using ggplot2, an R package for building highly customizable visualizations. The primary goal of the presentation is for audience members to leave feeling empowered to learn to use R to improve the quantity, quality, and reproducibility of their work\n","date":1467385200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467385200,"objectID":"90957c2ce4e542d00856f84715b63e68","permalink":"/talk/2016-air/","publishdate":"2016-07-01T15:00:00Z","relpermalink":"/talk/2016-air/","section":"talk","summary":"The primary goal of the presentation is for audience members to leave feeling empowered to learn to use R to improve the quantity, quality, and reproducibility of their work.","tags":["air2016","ggplot2"],"title":"Creating Data Visualizations Using R: An Introduction for Non-Programmers","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Arie Spirgel","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Arie Spirgel","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]