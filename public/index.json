[{"authors":["admin"],"categories":null,"content":"My background is in cognitive psychology, and for my dissertation research, I examined how writing helps (or doesn\u0026rsquo;t help) people learn new information. I have worked in education since 2011, and I am certified by RStudio to teach the tidyverse. I am available to train groups or individuals to learn R and the tidyverse, and to consult with organizations to help them meet their data analytic needs.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://arie.rbind.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"My background is in cognitive psychology, and for my dissertation research, I examined how writing helps (or doesn\u0026rsquo;t help) people learn new information. I have worked in education since 2011, and I am certified by RStudio to teach the tidyverse. I am available to train groups or individuals to learn R and the tidyverse, and to consult with organizations to help them meet their data analytic needs.","tags":null,"title":"Arie Spirgel","type":"authors"},{"authors":null,"categories":null,"content":"Test\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4d7d8e2a0129bf0277ebe9bf2ca8cce","permalink":"https://arie.rbind.io/about/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/about/","section":"about","summary":"Test","tags":null,"title":"","type":"about"},{"authors":["arie"],"categories":["nba"],"content":"\r\r\r\rMichael. Larry. Moses. Mention these names to the most casual of NBA fans, and they know that you mean Jordan, Bird, and Malone, and not Doleac, Siegfried, and Brown. Even among other highly accomplished individuals, the all-time greats are so transcendent that they are recognized by their first names alone.\nWe all agree who the best Michael is, but who are the other great (or good) Michaels? And LeBron is the best LeBron, but has a different LeBron ever played in the NBA (spoiler alert: no)? My goal here was to answer these types of questions by building the best possible starting fives based on NBA players who share a first name. Shea Serrano from the Ringer did the same thing a few years ago, but here, I use a more statistical approach than he did, and slightly different rules. The rules/system I used are:\n\rFor each player, I selected the season that they accomplished their highest win share per 48 minutes AND played at least 1,000 minutes.\rThe best starting fives are those that had the highest combined win shares per 48.\rEach team must include two guards, two forwards, and one center. This meant that, for example, Laker great and guard Michael Cooper was not on team Michael, because Michael Redd’s best season was better than Cooper’s best season, and you already know who got the other guard spot on team Michael.\rI used names exactly as they appear in Basketball Reference (e.g., David Robinson counts only as a David, not as a Dave).\rThe Basketball Reference data spans from the 1950-1951 season to the 2019-2020 (heretofore) abbreviated season, so every player from that period can be included.\r\rStep 1: Get the Data from Basketball Reference\rlibrary(tidyverse)\rlibrary(nbastatR)\rlibrary(ggrepel)\rlibrary(DT)\r# get data from BREF\rws \u0026lt;- bref_players_stats(seasons = 1951:2020, tables = c(\u0026quot;advanced\u0026quot;, \u0026quot;totals\u0026quot;))\r\rStep 2: Clean Data\r# select variables and filter for min 1,000 minutes\rws \u0026lt;- ws %\u0026gt;% filter(minutes \u0026gt;= 1000) %\u0026gt;% select(year = yearSeason,\rplayer = namePlayer, id = idPlayerNBA, position = groupPosition,\rwin_share = ratioWSPer48) # give first name its own column\rws \u0026lt;- ws %\u0026gt;% separate(player, into = c(\u0026quot;first\u0026quot;, \u0026quot;last\u0026quot;), sep = \u0026quot; \u0026quot;,\rremove = FALSE) %\u0026gt;% mutate(first = str_trim(first, side = \u0026quot;both\u0026quot;))\r\rStep 3: Select Each Player’s Best Season\r# select each player\u0026#39;s best season.\r# michael cage tied for his best seasons at forward and center. # so I decided to select him as a center.\r# the next best michael center was doleac.\rbest_season \u0026lt;- ws %\u0026gt;% group_by(player, id) %\u0026gt;%\rfilter(win_share == max(win_share)) %\u0026gt;% ungroup() %\u0026gt;% select(-year) %\u0026gt;% unique() %\u0026gt;% filter(id != 262 | position == \u0026quot;F\u0026quot;)\r\rStep 4: Build Starting Fives\rbest_season \u0026lt;- best_season %\u0026gt;% group_by(first, position) %\u0026gt;% mutate(rank = rank(-win_share)) %\u0026gt;% ungroup() best_season \u0026lt;- best_season %\u0026gt;% filter( (position %in% c(\u0026quot;F\u0026quot;, \u0026quot;G\u0026quot;) \u0026amp; rank %in% c(1, 2) |\rposition == \u0026quot;C\u0026quot; \u0026amp; rank == 1) ) %\u0026gt;% add_count(first) %\u0026gt;% filter(n == 5)\rbest_names \u0026lt;- best_season %\u0026gt;% group_by(first) %\u0026gt;% mutate(total = sum(win_share)) %\u0026gt;% ungroup() \r\rDrumroll…Results\rLike Shea in his piece, I concluded that you can not build a better team than a team made up Kevins. Here are the top 20 first name teams:\nbest_names %\u0026gt;% distinct(first, total) %\u0026gt;% mutate(first = fct_reorder(first, total)) %\u0026gt;% arrange(-total) %\u0026gt;% slice(1:20) %\u0026gt;% ggplot(aes(x = total, y = first,\rfill = total)) +\rgeom_col() +\rguides(fill = FALSE) +\rscale_fill_gradient(low = \u0026quot;yellow\u0026quot;, high = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Win Share / 48\u0026quot;, y = NULL,\rtitle = \u0026quot;Which NBA First Name Creates the Best Starting Five?\u0026quot;,\rsubtitle = \u0026quot;Teams must include two guards, two forwards, and one center.\u0026quot;,\rcaption = \u0026quot;Data is from Basketball Reference, and spans 1951 to 2020 seasons.\\nCreated by Arie Spirgel\u0026quot;) \rAnd here is my complete list of starting fives:\ndt \u0026lt;- best_names %\u0026gt;% select(`First Name` = first, last, `Combined WS / 48` = total) %\u0026gt;% group_by(`First Name`) %\u0026gt;% mutate(rank = rank(last, ties.method = \u0026quot;first\u0026quot;)) %\u0026gt;% spread(rank, last) %\u0026gt;% arrange(-`Combined WS / 48`) %\u0026gt;% datatable(filter = \u0026quot;top\u0026quot;, rownames = FALSE)\rwidgetframe::frameWidget(dt)\r\r{\"x\":{\"url\":\"/post/nba-first-names/nba-first-names_files/figure-html//widgets/widget_unnamed-chunk-8.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]}\r\rMy (Not So Statistical) Thoughts on a Hypothetical Season\rIf you were building a starting five of any NBA players, regardless of first name, you could not do much better than Kevin Garnett and Kevin Durant as your forwards. The guards on the Kevins are Johnson and Martin, who would complement the forwards well. Johnson was an explosive point-guard who averaged 17.9 points and 9.1 assists for his 12-year NBA career. And Kevin Martin’s shooting would space the floor, helping Garnett and Durant further dominate in all of the ways that they do.\nAfter Kevin, my list diverged from Shea’s, in large part because of the different rules we used. He had the Michaels second, but they were 11th on my list. This is of course silly, because prime MJ with I-don’t-care-who would never finish 11th in any actual basketball tournament. We both had the Larrys and Chrises highly ranked, but the Georges and Bobs were third and fourth on my list, who Shea respectively doesn’t mention and thinks they would be fun to watch but wouldn’t win a game.\nSo, smart money is on the Kevins, but they’d be lucky to avoid a match-up with the Davids, whose frontcourt of Robinson, Lee and West, at a combined 20.5 feet, is more Goliath than David. In addition to Robinson, the Davids also have a Hall of Famer in the backcourt with Thompson. The Davids would be physical to play against and fun to watch, but like everyone else, likely wouldn’t be able to keep up with the Kevins.\nThe James team would also be a tough out. Harden would love to be surrounded by shooters like Posey and Jones and would thrive on the break with Worthy. But in this alternate reality, if Worthy had still spent his career playing with Magic Johnson - one of the most unselfish players ever - he would struggle watching Harden dribble around for 18 seconds before launching isolation step-back threes. That said, with the right coaching and the right sacrifices, this team is a dark horse.\nTim Duncan’s teams are always good (because he is Tim Duncan). But then, put him in the two-man game with five-time all-star Tim Hardaway and have Legler spot up in the corner, and this team is closer to great. If their other forward - Tim Thomas - were to heed Duncan’s mentorship, perhaps his career would reach greater heights and he could help to earn the Tims a finals bout with the Kevins.\nThe most fun team to watch would be the Chrises, who in Paul, Mullin, and Webber, have three of the more creative players, and in Anderson, one of the most expressive. Play-by-play announcers would regularly applaud their ball movement and the extra pass, and if they stayed healthy, could be a problem for more heavily favored teams. Either way, this team is a League Pass fan-favorite.\nThe Derricks have two big-name stars in Coleman and Rose, and a solid supporting cost in White, Jones Jr., and Favors. Loyal fans would go into the season with high hopes and championship aspirations, but ultimately, this team doesn’t have the star-power to get very far.\nSome other notable teams. With Barkley and Oakley, the Charles team would undoubtedly lead the league in technicals. The Steves would be one of the best shooting teams - led by Nash, Kerr, and Novack - but with such a small backcourt, would not be able to compete. The all-time top Dannys might not immediately come to mind (Ainge, Granger, Green, Manning, and Fortson), but they are a good shooting team with size and experience, and whose whole is greater than the sum of its parts.\nAny discussion of all-time NBA greats begins with Michael, LeBron, Bill Russell, Magic, and Kareem. Except this one, that is contrived and based on a hypothetical world. In this scenario, which is based on made-up rules and could only exist if traveling through time were possible, it is the Kevins who come out on top. Congratulations to them!\n\r","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590537600,"objectID":"c24d4df6642f78714d0b04b6ee4ff03e","permalink":"https://arie.rbind.io/post/nba-first-names/nba-first-names/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/post/nba-first-names/nba-first-names/","section":"post","summary":"Michael. Larry. Moses. Mention these names to the most casual of NBA fans, and they know that you mean Jordan, Bird, and Malone, and not Doleac, Siegfried, and Brown. Even among other highly accomplished individuals, the all-time greats are so transcendent that they are recognized by their first names alone.\nWe all agree who the best Michael is, but who are the other great (or good) Michaels?","tags":["ggplot2"],"title":"Which NBA First Name Creates the Best Starting Five?","type":"post"},{"authors":["arie"],"categories":["education"],"content":"\rIf you’ve downloaded enough data from the IPEDS Data Center using the “Compare Institutions” interface, you’ve probably realized that, depending on what you’re downloading, the data provided is rarely in a format ready for analysis. Here, via a specific example, I describe what makes the IPEDS data format impractical, and how to use R to resolve that.\nReading in the Data\rI first downloaded Fall 2012 to Fall 2018 distance education headcounts for every college and university in the IPEDS Data Center. In this first section, I read in the data, and display a subset of what the full data set looks like.\nlibrary(tidyverse)\rlibrary(scales)\rtheme_set(theme_light())\rdistance \u0026lt;- read_csv(\u0026quot;raw-data/distance-fall-12-18.csv\u0026quot;)\rThe data set contains 6,800 rows and 43 columns, and ignoring the Institution Name column, each of the remaining columns is some version of the following: Students enrolled exclusively in distance education courses (EF2018A_DIST Undergraduate total). Under that specific column, for each of the 6,800 institutions that reported data, are headcounts for exclusively distance undergraduate students in the fall term of 2018. The problem, thus, is that this column (and all the other ones like it) actually contains three pieces of information:\nLevel, which can take on the values undergraduate or graduate.\rModality, which can take on the values exlusively distance, some distance, or no distance.\rYear, which can take on any integer value from 2012 to 2018.\r\rThis untidy format is exactly what makes IPEDS data tricky to work with. In contrast, tidy data - which means each variable is in its own column, each observation is in its own row, and each value is in its own cell1 - is advantageous not just for working with data in R, but other software as well (e.g., pivot tables in Excel).\n\rTidying the Data\rThe first step to tidying this data is to pivot it so that all of the column names that contain the type of headcount are in one column, and the actual headcounts are in a different column. To do that, I use the gather()2 function. I first provide gather() with the names of the two new variables that are being created - I call them variable and headcount, but they can be called anything you want - and then which columns I want pivoted from wide to long; here, I pivot everything from the 2nd column to the last column of the data set.\ndistance \u0026lt;- distance %\u0026gt;% gather(variable, headcount, 2:ncol(.))\rdistance\r## # A tibble: 285,600 x 3\r## `Institution Name` variable headcount\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Educational Technical College… Students enrolled exclusively in di… NA\r## 2 A T Still University of Healt… Students enrolled exclusively in di… NA\r## 3 Aaniiih Nakoda College Students enrolled exclusively in di… NA\r## 4 ABC Adult School Students enrolled exclusively in di… NA\r## 5 ABC Beauty Academy Students enrolled exclusively in di… NA\r## 6 ABCO Technology Students enrolled exclusively in di… NA\r## 7 Abcott Institute Students enrolled exclusively in di… NA\r## 8 Abdill Career College Inc Students enrolled exclusively in di… NA\r## 9 Abilene Christian University Students enrolled exclusively in di… 32\r## 10 Abraham Baldwin Agricultural … Students enrolled exclusively in di… 377\r## # … with 285,590 more rows\rAs you can see above, the data set now only has three columns, not 43. Same data, different layout. Looks better already, right?!?\nWe’re not done though. Remember, each row of the variable column contains three pieces of information: level, modality, and year. So for the next three steps I split that column apart so each of these variables are in their own column. First, I’ll make a new column for level.\nThere are countless ways of reaching the same endpoint in R, and in this instance, I use str_detect() to tell R to put “Undergraduate” in the level column if it detects the string “Undergraduate” in the variable column, and then perform the analogous task for “Graduate”.\ndistance \u0026lt;- distance %\u0026gt;% mutate(level = case_when(\rstr_detect(variable, \u0026quot;Undergraduate\u0026quot;) ~ \u0026quot;Undergraduate\u0026quot;,\rstr_detect(variable, \u0026quot;Graduate\u0026quot;) ~ \u0026quot;Graduate\u0026quot;))\rdistance\r## # A tibble: 285,600 x 4\r## `Institution Name` variable headcount level ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Educational Technical Coll… Students enrolled exclusively … NA Underg…\r## 2 A T Still University of He… Students enrolled exclusively … NA Underg…\r## 3 Aaniiih Nakoda College Students enrolled exclusively … NA Underg…\r## 4 ABC Adult School Students enrolled exclusively … NA Underg…\r## 5 ABC Beauty Academy Students enrolled exclusively … NA Underg…\r## 6 ABCO Technology Students enrolled exclusively … NA Underg…\r## 7 Abcott Institute Students enrolled exclusively … NA Underg…\r## 8 Abdill Career College Inc Students enrolled exclusively … NA Underg…\r## 9 Abilene Christian Universi… Students enrolled exclusively … 32 Underg…\r## 10 Abraham Baldwin Agricultur… Students enrolled exclusively … 377 Underg…\r## # … with 285,590 more rows\rSee the new column on the end with level?\nNext I do the same thing for modality: I tell R to look for specific strings, and make a new column based on those strings.\ndistance \u0026lt;- distance %\u0026gt;% mutate(modality = case_when(\rstr_detect(variable, \u0026quot;not enrolled in any\u0026quot;) ~ \u0026quot;No Distance\u0026quot;,\rstr_detect(variable, \u0026quot;in some\u0026quot;) ~ \u0026quot;Some Distance\u0026quot;,\rstr_detect(variable, \u0026quot;exclusively\u0026quot;) ~ \u0026quot;Exclusively Distance\u0026quot;))\rdistance\r## # A tibble: 285,600 x 5\r## `Institution Name` variable headcount level modality ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Educational Technical C… Students enrolled exclu… NA Under… Exclusive…\r## 2 A T Still University of… Students enrolled exclu… NA Under… Exclusive…\r## 3 Aaniiih Nakoda College Students enrolled exclu… NA Under… Exclusive…\r## 4 ABC Adult School Students enrolled exclu… NA Under… Exclusive…\r## 5 ABC Beauty Academy Students enrolled exclu… NA Under… Exclusive…\r## 6 ABCO Technology Students enrolled exclu… NA Under… Exclusive…\r## 7 Abcott Institute Students enrolled exclu… NA Under… Exclusive…\r## 8 Abdill Career College I… Students enrolled exclu… NA Under… Exclusive…\r## 9 Abilene Christian Unive… Students enrolled exclu… 32 Under… Exclusive…\r## 10 Abraham Baldwin Agricul… Students enrolled exclu… 377 Under… Exclusive…\r## # … with 285,590 more rows\rThe last step of tidying is to get year in its own column. I could tell R to make a new variable and put “2012” if it detects “2012”, “2013” if it detects “2013”, and so-on, but there is a much simpler way: the parse_number() function, which drops any non-numeric characters from a string.\ndistance \u0026lt;- distance %\u0026gt;% mutate(year = parse_number(variable))\rThe tidying is now done, and so although this next step isn’t necessary, renaming and reordering the variables and factor levels will make the data easier to work with.\n# rename columns, reorder factor levels (e.g., Undergraduate before Graduate)\rdistance \u0026lt;- distance %\u0026gt;%\rselect(institution_name = `Institution Name`, level,\rmodality, year, headcount) %\u0026gt;% mutate(level = fct_relevel(level, \u0026quot;Undergraduate\u0026quot;),\rmodality = fct_relevel(modality, \u0026quot;Some Distance\u0026quot;)) distance\r## # A tibble: 285,600 x 5\r## institution_name level modality year headcount\r## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Educational Technical College-Reci… Undergrad… Exclusively D… 2018 NA\r## 2 A T Still University of Health Sci… Undergrad… Exclusively D… 2018 NA\r## 3 Aaniiih Nakoda College Undergrad… Exclusively D… 2018 NA\r## 4 ABC Adult School Undergrad… Exclusively D… 2018 NA\r## 5 ABC Beauty Academy Undergrad… Exclusively D… 2018 NA\r## 6 ABCO Technology Undergrad… Exclusively D… 2018 NA\r## 7 Abcott Institute Undergrad… Exclusively D… 2018 NA\r## 8 Abdill Career College Inc Undergrad… Exclusively D… 2018 NA\r## 9 Abilene Christian University Undergrad… Exclusively D… 2018 32\r## 10 Abraham Baldwin Agricultural Colle… Undergrad… Exclusively D… 2018 377\r## # … with 285,590 more rows\rBehold, tidy data!\n\rVisualizing the Data\rWith the data in a tidy format you can now do…pretty much whatever you want with it! In the examples below, I chose to visualize it, which demonstrates how - thanks to tidy data(!) - you can recycle the same code with slight alterations to make different plots. First, here are overall trends in distance education.\ndistance %\u0026gt;% mutate(headcount = replace_na(headcount, 0)) %\u0026gt;% group_by(year, modality) %\u0026gt;% summarise(total = sum(headcount)) %\u0026gt;% group_by(year) %\u0026gt;% mutate(prop = total / sum(total)) %\u0026gt;% ungroup() %\u0026gt;% filter(modality != \u0026quot;No Distance\u0026quot;) %\u0026gt;% ggplot(aes(x = factor(year), y = prop, fill = modality)) +\rgeom_col() +\rscale_y_continuous(label = percent_format()) +\rtheme(legend.position = \u0026quot;top\u0026quot;) +\rlabs(x = \u0026quot;Fall Term\u0026quot;, y = \u0026quot;% of Students\u0026quot;,\rtitle = \u0026quot;Percentage of Students Enrolled in Distance Education\u0026quot;,\rfill = NULL,\rsubtitle = \u0026quot;Fall 2012 to Fall 2018\u0026quot;,\rcaption = \u0026quot;Source: IPEDS Data Center\u0026quot;)\rNext, I change the grouping variables to repeat the same chart except here I partition the data by level.\ndistance %\u0026gt;% mutate(headcount = replace_na(headcount, 0)) %\u0026gt;% group_by(year, modality, level) %\u0026gt;% summarise(total = sum(headcount)) %\u0026gt;% group_by(year, level) %\u0026gt;% mutate(prop = total / sum(total)) %\u0026gt;% ungroup() %\u0026gt;% filter(modality != \u0026quot;No Distance\u0026quot;) %\u0026gt;% ggplot(aes(x = factor(year), y = prop, fill = modality)) +\rgeom_col() +\rfacet_wrap(~level) +\rscale_y_continuous(label = percent_format()) +\rtheme(legend.position = \u0026quot;top\u0026quot;) +\rlabs(x = \u0026quot;Fall Term\u0026quot;, y = \u0026quot;% of Students\u0026quot;,\rtitle = \u0026quot;Percentage of Students Enrolled in Distance Education\u0026quot;,\rfill = NULL,\rsubtitle = \u0026quot;Fall 2012 to Fall 2018\u0026quot;,\rcaption = \u0026quot;Source: IPEDS Data Center\u0026quot;)\rAnd once more, limiting the results to a single institution: Florida State University.\ndistance %\u0026gt;% filter(institution_name == \u0026quot;Florida State University\u0026quot;) %\u0026gt;% mutate(headcount = replace_na(headcount, 0)) %\u0026gt;% group_by(year, modality, level) %\u0026gt;% summarise(total = sum(headcount)) %\u0026gt;% group_by(year, level) %\u0026gt;% mutate(prop = total / sum(total)) %\u0026gt;% ungroup() %\u0026gt;% filter(modality != \u0026quot;No Distance\u0026quot;) %\u0026gt;% ggplot(aes(x = factor(year), y = prop, fill = modality)) +\rgeom_col() +\rfacet_wrap(~level) +\rscale_y_continuous(label = percent_format()) +\rtheme(legend.position = \u0026quot;top\u0026quot;) +\rlabs(x = \u0026quot;Fall Term\u0026quot;, y = \u0026quot;% of Students\u0026quot;,\rtitle = \u0026quot;Percentage of Florida State U. Students Enrolled in Distance Education\u0026quot;,\rfill = NULL,\rsubtitle = \u0026quot;Fall 2012 to Fall 2018\u0026quot;,\rcaption = \u0026quot;Source: IPEDS Data Center\u0026quot;)\r\rConclusion\rAmong its many benefits, tidy data lets you devote more attention to what you want to do rather than how you want to do it. Yes, tidying data takes longer at the start, but in the long-run, it will save you time. In that way, it’s just like learning R!\n\r\rhttps://r4ds.had.co.nz/tidy-data.html↩\n\rpivot_longer() is an updated version of gather().↩\n\r\r\r","date":1586217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586217600,"objectID":"192c5e91c87a2cb378e6b554f60a74e1","permalink":"https://arie.rbind.io/post/online-education/distance-education/","publishdate":"2020-04-07T00:00:00Z","relpermalink":"/post/online-education/distance-education/","section":"post","summary":"If you’ve downloaded enough data from the IPEDS Data Center using the “Compare Institutions” interface, you’ve probably realized that, depending on what you’re downloading, the data provided is rarely in a format ready for analysis. Here, via a specific example, I describe what makes the IPEDS data format impractical, and how to use R to resolve that.\nReading in the Data\rI first downloaded Fall 2012 to Fall 2018 distance education headcounts for every college and university in the IPEDS Data Center.","tags":["r","ipeds"],"title":"Tidying IPEDS Data in R","type":"post"},{"authors":["arie"],"categories":["workshop"],"content":"\rLearn how to visualize data using R and ggplot2!\nWhether you’re interested in data science, business, or working on a dissertation or research project, knowing how to visualize data is a vital skill. In this free, 2.5 hour workshop, you’ll be introduced to the R programming language and learn to visualize data using ggplot2. In the last 30 minutes of the workshop, Janine Morris from NSU’s Writing and Communication Center will talk about the features of effective data visualizations.\n","date":1584100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584100800,"objectID":"82742011de3a1cf9277098f94dea4f28","permalink":"https://arie.rbind.io/talk/2020-wcc-ggplot2/","publishdate":"2020-03-07T00:00:00Z","relpermalink":"/talk/2020-wcc-ggplot2/","section":"talk","summary":"Introduction to R and ggplot2","tags":["wcc","ggplot2"],"title":"POSTPONED DUE TO CORONAVIRUS Introduction to R and ggplot2","type":"talk"},{"authors":["arie"],"categories":["institutional research"],"content":"\rIntroduction\rI first heard about R when I was in graduate school in 2008 and fellow students used it to analyze their data. I didn’t bother to learn it at the time because, one, I didn’t see the benefit of it, and two, I assumed that without any programming experience, it was too difficult. So I continued with my same workflow: Clean data and make charts in Excel, import data into SPSS to analyze it, and then paste my output into a Word document and write up the results.\nI started working in institutional research in 2013 and I still hadn’t made the switch to R, but was beginning to see the drawbacks of my workflow and the upside of coding. I often had to generate the same reports on a regular basis where the only thing that would change was the data. Or I’d have to generate the same charts or tables for each of the 15 colleges at the university, and on bad days, each of the 150-something majors. This quickly became unsustainable when I would, for example, get one of these requests late on a Friday afternoon and had to have it ready for a Board meeting on Monday. R increasingly seemed like a preferable alternative.\nFast-forward 7 years and my SPSS license has long since expired, I don’t recall the last time I made a chart in Excel, and the only thing I use Word for is making grocery lists. Today, my entire workflow exists inside of R.\nIn the intervening years, I have frequently met other institutional researchers who are stuck in the same mindset I was in 2008: For people who have never coded, R seems too overwhelming to learn, and even if they were to learn it, they do not see the benefits of doing so. In future posts I plan to address the former, but in this series of posts I want to address the latter: What’s the point of learning R for institutional research? Rather than list all of the reasons why R is an excellent choice for doing institutional research, I want to show examples of how I use it. In this post, I’ll demonstrate the scenario of using R to run many models.\nIf you are not an R user, do not worry about the details of the code below, but instead, pay attention to what the code is capable of producing.\n\rRunning One Model\rWhether you want to predict future enrollment or explain why some students do not graduate, modeling is an important skill in institutional research. To show how to run a linear model in R, for all colleges and universities in the IPEDS Data Center, I downloaded their state, one year retention rates (i.e., the percent of first-time in college students who re-enroll their second fall term), student-faculty ratios, and the number of undergraduate applications they received for a given year. Here is the code for reading in the data and what the first five rows of data look like:\nlibrary(tidyverse)\rlibrary(broom)\rlibrary(drlib)\ripeds \u0026lt;- read_rds(\u0026quot;processed-data/ipeds-sfr.rds\u0026quot;)\rhead(ipeds, 5)\r## # A tibble: 5 x 5\r## name state undergrad_applic… retention_rate student_faculty_…\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Educational Techni… Puerto… NA 11 21\r## 2 A T Still Universi… Missou… NA NA NA\r## 3 Aaniiih Nakoda Col… Montana NA 34 10\r## 4 ABC Adult School Califo… NA NA 4\r## 5 ABC Beauty Academy Texas NA 25 10\rIn this contrived example, to build a linear model with retention rate as the outcome and student-faculty ratio and number of undergraduate applications as the predictors, I took the ipeds data frame, piped it (%\u0026gt;%) to the lm function, and then cleaned up the results with the tidy() function from the broom package. This gives us the model results:\nipeds %\u0026gt;% lm(retention_rate ~ student_faculty_ratio + undergrad_applicants, data = .) %\u0026gt;% tidy()\r## # A tibble: 3 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 76.2 0.868 87.8 0. ## 2 student_faculty_ratio -0.326 0.0589 -5.54 3.45e- 8\r## 3 undergrad_applicants 0.000504 0.0000315 16.0 3.03e-54\rAt this point you may be thinking, “So what? I can just easily do the same thing in SPSS, or even Excel”. That is true, but what if instead of running one model, you had to run 150?\n\rRunning Many Models\rAs part of our university’s strategic business plan, I recently had to create separate models for each of the 150-something majors at the school. If I were still using SPSS, this would mean:\ndays of pointing and clicking and copying and pasting.\rdoing the same thing over and over again each time the project requirements changed, which is an inevitability.\rhaving no documentation about the decisions I made because everything was done by pointing and clicking.\r\rReturning to the original data set, let’s say I wanted repeat the same model above, but separately for each state. Using R, I first filter the data to only include states with at least 50 schools (an arbitrarily chosen cutoff point):\nipeds \u0026lt;- ipeds %\u0026gt;% add_count(state) %\u0026gt;% filter(n \u0026gt;= 50)\rNext, I turn the model into a function:\nstate_regression \u0026lt;- function(df) {\rdf %\u0026gt;% lm(retention_rate ~ student_faculty_ratio + undergrad_applicants, data = .) }\rFrom there, I can apply the function to each state in the data set, which returns a data frame with the model results for each state:\nipeds_model \u0026lt;- ipeds %\u0026gt;% group_by(state) %\u0026gt;% nest() %\u0026gt;% mutate(model = map(data, state_regression),\rtidy_model = map(model, tidy)) %\u0026gt;% unnest(tidy_model) head(ipeds_model, 5)\r## # A tibble: 5 x 8\r## # Groups: state [2]\r## state data model term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;list\u0026lt;df[,5\u0026gt; \u0026lt;lis\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Puerto … [146 × 5] \u0026lt;lm\u0026gt; (Intercept) 87.5 5.80 15.1 2.48e-18\r## 2 Puerto … [146 × 5] \u0026lt;lm\u0026gt; student_fac… -0.869 0.315 -2.76 8.68e- 3\r## 3 Puerto … [146 × 5] \u0026lt;lm\u0026gt; undergrad_a… 0.00208 0.00140 1.49 1.45e- 1\r## 4 Missouri [171 × 5] \u0026lt;lm\u0026gt; (Intercept) 84.5 5.46 15.5 1.91e-22\r## 5 Missouri [171 × 5] \u0026lt;lm\u0026gt; student_fac… -1.07 0.458 -2.33 2.30e- 2\rNow, with a separate model for each state all in a data frame, I can treat the model output like I would any other data. For example, here, I visualize the model results for each state:\nipeds_model %\u0026gt;% filter(term != \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% mutate(term = if_else(term == \u0026quot;student_faculty_ratio\u0026quot;,\r\u0026quot;Student/Faculty Ratio\u0026quot;, \u0026quot;# of Undergraduate Applications\u0026quot;)) %\u0026gt;% ggplot(aes(x = reorder_within(state, -estimate, term),\ry = estimate,\rymin = estimate - (2 * std.error),\rymax = estimate + (2 * std.error))) +\rgeom_pointrange(color = \u0026quot;grey60\u0026quot;) +\rcoord_flip() +\rguides(color = FALSE) +\rfacet_wrap(~term, scales = \u0026quot;free\u0026quot;, ncol = 2) +\rtheme_classic() +\rscale_x_reordered() +\rgeom_hline(yintercept = 0, linetype = 2) +\rlabs(\rtitle = str_wrap(\u0026quot;Is First-Year Retention Associated with Student-Faulty Ratio and/or Undergraduate Applications?\u0026quot;, 75),\rsubtitle = \u0026quot;Limited to states with at least 50 schools\u0026quot;,\rcaption = \u0026quot;Source: IPEDS Data Center\u0026quot;,\rx = NULL, y = \u0026quot;Estimate\u0026quot;)\r\rConclusion\rClaiming that there is more friction to learning R than there is to learning menu-driven tools is like saying learning to microwave TV dinners is easier than learning to cook the same meal from scratch. Both points might be true, but they obscure the ultimate goals of each: R, like cooking, unconstrains you, giving you the freedom to create whatever fills your imagination. And whether it’s running models for 150 majors or making soup for a large dinner party, learning to code and learning to cook can make your work not only more tenable, but more enjoyable, and in the long-run, simpler.\n\r","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"63a339f6dc11d9c30374d3378a4280ed","permalink":"https://arie.rbind.io/post/ipeds-many-models/01-r-for-institutional-research/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/ipeds-many-models/01-r-for-institutional-research/","section":"post","summary":"Introduction\rI first heard about R when I was in graduate school in 2008 and fellow students used it to analyze their data. I didn’t bother to learn it at the time because, one, I didn’t see the benefit of it, and two, I assumed that without any programming experience, it was too difficult. So I continued with my same workflow: Clean data and make charts in Excel, import data into SPSS to analyze it, and then paste my output into a Word document and write up the results.","tags":["modeling","ipeds"],"title":"Why Use R for Institutional Research? Part 1, Many Models","type":"post"},{"authors":["arie"],"categories":["education"],"content":"\rThe Curse of Knowledge in Everyday Life\rSeveral years ago my friend Lauren asked me for my recipe for BBQ seitan. I love food-related conversation, so I wasted no time. “Start by sauteing some chopped onion in oil…”, and as quickly as I began, she cut me off. “Hold on,” she interjected. “What kind of oil do you use? How much? How high do you turn the heat?”\nDissecting the conversation, what happened was that I implicitly made the absurd assumption that knowledge that is in my head must be in hers (i.e., “Use however much of whatever oil you’d like to at whatever heat you normally saute”). In other words, I fell victim to the curse of knowledge. I’m not an expert cook - just ask my wife who always keeps the salt and pepper shaker within arm’s reach when I prepare a meal - but I did naively explain the recipe to Lauren as if she possessed my idiosyncratic definition of “saute”.\n\rThe Curse of Knowledge When Teaching R\rScenarios like this are universal, and most of the time, they are harmless. However, they can be frustrating when people have invested time and money to learn R from you. Even if you yourself are relatively to new to R, it is easy to take for granted all that you know and what it’s like to be a true beginner. Consider the following questions and confusion that a new R user might have when you ask them to do something as seemingly innocuous as running a line of read_csv() code you’ve provided:\n\r“I just bought a book that says to use read.csv(), but you use read_csv(). They are so similar they must do exactly the same thing, right?”\r“Excuse me, but are you saying tibble? Do you mean table?”\r“I tried running read_csv() but I got an error saying the function couldn’t be found. How does that make sense?”\r“I thought you said colons aren’t allowed in function names, so why did you write readr::read_csv()?”\r“The code you shared says read_csv(\u0026quot;raw-data/survey-results.csv\u0026quot;) but I changed the / to \\ because that’s what the folders look like on my computer and now it doesn’t work. WTF, right?!”\r\rNot all of your students will tell you when they’re stuck, and because you can’t read their minds, what are you to do? Ask them! Whether it’s a one day workshop or a semester long course, giving frequent, brief, assessments will help you identify areas of confusion and guide your lessons.\nYou might be thinking that when you have a large group of people, resolving every question that every student has is unrealistic. That may be true, but it is a shame when a student falls behind because an instructor misses an opportunity for a simple clarification. Consider the following (intentionally confusing) passage from Bransford and Johnson (1972):\n\rThe procedure is actually quite simple. First you arrange things into different groups… Of course, one pile may be sufficient depending on how much there is to do. If you have to go somewhere else due to lack of facilities that is the next step, otherwise you are pretty well set. It is important not to overdo any particular endeavor. That is, it is better to do too few things at once than too many. In the short run this may not seem important, but complications from doing too many can easily arise. A mistake can be expensive as well… At first the whole procedure will seem complicated. Soon, however, it will become just another facet of life. It is difficult to foresee any end to the necessity for this task in the immediate future, but then one never can tell. After the procedure is completed one arranges the materials into different groups again. Then they can be put into their appropriate places. Eventually they will be used once more and the whole cycle will have to be repeated. However, that is part of life. (Bransford and Johnson 1972 p. 722)\n\rIf you’ve never seen this passage before, it probably makes little sense to you and its details are unlikely to stick in your mind. But if before you read it I gave you the passage’s title - Washing Clothes - it would suddenly become much clearer. There are plenty of “washing clothes” examples in R, and as the instructor, it’s your job to construct an environment that helps you identify them.\n\rConclusion\rFrequent assessments will alert you when you succumb to the curse of knowledge and help you to correct your biases. If you’re teaching R - or for that matter, anything - and you’re not regularly checking in on what your students know and what their misconceptions are, it’s worth asking yourself what your goals are, because maximizing student understanding may not be one of them.\n\r","date":1581638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581638400,"objectID":"38d650ffa71cd3fd39244dcd1bb1e282","permalink":"https://arie.rbind.io/post/curse-of-knowledge/curse-of-knowledge/","publishdate":"2020-02-14T00:00:00Z","relpermalink":"/post/curse-of-knowledge/curse-of-knowledge/","section":"post","summary":"The Curse of Knowledge in Everyday Life\rSeveral years ago my friend Lauren asked me for my recipe for BBQ seitan. I love food-related conversation, so I wasted no time. “Start by sauteing some chopped onion in oil…”, and as quickly as I began, she cut me off. “Hold on,” she interjected. “What kind of oil do you use? How much? How high do you turn the heat?”\nDissecting the conversation, what happened was that I implicitly made the absurd assumption that knowledge that is in my head must be in hers (i.","tags":["teaching","R","cognitive bias"],"title":"Fellow R Instructors: Beware of the Curse of Knowledge!","type":"post"},{"authors":["arie"],"categories":["workshop"],"content":"\rPart 1 covers:\n\rIntroduction to R.\rVisualization with ggplot2.\rTranformation with dplyr.\r\rPart 2 covers:\n\rModeling with modelr and broom.\rReporting with rmarkdown.\r\r","date":1578999600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578999600,"objectID":"95025f13029bf1a1a3ed7d3dae93e944","permalink":"https://arie.rbind.io/talk/colorado-ed/","publishdate":"2020-01-14T11:00:00Z","relpermalink":"/talk/colorado-ed/","section":"talk","summary":"R basics, ggplot, and dplyr.","tags":["tidyverse"],"title":"Welcome to the Tidyverse","type":"talk"},{"authors":["arie"],"categories":["visualization"],"content":"\rWith ggplot2 - the ubiquitous tool for making plots in R - you can create beautiful data visualizations without doing much to the defaults. By applying the template below (see R for Data Science), adding a theme (e.g., theme_light()), and giving your chart custom labels, you can have a publication-ready visualization.\nggplot(data = \u0026lt;DATA\u0026gt;) + \u0026lt;GEOM_FUNCTION\u0026gt;(\rmapping = aes(\u0026lt;MAPPINGS\u0026gt;),\rstat = \u0026lt;STAT\u0026gt;, position = \u0026lt;POSITION\u0026gt;\r) +\r\u0026lt;COORDINATE_FUNCTION\u0026gt; +\r\u0026lt;FACET_FUNCTION\u0026gt;\rBut the more I pay attention to how people respond to visualizations, the more I realize how minor improvements can make a major difference. Like cooking, where adding a little bit of salt or giving a dish a few extra minutes in the oven can transform a meal from acceptable to outstanding, relatively small changes to a chart can do the same.\nTake the example below, in which one of Stephanie Evergreen’s clients started with the slide on the top, and she helped them create the one below it:\nBoth charts contain the same information, and even if you can’t express why, you just know the one on the bottom is better. Continuing with the cooking analogy, the charts are like two Mexican restaurants that use the same ingredients, but at one, the guacamole is fresher, the rice is more flavorful, and the tortillas are made in house.\nggplot2 was designed so users can build any plot that they can imagine, so as attractive as its defaults are, my goal with this series of posts is to venture beyond the minor design adjustments I typically make and learn to tweak charts from ordinary to outstanding. My first goal was to use ggplot2 to reproduce the chart that Dr. Evergreen created for her client, which based on her post, I think she did in Excel.\nlibrary(tidyverse)\rlibrary(scales)\rlibrary(cowplot)\rStep 0: Create the Dataset\rIn addition to creating the data set, I used fct_reorder() so the bars will appear in order from highest to lowest growth in the chart.\ngrowth \u0026lt;- tribble(\r~region, ~growth, ~group,\r\u0026quot;Large city, Midwest\u0026quot;, .2, \u0026quot;Midwest\u0026quot;,\r\u0026quot;Large city, East Coast\u0026quot;, .175, \u0026quot;Other\u0026quot;,\r\u0026quot;Medium city, South\u0026quot;, .165, \u0026quot;Other\u0026quot;,\r\u0026quot;Medium city, Midwest\u0026quot;, .165, \u0026quot;Midwest\u0026quot;,\r\u0026quot;Small city, Midwest\u0026quot;, .14, \u0026quot;Midwest\u0026quot;, \u0026quot;Large city, West coast\u0026quot;, .10, \u0026quot;Other\u0026quot;\r) %\u0026gt;% mutate(region = fct_reorder(region, growth)) \r\rStep 1: Make the Foundational Chart\rThis chart contains all of the information that the final chart contains, but it’s like the decent restaurant you’ll never return to: fine, but unmemorable. In this step, I also narrowed the width of the bars, a subtle alteration that improves the feel of the chart as it gets closer to the finished product.\n(p \u0026lt;- growth %\u0026gt;% ggplot(aes(x = region, y = growth, fill = group)) + geom_col(width = .7) + coord_flip() + labs(x = NULL, y = NULL, title = \u0026quot;Geographic growth dominated by Midwest.\u0026quot;,\rsubtitle = \u0026quot;5 year growth\u0026quot;,\rcaption = \u0026quot;Source: Our Smart Source 2015\u0026quot;))\r\rStep 2: Update the Colors and Remove the Legend\rEven if you like the default ggplot2 colors - which I very much do - putting a dark color next to a muted color does a better job of highlighting a conclusion you might want to draw attention to. And by juxtaposing the green and gray, in combination with the chart’s title, the legend becomes extraneous and can be removed.\n(p \u0026lt;- p +\rscale_fill_manual(values = c(\u0026quot;#4D643D\u0026quot;, \u0026quot;#D7DBDD\u0026quot;)) +\rguides(fill = FALSE))\r\rStep 3: White Background and Minimal Gridlines\rThe white background can be achieved using theme_minimal(), and removing all of the grid lines other than the major-x ones cleans up the look of the chart.\n(p \u0026lt;- p +\rtheme_minimal() +\rtheme(panel.grid.minor.x = element_blank(),\rpanel.grid.major.y = element_blank()))\r\rStep 4: Change Font and Convert Axis to %\rTaken together, using a lighter color for the axis text and bolding the labels, make the chart look more professional. It’s so easy to rely on ggplot2’s default font choices that it’s also easy to forget that changing them can give your chart a completely different look. In this step, I also expressed the axis as percents, which is more consistent with how people think about growth (and happens to look better).\n(p \u0026lt;- p +\rtheme(plot.title = element_text(face = \u0026quot;bold\u0026quot;),\rplot.subtitle = element_text(face = \u0026quot;bold\u0026quot;),\rplot.caption = element_text(color = \u0026quot;gray53\u0026quot;, hjust = -.15,\rface = \u0026quot;bold\u0026quot;),\raxis.text.y = element_text(color = \u0026quot;gray30\u0026quot;, face = \u0026quot;bold\u0026quot;),\raxis.text.x = element_text(color = \u0026quot;gray30\u0026quot;, face = \u0026quot;bold\u0026quot;)) +\rscale_y_continuous(label = percent_format(), limits = c(0, .25))) \r\rStep 5: Add an Icon\rThe logo - which I added using the cowplot package - is functionally unnecessary but aesthetically powerful. It is the cilantro garnish on top of your beans and rice: Doesn’t add much flavor, but makes you all the more eager to dig in!\nggdraw() +\rdraw_image(\u0026quot;us-map.png\u0026quot;,\ry = .85,\rx = 0.1,\rwidth = .15,\rheight = .15\r) +\rdraw_plot(p)\r\rConclusion\rggplot2 has become so popular that I rarely, if ever, search online for a question about it that hasn’t already been asked and answered. This task was no different, with each question I had (e.g., how do I add an icon?) quickly resolved with a StackOverflow solution.\nSmall adjustments make a big difference, and the beauty of ggplot2 is that those adjustments are not only possible, but the knowledge to accomplish them is accessible (RStudio Community, R for Data Science, etc.). Stay tuned for more tricks on how you can make your ggplots more interpretable, compelling, and visually satisfying!\n\r","date":1578182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578182400,"objectID":"207a70b6862cfab3d34453cccc015a34","permalink":"https://arie.rbind.io/post/ggplotredo1/geographic-growth-midwest/","publishdate":"2020-01-05T00:00:00Z","relpermalink":"/post/ggplotredo1/geographic-growth-midwest/","section":"post","summary":"With ggplot2 - the ubiquitous tool for making plots in R - you can create beautiful data visualizations without doing much to the defaults. By applying the template below (see R for Data Science), adding a theme (e.g., theme_light()), and giving your chart custom labels, you can have a publication-ready visualization.\nggplot(data = \u0026lt;DATA\u0026gt;) + \u0026lt;GEOM_FUNCTION\u0026gt;(\rmapping = aes(\u0026lt;MAPPINGS\u0026gt;),\rstat = \u0026lt;STAT\u0026gt;, position = \u0026lt;POSITION\u0026gt;\r) +\r\u0026lt;COORDINATE_FUNCTION\u0026gt; +\r\u0026lt;FACET_FUNCTION\u0026gt;\rBut the more I pay attention to how people respond to visualizations, the more I realize how minor improvements can make a major difference.","tags":["ggplot2"],"title":"ggplot2: Going Beyond the Defaults","type":"post"},{"authors":["arie"],"categories":["nba"],"content":"\rOn October 29, 2018, in a game against the Chicago Bulls, Klay Thompson attempted 24 three-pointers, making 14 of them; both marks set NBA records. A couple months after that his teammate Stephen Curry attempted 14 three pointers in a single half of a single NBA game, which tied an NBA record. That same game, the Golden State Warriors and Sacramento Kings made a combined 41 threes, a total that had never been reached before. Just this morning when I checked ESPN.com I learned that the previous night James Harden scored 61 points in Madison Square Garden, but maybe more shocking, he did it while attempting 20 three-point shots (he only made five of them).\nFor anyone who has been paying attention to the NBA for more than a few years, these numbers look like typos. The NBA began using the three-point line in 1979, and on October 12 of that year - almost 40 years ago - Chris Ford made the first one. For most of its history the three-point shot has been like that kitchen gadget you’ve always had but rarely used until someone showed you what a powerful tool it could be (immersion blender maybe?).\nIn the case of the NBA, that someone would be Houston Rockets General Manager Daryl Morey, who wisely drew attention to math and the reality that three is worth more than two. More specifically, his insight was that the shots most worth taking are higher percentage attempts near the basket (layups and dunks, ideally), or lower percentage ones that are further from the basket but are worth three points (free throws are also an important part of the equation in so-called Morey-Ball). This means that if a team’s goal is to get the most points out of every position (and that is every team’s goal), mid-range shots should be discouraged: They are harder to make than layups and dunks and still worth only two points.\nThis has had a profound effect on how games are played and which players are valued. It wasn’t long ago that big men were expected to be able to post up near the basket and wouldn’t have to think about life outside the three-point arc. In 2019, a few games past the midpoint of the current season, Brook Lopez has already attempted 304 threes, Karl-Anthony Towns attempted 217, Marc Gasol attempted 193, Joel Embid attempted 178. Each of these players is listed as a center and is at least 7’0 feet tall. And the list goes on.\nAnd it’s not just who is shooting threes, it’s how they’re being shot. James Harden is setting new records for the number of unassisted threes made, meaning he doesn’t just shoot from deep as a compromise when better shots are not available, but it’s as if any shot that is not a three is a compromise. Or consider Curry, for whom the three-point line itself is merely a suggestion; he has made 45 of 94 threes (47.9%!) launched from between 30 and 35 feet since 2014-2015 (the line is 23 ft 9 inches from the basket, and 22 feet if you are standing in the corner).\nIn 2019, you don’t have to watch games closely to notice the abundance of three pointers that are being shot. But still, I wanted to attach some numbers to the obvious, so below, I present a brief quantitative history of NBA three-point attempts.\nPercentage of All Shot Attempts That Were Threes\rIt’s easy to forget how recent the explosion in three-point attempts has been. The chart below, which displays the percentage of all shot attempts that were threes, shows a gradual increase from 1998 to 2007. But from 2008 until 2012, the numbers leveled off, and it seemed peak-three had been reached. Then 2014 to 2018 happened, and the increase during that time period alone (8%) was greater than it was from all of 1998 to 2012 (7%)\nAlthough it’s a bit of an aside, despite the increase in the number of attempts, the percentage of three-pointers that have been made has remained relatively steady, as displayed below.\n\rDistribution of Threes\rAs I mentioned above, three-point shots are no longer limited to certain positions, a point that is reflected in the histogram below. It shows players’ average number of three-point attempts per game along the x-axis, and the number of players falling within that range for the season is represented along the y-axis. Over time, notice how the distribution gets flatter. In 1998, the group who shot between zero and 0.5 threes per game appeared with the most frequency. But by 2018 that bar dropped significantly and the other bars lifted up. And pay attention to right side of the chart beginning in 2016.\n\rTop Attempters by Year\rWho are these gunners that are changing how basketball is played? The chart below displays the three players who averaged the most threes per game for each season from 1998 to 2018. Even the casual fan will not be surprised by the more recent appearances on the chart: Splash-Brothers Curry and Thompson; and Gordon and Harden, whose GM is the aforementioned Daryl Morey.\n\rCurry Explains it All\rDaryl Morey’s influence on the game and the state of the evolution of the three-point shot is encapsulated by the shot charts below, which I downloaded using Todd Schneider’s ballr package. The chart on the left is Stephen Curry’s shot chart from his 2009-2010 rookie season, and the shot chart on the right is from his 2017-2018 season. It’s almost as if during that time the in-between game disappeared, leaving only three point shots and shots at the basket. That, folks, is modern basketball.\n\rConclusion\rThe new style of play is not for everyone. Like any trend, this one can’t go on forever, but I do wonder how far it can go until it reaches a breaking point. Maybe the breaking point comes from the league if they decide to move the line further back or establish a four-point line. Or maybe it comes from the next Daryl Morey who discovers an inefficiency in the way the game is currently played. Either way, enjoy the show, because there is nothing quite like watching Stephen Curry casually make game winning shots from close to half court!\n\r","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548288000,"objectID":"fad8752f3dec90a67908f0849183840c","permalink":"https://arie.rbind.io/post/history-3s/history-of-basketball/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/post/history-3s/history-of-basketball/","section":"post","summary":"On October 29, 2018, in a game against the Chicago Bulls, Klay Thompson attempted 24 three-pointers, making 14 of them; both marks set NBA records. A couple months after that his teammate Stephen Curry attempted 14 three pointers in a single half of a single NBA game, which tied an NBA record. That same game, the Golden State Warriors and Sacramento Kings made a combined 41 threes, a total that had never been reached before.","tags":["ggplot2","gganimate"],"title":"A Brief History of NBA Three-Point Attempts: 1998 to 2018 Seasons","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ba63ea3d55678df7fe7c8a43f1e2f005","permalink":"https://arie.rbind.io/list/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/list/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":["arie"],"categories":["education"],"content":"\rThe chief administrator job of a higher education institution is, as the title implies, the pinnacle of careers in academic administration. The chief administrator is a school’s spokesperson and guides its vision, affecting the lives of the thousands of students who pass through those institutions. And for the chief administrators who don’t care about the idealism of educating future generations, I would imagine the high-six/low-seven figure salaries that many earn is incentive enough. Despite the prominent roles that these administrators fill, there is a dearth of publicly available data on them.\nEvery institution that participates in federal student financial aid is required to submit data to Integrated Postsecondary Education System, or IPEDS. Eventually, the submitted data is made publicly available in the IPEDS Data Center. This includes numbers on admissions, student enrollment, degree completions, graduation rates, financial aid, finances, human resources, and libraries. Much of this data has to be reported by gender and race/ethnicity. For example, how many Hispanic female undergraduates began at an institution last fall? What is the graduation rate of male American Indian Alaska Natives? What is the average salary of female instructional staff on a 9-month contract? But for one reason or another, as far as I can tell, the only things that must be reported about an institution’s chief administrator are her or his name and title.\nNames, though, are not completely devoid of meaningful information. If you live in the United States and hear the name Steven, you probably think of a male, and if you hear the name Mary, you probably think of a female. Yes, some names are more ambiguous than others (my own being a good example), and some people’s names might belie the gender they identify with, but there is a degree of reliability that a person’s name offers in determining whether they are female or male. Thus, using first names to make educated guesses about chief administrators’ gender, my goal here was to describe gender representation among this set of individuals.\nThe first step was to download the names of the chief administrators for every institution in the IPEDS Data Center (n = 7108) and clean up the data. This required putting years in a consistent format; removing titles preceding first names (e.g., Dr., Ms., Mrs.); extracting first names into their own column; and adding variable labels for college sector.\nlibrary(tidyverse)\rlibrary(babynames)\rlibrary(scales)\radmin \u0026lt;- read_csv(\u0026quot;data/chief-admin-names.csv\u0026quot;)\r# tidy year admin \u0026lt;- admin %\u0026gt;% gather(\u0026quot;year\u0026quot;, \u0026quot;name\u0026quot;, `Name of chief administrator (HD2016)`,\r`Name of chief administrator (HD2015)`:`Name of Chief Administrator (IC90HD)`) # extract year from names and put in consistent format\radmin \u0026lt;- admin %\u0026gt;% mutate(year = parse_number(year),\ryear = ifelse(year \u0026gt;= 9596 \u0026amp; year \u0026lt;= 9798,\rstr_sub(start = 1, end = 2, year), year),\ryear = ifelse(nchar(year) == 2, paste0(\u0026quot;19\u0026quot;, year), year),\ryear = as.integer(year))\r# select and rename variables\radmin \u0026lt;- admin %\u0026gt;% select(unit_id = UnitID, sector_code = `Sector of institution (HD2016)`,\rinstitution_name = `Institution Name`,\ryear, name,\rundergrad_enroll_2016 = `Grand total (EF2016 All students Undergraduate total)`,\rgrad_enroll_2016 = `Grand total (EF2016 All students Graduate and First professional)`)\r# titles are in the first position for many names, so need to remove\r# those so can extract first posistion from names and have it reflect\r# first name\radmin \u0026lt;- admin %\u0026gt;% mutate(name = tolower(name),\rname = gsub(\u0026quot;\\\\.\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;dr\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;mr\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;ms\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;mrs\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;rev\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;reverend\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;very reverend\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;very\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;rabbi\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;msgr\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;dra\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = gsub(\u0026quot;\\\\\u0026lt;sr\\\\\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, name),\rname = str_trim(name, side = \u0026quot;both\u0026quot;),\rfull_name = tolower(name)) %\u0026gt;% separate(name, into = \u0026quot;first_name\u0026quot;, sep = \u0026quot; \u0026quot;)\r# add sector label\rlabels \u0026lt;- read_csv(\u0026quot;data/sector-value-labels.csv\u0026quot;) %\u0026gt;% select(sector_code = Value, sector_label = ValueLabel)\radmin \u0026lt;- left_join(admin, labels, by = \u0026quot;sector_code\u0026quot;)\rNext, I assigned a gender to each chief administrator based on her or his first name. To do this, I used R’s babynames package, which contains the number of babies born every year from 1880 to 2015 for each combination of name and sex1. I took the babynames data and calculated how often each name was given to females vs. males, and then assigned gender based on the higher proportion. For example, in the babynames data, about 72% of all newborns named Jaydin were male, so I assigned the name Jaydin to male. Names like Jaydin, however, were the exception: Most of the time, names went overwhelmingly to one sex or the other, with the vast majority of names in the babynames dataset being associated with only one sex.\n# read in babynames data and assign a proportion to female and male for each name\rbaby_names \u0026lt;- babynames %\u0026gt;% select(sex, name, n) %\u0026gt;% mutate(name = tolower(name))\rbaby_names \u0026lt;- baby_names %\u0026gt;% group_by(sex, name) %\u0026gt;% summarise(total = sum(n)) %\u0026gt;% ungroup() %\u0026gt;% group_by(name) %\u0026gt;% mutate(prop = total/sum(total)) %\u0026gt;%\rfilter(prop == max(prop)) %\u0026gt;% ungroup() %\u0026gt;% select(sex, first_name = name, prop)\r# one chief admin officer has name gold and it happens to be one that is .5 prop,\r# so removed from dataset. (gold was given as full name, so i\u0026#39;m *guessing* this # is actually last name).\rbaby_names \u0026lt;- baby_names %\u0026gt;% filter(first_name != \u0026quot;gold\u0026quot; | prop != .5)\rHistorical Trends\rThis gave me a “dictionary” containing the probabilistic sex of 97,430 first names, which I then linked to the chief administrator data2, making it possible to examine historical trends in gender differences among chief administrators. (The babynames data is based on sex, but once I link it to adults’ names (i.e., the administrators), I make the (often wrong) assumption that names reflect gender. Also, of course, with this data, it is not possible to account for gender non-binary administrators).\n# join ipeds and babynames, removing rows where there were no matches\radmin \u0026lt;- left_join(admin, baby_names, by = \u0026quot;first_name\u0026quot;) %\u0026gt;% mutate(institution_name = gsub(\u0026quot;-\u0026quot;, \u0026quot; \u0026quot;, institution_name)) %\u0026gt;% filter(!is.na(sex))\radmin \u0026lt;- admin %\u0026gt;% select(unit_id, institution_name, sector_label, year, undergrad_enroll_2016, grad_enroll_2016, full_name, first_name, sex)\r# several variables are for 2016 only, so put those in their own data frame\radmin_2016 \u0026lt;- admin %\u0026gt;% filter(year == 2016) %\u0026gt;% select(-year)\radmin \u0026lt;- admin %\u0026gt;% select(-undergrad_enroll_2016, -grad_enroll_2016)\r# proportion female by year\rfemale_prop_sex \u0026lt;- admin %\u0026gt;% count(year, sex) %\u0026gt;%\rgroup_by(year) %\u0026gt;% mutate(year_total = sum(n)) %\u0026gt;% ungroup() %\u0026gt;% mutate(prop_sex = n/year_total) %\u0026gt;% filter(sex == \u0026quot;F\u0026quot;)\rfemale_prop_sex %\u0026gt;% ggplot(aes(x = factor(year), y = prop_sex, group = 1)) +\rgeom_line() +\rgeom_point() +\rtheme_minimal() +\rtheme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\rscale_y_continuous(label = percent_format()) +\rlabs(x = \u0026quot;Year\u0026quot;, y = \u0026quot;Percent Women\u0026quot;,\rtitle = \u0026quot;Percentage of Chief Administrators Who Are Women\u0026quot;,\rsubtitle = \u0026quot;All IPEDS Institutions, Years 1990-2016\u0026quot;)\rThere are three things to notice about the chart:\nThe increasing percentage of chief administrators who are women3.\rThe still minority percentage of chief administrators who are women\rThe 35% figure I came up with for 2016 is roughly consistent with a survey reporting that 30% of 2016 college presidents were women), and compatible with the trends reported here, lending some support to the approach I selected.\r\r\rInstitution Type\rSchool, as it’s used in IPEDS, is a broad term that covers vastly different types of institutions. It includes everything from a cosmetology school that enrolls a handful of students each year to flagship schools with billion-plus endowments and tens of thousands of students. That is to say, chief administrator positions vary in prestige, responsibility, salary, and a host of other intangibles. With that in mind, I calculated the percentage of female chief administrators, by sector.\n# proportion female by sector\rfemale_by_sector \u0026lt;- admin_2016 %\u0026gt;% count(sector_label, sex) %\u0026gt;% group_by(sector_label) %\u0026gt;% mutate(prop_sex = n/sum(n)) %\u0026gt;% ungroup() %\u0026gt;% filter(sex == \u0026quot;F\u0026quot;)\rfemale_by_sector %\u0026gt;%\rggplot(aes(x = reorder(sector_label, prop_sex), y = prop_sex)) +\rgeom_col() +\rcoord_flip() +\rscale_y_continuous(label = percent_format()) +\rtheme_minimal() +\rlabs(x = NULL, y = \u0026quot;Percent Women\u0026quot;,\rtitle = \u0026quot;Percentage of Chief Aministrators Who Are Women,\\nby Sector\u0026quot;,\rsubtitle = \u0026quot;All IPEDS Institutions, Year 2016\u0026quot;)\rAlthough they are a minority in every sector, women make up a higher percentage of chief administrators at 2-year schools than at 4-year schools. One institution type is not better than another, but they serve different functions (e.g., teaching vs. research), meaning the disparities by sector further exaggerate the existing imbalance. For example, in 2016, 35% of academic chief administrators were women, yet the institutions they led accounted for only 29% of all students enrolled at institutions of higher education.\n\rConclusion\rThe overall trend is moving in the right direction, but change is slow: From 1990 to 2016, the average yearly increase in the percentage of women chief administrators was 1%. These are prestigious jobs that aren’t vacated haphazardly, so for the near-term, the disparity is here to stay: If the same rate of change observed from 1990 to 2016 continues, it will take 23 years until gender parity among chief administrators is achieved.\nThis is of course not a problem unique to higher education, but a societal one that begins well before women submit job applications. We must be aware of our biases - stop always telling your niece that she is pretty and her brother that he is smart! - however subtle they are, and correct them. You don’t have to be a woman to care about this, after all, “human rights are women’s rights”.\n\r\rThe data is restricted to combinations of five for more.↩\n\rI removed rows where there were no matches between first names in the IPEDS data and first names in the babynames data.↩\n\rYou may have noticed a slight dip from 2006 to 2009. Considering how abrupt it is, I’m skeptical it represents a real trend. My guess is that it is instead reflective of an unrelated change in the underlying data (e.g., different reporting requirements), but I’m not entirely sure.↩\n\r\r\r","date":1535068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535068800,"objectID":"12c4de63f37f591f7e194fe9ddaa5b1e","permalink":"https://arie.rbind.io/post/gender-college-presidents/gender-college-presidents/","publishdate":"2018-08-24T00:00:00Z","relpermalink":"/post/gender-college-presidents/gender-college-presidents/","section":"post","summary":"The chief administrator job of a higher education institution is, as the title implies, the pinnacle of careers in academic administration. The chief administrator is a school’s spokesperson and guides its vision, affecting the lives of the thousands of students who pass through those institutions. And for the chief administrators who don’t care about the idealism of educating future generations, I would imagine the high-six/low-seven figure salaries that many earn is incentive enough.","tags":["gender","ipeds"],"title":"An Analysis of Gender Disparity Among Higher Education Chief Administrators","type":"post"},{"authors":["arie"],"categories":["education"],"content":"\rData: It’s become a cliche to say that it’s everywhere and in quantities that are unimaginable. But data in its raw form, whether in a structured database or on the internet, is of limited use until a human does something to it: Gather it, clean it, visualize it, model it, write about it, and so-on.\nThe amount of data that those in institutional research encounter requires powerful tools to work with. And those tools exist. Lots of them. Everything from free, open-source software, to software costing hundreds of thousands dollars offered by companies that won’t stop emailing you despite unsubscribing from their list on a weekly basis.\nOn the rare occasion someone asks me which software I’d recommend, I always say R. In my experience, its ability to do everything you’d want and need to do as an institutional researcher is unmatched (cut to five minutes later when that someone regrets having asked me and is looking for ways to exit the conversation). But rather than use this space to drone on about why I think R is amazing, my goal here is to reveal the software preferences of others in the field. (And obviously, I used R to do this!)\nIt’s not a perfect approach, but my thinking was I could see how often different tools were mentioned in AIR Forum program books and how that’s changed over time. My first step was to download the program books from the forum website going back six years. Next, I created a function to read each of the programs books into R and tidy them so every individual word within each book is contained on its own line in a single data frame.\n# load packages\rlibrary(pdftools)\rlibrary(tidyverse)\rlibrary(tidytext)\rlibrary(scales)\r# function to read in PDFs and get one word per line.\rtext_prepr \u0026lt;- function(doc, forum_year){\rreg \u0026lt;- \u0026quot;([^A-Za-z\\\\d#@\u0026#39;]|\u0026#39;(?![A-Za-z\\\\d#@]))\u0026quot;\rdf \u0026lt;- pdf_text(doc)\rdf \u0026lt;- data.frame(df)\rdf \u0026lt;- df %\u0026gt;%\rrename(text = df) %\u0026gt;%\runnest_tokens(word, text, token = \u0026quot;regex\u0026quot;, pattern = reg) %\u0026gt;%\rmutate(year = forum_year)\rreturn(df)\r}\r# apply function to programs books\rall_years \u0026lt;- bind_rows(\rtext_prepr(\u0026quot;data/AIR-2018-Forum-Program-Book.pdf\u0026quot;, 2018),\rtext_prepr(\u0026quot;data/2017-AIR-Forum-Program-Book.pdf\u0026quot;, 2017),\rtext_prepr(\u0026quot;data/2016_AIR-Forum_Program-Book.pdf\u0026quot;, 2016),\rtext_prepr(\u0026quot;data/2015-Forum-Program-Book-Web.pdf\u0026quot;, 2015),\rtext_prepr(\u0026quot;data/2014ForumProgramBookFinal.pdf\u0026quot;, 2014),\rtext_prepr(\u0026quot;data/2013finalprogram.pdf\u0026quot;, 2013)\r)\rThis resulted in a data frame with a total 447582 rows (one row for each word), a glimpse of which is printed below:\nhead(all_years)\r## word year\r## 1 association 2018\r## 2 for 2018\r## 3 institutional 2018\r## 4 research 2018\r## 5 2018 2018\r## 6 may 2018\rNext, I decided on which words I would ask R to look for. Somewhat arbitrarily and somewhat based on my experience at AIR Forums, I chose the following: Excel, R, SAS, SPSS, and Tableau. The code below (1) searches for mentions of those tools in the list of words created above (2) counts the results by year (3) builds a chart of the results. To clarify, the resulting chart displays the number of times each software is mentioned in each of the AIR Forum program books for each of the respective years.\n# filter for keywords and count by word and year\rsoftware \u0026lt;- all_years %\u0026gt;% mutate(word = tolower(word)) %\u0026gt;% filter(word %in% c(\u0026quot;excel\u0026quot;, \u0026quot;r\u0026quot;, \u0026quot;sas\u0026quot;, \u0026quot;spss\u0026quot;, \u0026quot;tableau\u0026quot;)) %\u0026gt;% count(word, year) %\u0026gt;% complete(word, year, fill = list(n = 0))\r# make data frame of only 2018, so can include as labels at end of lines\rsoftware_2018 \u0026lt;- software %\u0026gt;% filter(year == 2018)\r# create chart\rggplot() +\rgeom_line(data = software, aes(x = year, y = n, color = word, group = word), size = 2) +\rgeom_text(data = software_2018, aes(x = year, y = n, label = word), nudge_y = 2) +\rgeom_point(data = software, aes(x = year, y = n, color = word), size = 3) +\rtheme_minimal() +\rlabs(x = \u0026quot;Forum Year\u0026quot;, y = \u0026quot;# of Mentions\u0026quot;,\rtitle = \u0026quot;Number of Times Software is Mentioned in AIR Forum Program Book\u0026quot;,\rsubtitle = \u0026quot;2013 to 2018 Forums\u0026quot;) +\rguides(color = FALSE)\rWhat stands out - and confirms what I’ve noticed at AIR Forums - is the rapid rise of Tableau. In 2014 I had never heard of it. In 2018, not knowing at least something about it seems unavoidable. Tableau is known for visual analytics, so it’s not surprising that its rise in populairty has coincided with an increasing interest in data visualization at AIR Forums.\n# filter for keyword and count by word and year\rvisualize \u0026lt;- all_years %\u0026gt;% group_by(year) %\u0026gt;% mutate(total = n()) %\u0026gt;% ungroup() %\u0026gt;% mutate(visualiz = str_detect(word, \u0026quot;visualiz\u0026quot;)) %\u0026gt;%\rgroup_by(year) %\u0026gt;% summarise(prop = mean(visualiz)) %\u0026gt;% ungroup()\r# create chart\rggplot(visualize, aes(x = year, y = prop)) +\rgeom_line(size = 2) +\rgeom_point(size = 3) +\rtheme_minimal() +\rlabs(x = \u0026quot;Forum Year\u0026quot;, y = \u0026quot;% of Mentions\u0026quot;,\rtitle = \u0026#39;Percent of All Words That Were \u0026quot;Visualiz*\u0026quot; in AIR Forum Program Book\u0026#39;,\rsubtitle = \u0026quot;2013 to 2018 Forums\u0026quot;) +\rguides(color = FALSE) +\rscale_y_continuous(label = percent_format())\rReturning to the first chart, perhaps it’s my background in psychology - a field which has historically been dominated by SPSS - but I was surprised how little SPSS is mentioned (although, I wouldn’t be surprised to see it decrease in the future). As for R, it seems to have a presence, but I’m curious to see what happens to its popularity in institutional research over the next few years. Judging by my experience at recent Forums and the development of R tools that decrease the barrier to entry, my prediction is that interest will only grow.\nI’m not sure if this need be said, but I will: This approach to evaluating software popularity is fraught with limitations. My goal, however, was not to get a precise estimate. Rather, I was interested in one, getting a general sense of broad trends, and two, sharing how just a little of bit R code can do so much!\n","date":1535068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535068800,"objectID":"4ae2c2f6beaa89f4f865762527bb6cb7","permalink":"https://arie.rbind.io/post/air-forum-text-software-frequency/air-forum-text-software-frequency/","publishdate":"2018-08-24T00:00:00Z","relpermalink":"/post/air-forum-text-software-frequency/air-forum-text-software-frequency/","section":"post","summary":"Data: It’s become a cliche to say that it’s everywhere and in quantities that are unimaginable. But data in its raw form, whether in a structured database or on the internet, is of limited use until a human does something to it: Gather it, clean it, visualize it, model it, write about it, and so-on.\nThe amount of data that those in institutional research encounter requires powerful tools to work with.","tags":["r","spss"],"title":"Popularity of Various Data Analytic Tools at AIR Forums","type":"post"},{"authors":["arie"],"categories":["conference presentation"],"content":"\rAlthough the concept of reproducibility is typically reserved for the sciences, the presenter will argue that by adopting its principles, IR offices would see immeasurable benefits in efficiency, accuracy, and transparency. Reproducible workflows preserve every decision made about data analyses (e.g., removing a student who withdrew) and allow users to quickly and accurately respond to requests for modifications (e.g., group tables by college instead of major). One barrier to reproducibility, however, is that it requires coding. Using examples from the free R programming language, the presenter will show that not only is R an ideal software for reproducibility, but that many of its modern features are designed to get novices quickly doing powerful things. The primary goals of the presentation are for audience members to leave convinced that they can learn R, and that if they do, they will become better at their jobs.\n","date":1527763500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527763500,"objectID":"136786ca731bb255e98898d725bb0902","permalink":"https://arie.rbind.io/talk/2018-air/","publishdate":"2018-05-31T10:45:00Z","relpermalink":"/talk/2018-air/","section":"talk","summary":"Demonstrating the value of reproducibility with R and R Markdown to IR institutional researchers.","tags":["air2018","rmarkdown"],"title":"Achieving Reproducibility in IR With R","type":"talk"},{"authors":["arie"],"categories":["conference presentation"],"content":"\rA multitude of tools and promising practices exist for visualizing data for informing and decision making. This session will feature a showcase of tools and techniques for utilizing data to inform and present. Presenters were all provided the same dataset and guiding questions and each used a different tool (Excel, R, Tableau, and SAS Visual Analytics) to create a visual display of the data. Each will present their work with the session moderator highlighting best practice in visualization throughout the presentation.\n","date":1496158200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496158200,"objectID":"438becd2c97e08124ced2511c9b9fc69","permalink":"https://arie.rbind.io/talk/2017-air/","publishdate":"2017-05-30T15:30:00Z","relpermalink":"/talk/2017-air/","section":"talk","summary":"This session features a showcase of tools and techniques for utilizing data to inform and present.","tags":["air2017","ggplot2"],"title":"Data Visualization Showcase","type":"talk"},{"authors":["arie"],"categories":["conference presentation"],"content":"\rCreating graphs is a central part of the IR workload, but it can be frustrating. If you make graphs using Excel, you might spend hours mindlessly pointing-and-clicking, which then has to be repeated if updates to the graphs are required. The presenter will begin by arguing in favor of two points: First, IR professionals would be better off using R (a free and open-source programming language), and second, even those without any programming experience can learn R well enough to reap its majesty. A demonstration will be given on how to create and modify a scatter plot using ggplot2, an R package for building highly customizable visualizations. The primary goal of the presentation is for audience members to leave feeling empowered to learn to use R to improve the quantity, quality, and reproducibility of their work\n","date":1467385200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467385200,"objectID":"90957c2ce4e542d00856f84715b63e68","permalink":"https://arie.rbind.io/talk/2016-air/","publishdate":"2016-07-01T15:00:00Z","relpermalink":"/talk/2016-air/","section":"talk","summary":"The primary goal of the presentation is for audience members to leave feeling empowered to learn to use R to improve the quantity, quality, and reproducibility of their work.","tags":["air2016","ggplot2"],"title":"Creating Data Visualizations Using R: An Introduction for Non-Programmers","type":"talk"}]