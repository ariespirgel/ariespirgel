<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arie Spirgel</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Arie Spirgel</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 01 Jan 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Arie Spirgel</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Which NBA First Name Creates the Best Starting Five?</title>
      <link>/post/nba-first-names/nba-first-names/</link>
      <pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/nba-first-names/nba-first-names/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Michael. Larry. Moses. Mention these names to the most casual of NBA fans, and they know that you mean Jordan, Bird, and Malone, and not Doleac, Siegfried, and Brown. Even among other highly accomplished individuals, the all-time greats are so transcendent that they are recognized by their first names alone.&lt;/p&gt;
&lt;p&gt;We all agree who the best Michael is, but who are the other great (or good) Michaels? And LeBron is the best LeBron, but has a different LeBron ever played in the NBA (spoiler alert: no)? My goal here was to answer these types of questions by building the best possible starting fives based on NBA players who share a first name. Shea Serrano from the Ringer did the &lt;a href=&#34;https://www.theringer.com/nba/2017/7/7/16077712/nba-first-name-game-e1a2249dcab0&#34;&gt;same thing&lt;/a&gt; a few years ago, but here, I use a more statistical approach than he did, and slightly different rules. The rules/system I used are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each player, I selected the season that they accomplished their highest &lt;a href=&#34;https://www.basketball-reference.com/about/ws.html&#34;&gt;win share per 48&lt;/a&gt; minutes AND played at least 1,000 minutes.&lt;/li&gt;
&lt;li&gt;The best starting fives are those that had the highest &lt;em&gt;combined&lt;/em&gt; win shares per 48.&lt;/li&gt;
&lt;li&gt;Each team must include two guards, two forwards, and one center. This meant that, for example, Laker great and guard Michael Cooper was not on team Michael, because Michael Redd’s best season was better than Cooper’s best season, and you already know who got the other guard spot on team Michael.&lt;/li&gt;
&lt;li&gt;I used names exactly as they appear in Basketball Reference (e.g., David Robinson counts only as a David, not as a Dave).&lt;/li&gt;
&lt;li&gt;The Basketball Reference data spans from the 1950-1951 season to the 2019-2020 (heretofore) abbreviated season, so every player from that period can be included.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;step-1-get-the-data-from-basketball-reference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Get the Data from Basketball Reference&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(nbastatR)
library(ggrepel)
library(DT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get data from BREF
ws &amp;lt;- bref_players_stats(seasons = 1951:2020, tables = c(&amp;quot;advanced&amp;quot;, &amp;quot;totals&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-clean-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Clean Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select variables and filter for min 1,000 minutes
ws &amp;lt;- ws %&amp;gt;% 
  filter(minutes &amp;gt;= 1000) %&amp;gt;% 
  select(year = yearSeason,
         player = namePlayer, id = idPlayerNBA, 
         position = groupPosition,
         win_share = ratioWSPer48) 

# give first name its own column
ws &amp;lt;- ws %&amp;gt;% 
  separate(player, into = c(&amp;quot;first&amp;quot;, &amp;quot;last&amp;quot;), sep = &amp;quot; &amp;quot;,
           remove = FALSE) %&amp;gt;% 
  mutate(first = str_trim(first, side = &amp;quot;both&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-select-each-players-best-season&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Select Each Player’s Best Season&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select each player&amp;#39;s best season.
# michael cage tied for his best seasons at forward and center.  
# so I decided to select him as a center.
# the next best michael center was doleac.
best_season &amp;lt;- ws %&amp;gt;% 
  group_by(player, id) %&amp;gt;%
  filter(win_share == max(win_share)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  select(-year) %&amp;gt;% 
  unique() %&amp;gt;% 
  filter(id != 262 | position == &amp;quot;F&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-build-starting-fives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4: Build Starting Fives&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_season &amp;lt;- best_season %&amp;gt;% 
  group_by(first, position) %&amp;gt;% 
  mutate(rank = rank(-win_share)) %&amp;gt;% 
  ungroup() 

best_season &amp;lt;- best_season %&amp;gt;% 
  filter( (position %in% c(&amp;quot;F&amp;quot;, &amp;quot;G&amp;quot;) &amp;amp; rank %in% c(1, 2) |
            position == &amp;quot;C&amp;quot; &amp;amp; rank == 1) ) %&amp;gt;% 
  add_count(first) %&amp;gt;% 
  filter(n == 5)

best_names &amp;lt;- best_season %&amp;gt;% 
  group_by(first) %&amp;gt;% 
  mutate(total = sum(win_share)) %&amp;gt;% 
  ungroup() &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;drumrollresults&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Drumroll…Results&lt;/h2&gt;
&lt;p&gt;Like Shea in his piece, I concluded that you can not build a better team than a team made up Kevins. Here are the top 20 first name teams:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_names %&amp;gt;% 
  distinct(first, total) %&amp;gt;% 
  mutate(first = fct_reorder(first, total)) %&amp;gt;% 
  arrange(-total) %&amp;gt;% 
  slice(1:20) %&amp;gt;% 
  ggplot(aes(x = total, y = first,
             fill = total)) +
  geom_col() +
  guides(fill = FALSE) +
  scale_fill_gradient(low = &amp;quot;yellow&amp;quot;, high = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Win Share / 48&amp;quot;, y = NULL,
       title = &amp;quot;Which NBA First Name Creates the Best Starting Five?&amp;quot;,
       subtitle = &amp;quot;Teams must include two guards, two forwards, and one center.&amp;quot;,
       caption = &amp;quot;Data is from Basketball Reference, and spans 1951 to 2020 seasons.\nCreated by Arie Spirgel&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/nba-first-names/nba-first-names_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here is my complete list of starting fives:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dt &amp;lt;- best_names %&amp;gt;% 
  select(`First Name` = first, last, `Combined WS / 48` = total) %&amp;gt;% 
  group_by(`First Name`) %&amp;gt;% 
  mutate(rank = rank(last, ties.method = &amp;quot;first&amp;quot;)) %&amp;gt;% 
  spread(rank, last) %&amp;gt;% 
  arrange(-`Combined WS / 48`) %&amp;gt;% 
  datatable(filter = &amp;quot;top&amp;quot;, rownames = FALSE)


widgetframe::frameWidget(dt)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/nba-first-names/nba-first-names_files/figure-html//widgets/widget_unnamed-chunk-8.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;my-not-so-statistical-thoughts-on-a-hypothetical-season&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My (Not So Statistical) Thoughts on a Hypothetical Season&lt;/h2&gt;
&lt;p&gt;If you were building a starting five of &lt;em&gt;any&lt;/em&gt; NBA players, regardless of first name, you could not do much better than Kevin Garnett and Kevin Durant as your forwards. The guards on the Kevins are Johnson and Martin, who would complement the forwards well. Johnson was an explosive point-guard who averaged 17.9 points and 9.1 assists for his 12-year NBA career. And Kevin Martin’s shooting would space the floor, helping Garnett and Durant further dominate in all of the ways that they do.&lt;/p&gt;
&lt;p&gt;After Kevin, my list diverged from Shea’s, in large part because of the different rules we used. He had the Michaels second, but they were 11th on my list. This is of course silly, because prime MJ with I-don’t-care-who would never finish 11th in any actual basketball tournament. We both had the Larrys and Chrises highly ranked, but the Georges and Bobs were third and fourth on my list, who Shea respectively doesn’t mention and thinks they would be fun to watch but wouldn’t win a game.&lt;/p&gt;
&lt;p&gt;So, smart money is on the Kevins, but they’d be lucky to avoid a match-up with the Davids, whose frontcourt of Robinson, Lee and West, at a combined 20.5 feet, are more Goliath than David. In addition to Robinson, the Davids also have a Hall of Famer in the backcourt with Thompson. The Davids would be physical to play against and fun to watch, but like everyone else, likely wouldn’t be able to keep up with the Kevins.&lt;/p&gt;
&lt;p&gt;The James team would also be a tough out. Harden would love to be surrounded by shooters like Posey and Jones and would thrive on the break with Worthy. But in this alternate reality, if Worthy had still spent his career playing with Magic Johnson - one of the most unselfish players ever - he would struggle watching Harden dribble around for 18 seconds before launching isolation step-back threes. That said, with the right coaching and the right sacrifices, this team is a dark horse.&lt;/p&gt;
&lt;p&gt;Tim Duncan’s teams are always good (because he is Tim Duncan). But then, put him in the two-man game with five-time all-star Tim Hardaway and have Legler spot up in the corner, and this team is closer to great. If their other forward - Tim Thomas - were to heed Duncan’s mentorship, perhaps his career would reach greater heights and he could help to earn the Tims a finals bout with the Kevins.&lt;/p&gt;
&lt;p&gt;The most fun team to watch would be the Chrises, who in Paul, Mullin, and Webber, have three of the more creative players, and in Anderson, one of the most expressive. Play-by-play announcers would regularly applaud their ball movement and the extra pass, and if they stayed healthy, could be a problem for more heavily favored teams. Either way, this team is a League Pass fan-favorite.&lt;/p&gt;
&lt;p&gt;The Derricks have two big-name stars in Coleman and Rose, and a solid supporting cost in White, Jones Jr., and Favors. Loyal fans would go into the season with high hopes and championship aspirations, but ultimately, this team doesn’t have the star-power to get very far.&lt;/p&gt;
&lt;p&gt;Some other notable teams. With Barkley and Oakley, the Charles team would undoubtedly lead the league in technicals. The Steves would be one of the best shooting teams - led by Nash, Kerr, and Novack - but with such a small backcourt, would not be able to compete. The all-time top Dannys might not immediately come to mind (Ainge, Granger, Green, Manning, and Fortson), but they are a good shooting team with size and experience, and whose whole is greater than the sum of its parts.&lt;/p&gt;
&lt;p&gt;Any discussion of all-time NBA greats begins with Michael, LeBron, Bill Russell, Magic, and Kareem. Except this one, that is contrived and based on a hypothetical world. In this scenario, which is based on made-up rules and could only exist if traveling through time were possible, it is the Kevins who come out on top. Congratulations to them!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidying IPEDS Data in R</title>
      <link>/post/online-education/distance-education/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/online-education/distance-education/</guid>
      <description>


&lt;p&gt;If you’ve downloaded enough data from the &lt;a href=&#34;https://nces.ed.gov/ipeds/use-the-data&#34;&gt;IPEDS Data Center&lt;/a&gt; using the “Compare Institutions” interface, you’ve probably realized that, depending on what you’re downloading, the data provided is rarely in a format ready for analysis. Here, via a specific example, I describe what makes the IPEDS data format impractical, and how to use R to resolve that.&lt;/p&gt;
&lt;div id=&#34;reading-in-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading in the Data&lt;/h2&gt;
&lt;p&gt;I first downloaded Fall 2012 to Fall 2018 distance education headcounts for every college and university in the IPEDS Data Center. In this first section, I read in the data, and display a subset of what the full data set looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(scales)

theme_set(theme_light())

distance &amp;lt;- read_csv(&amp;quot;raw-data/distance-fall-12-18.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/data-preview.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The data set contains 6,800 rows and 43 columns, and ignoring the &lt;code&gt;Institution Name&lt;/code&gt; column, each of the remaining columns is some version of the following: &lt;code&gt;Students enrolled exclusively in distance education courses (EF2018A_DIST  Undergraduate total)&lt;/code&gt;. Under that specific column, for each of the 6,800 institutions that reported data, are headcounts for &lt;strong&gt;exclusively distance&lt;/strong&gt; &lt;strong&gt;undergraduate students&lt;/strong&gt; in the fall term of &lt;strong&gt;2018&lt;/strong&gt;. The problem, thus, is that this column (and all the other ones like it) actually contains three pieces of information:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Level&lt;/strong&gt;, which can take on the values &lt;em&gt;undergraduate&lt;/em&gt; or &lt;em&gt;graduate&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modality&lt;/strong&gt;, which can take on the values &lt;em&gt;exlusively distance&lt;/em&gt;, &lt;em&gt;some distance&lt;/em&gt;, or &lt;em&gt;no distance&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Year&lt;/strong&gt;, which can take on any integer value from &lt;em&gt;2012&lt;/em&gt; to &lt;em&gt;2018&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This untidy format is exactly what makes IPEDS data tricky to work with. In contrast, tidy data - which means each variable is in its own column, each observation is in its own row, and each value is in its own cell&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; - is advantageous not just for working with data in R, but other software as well (e.g., pivot tables in Excel).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidying-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidying the Data&lt;/h2&gt;
&lt;p&gt;The first step to tidying this data is to pivot it so that all of the column names that contain the type of headcount are in one column, and the actual headcounts are in a different column. To do that, I use the &lt;code&gt;gather()&lt;/code&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; function. I first provide &lt;code&gt;gather()&lt;/code&gt; with the names of the two new variables that are being created - I call them &lt;code&gt;variable&lt;/code&gt; and &lt;code&gt;headcount&lt;/code&gt;, but they can be called anything you want - and then which columns I want pivoted from wide to long; here, I pivot everything from the 2nd column to the last column of the data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distance &amp;lt;- distance %&amp;gt;% 
  gather(variable, headcount, 2:ncol(.))

distance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 285,600 x 3
##    `Institution Name`             variable                             headcount
##    &amp;lt;chr&amp;gt;                          &amp;lt;chr&amp;gt;                                    &amp;lt;dbl&amp;gt;
##  1 Educational Technical College… Students enrolled exclusively in di…        NA
##  2 A T Still University of Healt… Students enrolled exclusively in di…        NA
##  3 Aaniiih Nakoda College         Students enrolled exclusively in di…        NA
##  4 ABC Adult School               Students enrolled exclusively in di…        NA
##  5 ABC Beauty Academy             Students enrolled exclusively in di…        NA
##  6 ABCO Technology                Students enrolled exclusively in di…        NA
##  7 Abcott Institute               Students enrolled exclusively in di…        NA
##  8 Abdill Career College Inc      Students enrolled exclusively in di…        NA
##  9 Abilene Christian University   Students enrolled exclusively in di…        32
## 10 Abraham Baldwin Agricultural … Students enrolled exclusively in di…       377
## # … with 285,590 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see above, the data set now only has three columns, not 43. Same data, different layout. Looks better already, right?!?&lt;/p&gt;
&lt;p&gt;We’re not done though. Remember, each row of the &lt;code&gt;variable&lt;/code&gt; column contains three pieces of information: level, modality, and year. So for the next three steps I split that column apart so each of these variables are in their own column. First, I’ll make a new column for level.&lt;/p&gt;
&lt;p&gt;There are countless ways of reaching the same endpoint in R, and in this instance, I use &lt;code&gt;str_detect()&lt;/code&gt; to tell R to put “Undergraduate” in the &lt;code&gt;level&lt;/code&gt; column if it detects the string “Undergraduate” in the &lt;code&gt;variable&lt;/code&gt; column, and then perform the analogous task for “Graduate”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distance &amp;lt;- distance %&amp;gt;% 
  mutate(level = case_when(
           str_detect(variable, &amp;quot;Undergraduate&amp;quot;) ~ &amp;quot;Undergraduate&amp;quot;,
           str_detect(variable, &amp;quot;Graduate&amp;quot;) ~ &amp;quot;Graduate&amp;quot;))

distance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 285,600 x 4
##    `Institution Name`          variable                        headcount level  
##    &amp;lt;chr&amp;gt;                       &amp;lt;chr&amp;gt;                               &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  
##  1 Educational Technical Coll… Students enrolled exclusively …        NA Underg…
##  2 A T Still University of He… Students enrolled exclusively …        NA Underg…
##  3 Aaniiih Nakoda College      Students enrolled exclusively …        NA Underg…
##  4 ABC Adult School            Students enrolled exclusively …        NA Underg…
##  5 ABC Beauty Academy          Students enrolled exclusively …        NA Underg…
##  6 ABCO Technology             Students enrolled exclusively …        NA Underg…
##  7 Abcott Institute            Students enrolled exclusively …        NA Underg…
##  8 Abdill Career College Inc   Students enrolled exclusively …        NA Underg…
##  9 Abilene Christian Universi… Students enrolled exclusively …        32 Underg…
## 10 Abraham Baldwin Agricultur… Students enrolled exclusively …       377 Underg…
## # … with 285,590 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the new column on the end with &lt;code&gt;level&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;Next I do the same thing for modality: I tell R to look for specific strings, and make a new column based on those strings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distance &amp;lt;- distance %&amp;gt;% 
         mutate(modality = case_when(
           str_detect(variable, &amp;quot;not enrolled in any&amp;quot;) ~ &amp;quot;No Distance&amp;quot;,
           str_detect(variable, &amp;quot;in some&amp;quot;) ~ &amp;quot;Some Distance&amp;quot;,
           str_detect(variable, &amp;quot;exclusively&amp;quot;) ~ &amp;quot;Exclusively Distance&amp;quot;))

distance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 285,600 x 5
##    `Institution Name`       variable                 headcount level  modality  
##    &amp;lt;chr&amp;gt;                    &amp;lt;chr&amp;gt;                        &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     
##  1 Educational Technical C… Students enrolled exclu…        NA Under… Exclusive…
##  2 A T Still University of… Students enrolled exclu…        NA Under… Exclusive…
##  3 Aaniiih Nakoda College   Students enrolled exclu…        NA Under… Exclusive…
##  4 ABC Adult School         Students enrolled exclu…        NA Under… Exclusive…
##  5 ABC Beauty Academy       Students enrolled exclu…        NA Under… Exclusive…
##  6 ABCO Technology          Students enrolled exclu…        NA Under… Exclusive…
##  7 Abcott Institute         Students enrolled exclu…        NA Under… Exclusive…
##  8 Abdill Career College I… Students enrolled exclu…        NA Under… Exclusive…
##  9 Abilene Christian Unive… Students enrolled exclu…        32 Under… Exclusive…
## 10 Abraham Baldwin Agricul… Students enrolled exclu…       377 Under… Exclusive…
## # … with 285,590 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last step of tidying is to get &lt;code&gt;year&lt;/code&gt; in its own column. I &lt;em&gt;could&lt;/em&gt; tell R to make a new variable and put “2012” if it detects “2012”, “2013” if it detects “2013”, and so-on, but there is a much simpler way: the &lt;code&gt;parse_number()&lt;/code&gt; function, which drops any non-numeric characters from a string.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distance &amp;lt;- distance %&amp;gt;% 
  mutate(year = parse_number(variable))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tidying is now done, and so although this next step isn’t necessary, renaming and reordering the variables and factor levels will make the data easier to work with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rename columns, reorder factor levels (e.g., Undergraduate before Graduate)
distance &amp;lt;- distance %&amp;gt;%
  select(institution_name = `Institution Name`, level,
         modality, year, headcount) %&amp;gt;% 
  mutate(level = fct_relevel(level, &amp;quot;Undergraduate&amp;quot;),
         modality = fct_relevel(modality, &amp;quot;Some Distance&amp;quot;)) 

distance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 285,600 x 5
##    institution_name                    level      modality        year headcount
##    &amp;lt;chr&amp;gt;                               &amp;lt;fct&amp;gt;      &amp;lt;fct&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 Educational Technical College-Reci… Undergrad… Exclusively D…  2018        NA
##  2 A T Still University of Health Sci… Undergrad… Exclusively D…  2018        NA
##  3 Aaniiih Nakoda College              Undergrad… Exclusively D…  2018        NA
##  4 ABC Adult School                    Undergrad… Exclusively D…  2018        NA
##  5 ABC Beauty Academy                  Undergrad… Exclusively D…  2018        NA
##  6 ABCO Technology                     Undergrad… Exclusively D…  2018        NA
##  7 Abcott Institute                    Undergrad… Exclusively D…  2018        NA
##  8 Abdill Career College Inc           Undergrad… Exclusively D…  2018        NA
##  9 Abilene Christian University        Undergrad… Exclusively D…  2018        32
## 10 Abraham Baldwin Agricultural Colle… Undergrad… Exclusively D…  2018       377
## # … with 285,590 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Behold, tidy data!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing the Data&lt;/h2&gt;
&lt;p&gt;With the data in a tidy format you can now do…pretty much whatever you want with it! In the examples below, I chose to visualize it, which demonstrates how - thanks to tidy data(!) - you can recycle the same code with slight alterations to make different plots. First, here are overall trends in distance education.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distance %&amp;gt;% 
  mutate(headcount = replace_na(headcount, 0)) %&amp;gt;% 
  group_by(year, modality) %&amp;gt;% 
  summarise(total = sum(headcount)) %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  mutate(prop = total / sum(total)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(modality != &amp;quot;No Distance&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = factor(year), y = prop, fill = modality)) +
  geom_col() +
  scale_y_continuous(label = percent_format()) +
  theme(legend.position = &amp;quot;top&amp;quot;) +
  labs(x = &amp;quot;Fall Term&amp;quot;, y = &amp;quot;% of Students&amp;quot;,
       title = &amp;quot;Percentage of Students Enrolled in Distance Education&amp;quot;,
       fill = NULL,
       subtitle = &amp;quot;Fall 2012 to Fall 2018&amp;quot;,
       caption = &amp;quot;Source: IPEDS Data Center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/online-education/Distance-Education_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, I change the grouping variables to repeat the same chart except here I partition the data by level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distance %&amp;gt;% 
  mutate(headcount = replace_na(headcount, 0)) %&amp;gt;% 
  group_by(year, modality, level) %&amp;gt;% 
  summarise(total = sum(headcount)) %&amp;gt;% 
  group_by(year, level) %&amp;gt;% 
  mutate(prop = total / sum(total)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(modality != &amp;quot;No Distance&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = factor(year), y = prop, fill = modality)) +
  geom_col() +
  facet_wrap(~level) +
  scale_y_continuous(label = percent_format()) +
  theme(legend.position = &amp;quot;top&amp;quot;) +
  labs(x = &amp;quot;Fall Term&amp;quot;, y = &amp;quot;% of Students&amp;quot;,
       title = &amp;quot;Percentage of Students Enrolled in Distance Education&amp;quot;,
       fill = NULL,
       subtitle = &amp;quot;Fall 2012 to Fall 2018&amp;quot;,
       caption = &amp;quot;Source: IPEDS Data Center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/online-education/Distance-Education_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And once more, limiting the results to a single institution: Florida State University.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distance %&amp;gt;% 
  filter(institution_name == &amp;quot;Florida State University&amp;quot;) %&amp;gt;% 
  mutate(headcount = replace_na(headcount, 0)) %&amp;gt;% 
  group_by(year, modality, level) %&amp;gt;% 
  summarise(total = sum(headcount)) %&amp;gt;% 
  group_by(year, level) %&amp;gt;% 
  mutate(prop = total / sum(total)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(modality != &amp;quot;No Distance&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = factor(year), y = prop, fill = modality)) +
  geom_col() +
  facet_wrap(~level) +
  scale_y_continuous(label = percent_format()) +
  theme(legend.position = &amp;quot;top&amp;quot;) +
  labs(x = &amp;quot;Fall Term&amp;quot;, y = &amp;quot;% of Students&amp;quot;,
       title = &amp;quot;Percentage of Florida State U. Students Enrolled in Distance Education&amp;quot;,
       fill = NULL,
       subtitle = &amp;quot;Fall 2012 to Fall 2018&amp;quot;,
       caption = &amp;quot;Source: IPEDS Data Center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/online-education/Distance-Education_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Among its many benefits, tidy data lets you devote more attention to &lt;em&gt;what&lt;/em&gt; you want to do rather than &lt;em&gt;how&lt;/em&gt; you want to do it. Yes, tidying data takes longer at the start, but in the long-run, it will save you time. In that way, it’s just like learning R!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html&#34; class=&#34;uri&#34;&gt;https://r4ds.had.co.nz/tidy-data.html&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;code&gt;pivot_longer()&lt;/code&gt; is an updated version of &lt;code&gt;gather()&lt;/code&gt;.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>POSTPONED DUE TO CORONAVIRUS Introduction to R and ggplot2</title>
      <link>/talk/2020-wcc-ggplot2/</link>
      <pubDate>Fri, 13 Mar 2020 12:00:00 +0000</pubDate>
      <guid>/talk/2020-wcc-ggplot2/</guid>
      <description>


&lt;p&gt;Learn how to visualize data using R and ggplot2!&lt;/p&gt;
&lt;p&gt;Whether you’re interested in data science, business, or working on a dissertation or research project, knowing how to visualize data is a vital skill. In this free, 2.5 hour workshop, you’ll be introduced to the R programming language and learn to visualize data using ggplot2. In the last 30 minutes of the workshop, Janine Morris from NSU’s Writing and Communication Center will talk about the features of effective data visualizations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Use R for Institutional Research? Part 1, Many Models</title>
      <link>/post/ipeds-many-models/01-r-for-institutional-research/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/ipeds-many-models/01-r-for-institutional-research/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I first heard about R when I was in graduate school in 2008 and fellow students used it to analyze their data. I didn’t bother to learn it at the time because, one, I didn’t see the benefit of it, and two, I assumed that without any programming experience, it was too difficult. So I continued with my same workflow: Clean data and make charts in Excel, import data into SPSS to analyze it, and then paste my output into a Word document and write up the results.&lt;/p&gt;
&lt;p&gt;I started working in institutional research in 2013 and I still hadn’t made the switch to R, but was beginning to see the drawbacks of my workflow and the upside of coding. I often had to generate the same reports on a regular basis where the only thing that would change was the data. Or I’d have to generate the same charts or tables for each of the 15 colleges at the university, and on bad days, each of the 150-something majors. This quickly became unsustainable when I would, for example, get one of these requests late on a Friday afternoon and had to have it ready for a Board meeting on Monday. R increasingly seemed like a preferable alternative.&lt;/p&gt;
&lt;p&gt;Fast-forward 7 years and my SPSS license has long since expired, I don’t recall the last time I made a chart in Excel, and the only thing I use Word for is making grocery lists. Today, my entire workflow exists inside of R.&lt;/p&gt;
&lt;p&gt;In the intervening years, I have frequently met other institutional researchers who are stuck in the same mindset I was in 2008: For people who have never coded, R seems too overwhelming to learn, and even if they were to learn it, they do not see the benefits of doing so. In future posts I plan to address the former, but in this series of posts I want to address the latter: What’s the point of learning R for institutional research? Rather than list all of the reasons why R is an excellent choice for doing institutional research, I want to show examples of how I use it. In this post, I’ll demonstrate the scenario of using R to run many models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you are not an R user, do not worry about the details of the code below, but instead, pay attention to what the code is capable of producing&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-one-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running One Model&lt;/h2&gt;
&lt;p&gt;Whether you want to predict future enrollment or explain why some students do not graduate, modeling is an important skill in institutional research. To show how to run a linear model in R, for all colleges and universities in the &lt;a href=&#34;https://nces.ed.gov/ipeds/use-the-data&#34;&gt;IPEDS Data Center&lt;/a&gt;, I downloaded their state, one year retention rates (i.e., the percent of first-time in college students who re-enroll their second fall term), student-faculty ratios, and the number of undergraduate applications they received for a given year. Here is the code for reading in the data and what the first five rows of data look like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(broom)
library(drlib)

ipeds &amp;lt;- read_rds(&amp;quot;processed-data/ipeds-sfr.rds&amp;quot;)

head(ipeds, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 5
##   name                state   undergrad_applic… retention_rate student_faculty_…
##   &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
## 1 Educational Techni… Puerto…                NA             11                21
## 2 A T Still Universi… Missou…                NA             NA                NA
## 3 Aaniiih Nakoda Col… Montana                NA             34                10
## 4 ABC Adult School    Califo…                NA             NA                 4
## 5 ABC Beauty Academy  Texas                  NA             25                10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this contrived example, to build a linear model with retention rate as the outcome and student-faculty ratio and number of undergraduate applications as the predictors, I took the &lt;code&gt;ipeds&lt;/code&gt; data frame, piped it (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) to the &lt;code&gt;lm&lt;/code&gt; function, and then cleaned up the results with the &lt;code&gt;tidy()&lt;/code&gt; function from the broom package. This gives us the model results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ipeds %&amp;gt;% 
  lm(retention_rate ~ student_faculty_ratio + undergrad_applicants, data = .) %&amp;gt;% 
  tidy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##   term                   estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;                     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)           76.2      0.868         87.8  0.      
## 2 student_faculty_ratio -0.326    0.0589        -5.54 3.45e- 8
## 3 undergrad_applicants   0.000504 0.0000315     16.0  3.03e-54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point you may be thinking, “So what? I can just easily do the same thing in SPSS, or even Excel”. That is true, but what if instead of running one model, you had to run 150?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-many-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running Many Models&lt;/h2&gt;
&lt;p&gt;As part of our university’s strategic business plan, I recently had to create separate models for each of the 150-something majors at the school. If I were still using SPSS, this would mean:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;days of pointing and clicking and copying and pasting.&lt;/li&gt;
&lt;li&gt;doing the same thing over and over again each time the project requirements changed, which is an inevitability.&lt;/li&gt;
&lt;li&gt;having no documentation about the decisions I made because everything was done by pointing and clicking.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Returning to the original data set, let’s say I wanted repeat the same model above, but separately for each state. Using R, I first filter the data to only include states with at least 50 schools (an arbitrarily chosen cutoff point):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ipeds &amp;lt;- ipeds %&amp;gt;% 
  add_count(state) %&amp;gt;% 
  filter(n &amp;gt;= 50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I turn the model into a function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;state_regression &amp;lt;- function(df) {
 df %&amp;gt;% 
  lm(retention_rate ~ student_faculty_ratio + undergrad_applicants, data = .) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, I can apply the function to each state in the data set, which returns a data frame with the model results for each state:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ipeds_model &amp;lt;- ipeds %&amp;gt;% 
  group_by(state) %&amp;gt;% 
  nest() %&amp;gt;% 
  mutate(model = map(data, state_regression),
         tidy_model = map(model, tidy)) %&amp;gt;% 
  unnest(tidy_model) 

head(ipeds_model, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 8
## # Groups:   state [2]
##   state            data model term         estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;list&amp;lt;df[,5&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 Puerto …    [146 × 5] &amp;lt;lm&amp;gt;  (Intercept)  87.5       5.80        15.1  2.48e-18
## 2 Puerto …    [146 × 5] &amp;lt;lm&amp;gt;  student_fac… -0.869     0.315       -2.76 8.68e- 3
## 3 Puerto …    [146 × 5] &amp;lt;lm&amp;gt;  undergrad_a…  0.00208   0.00140      1.49 1.45e- 1
## 4 Missouri    [171 × 5] &amp;lt;lm&amp;gt;  (Intercept)  84.5       5.46        15.5  1.91e-22
## 5 Missouri    [171 × 5] &amp;lt;lm&amp;gt;  student_fac… -1.07      0.458       -2.33 2.30e- 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, with a separate model for each state all in a data frame, I can treat the model output like I would any other data. For example, here, I visualize the model results for each state:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ipeds_model %&amp;gt;% 
  filter(term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;% 
  mutate(term = if_else(term == &amp;quot;student_faculty_ratio&amp;quot;,
                        &amp;quot;Student/Faculty Ratio&amp;quot;, &amp;quot;# of Undergraduate Applications&amp;quot;)) %&amp;gt;% 
  ggplot(aes(x = reorder_within(state, -estimate, term),
             y = estimate,
             ymin = estimate - (2 * std.error),
             ymax = estimate + (2 * std.error))) +
  geom_pointrange(color = &amp;quot;grey60&amp;quot;) +
  coord_flip() +
  guides(color = FALSE) +
  facet_wrap(~term, scales = &amp;quot;free&amp;quot;, ncol = 2) +
  theme_classic() +
  scale_x_reordered() +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(
    title = str_wrap(&amp;quot;Is First-Year Retention Associated with Student-Faulty Ratio and/or Undergraduate Applications?&amp;quot;, 75),
       subtitle = &amp;quot;Limited to states with at least 50 schools&amp;quot;,
       caption = &amp;quot;Source: IPEDS Data Center&amp;quot;,
       x = NULL, y = &amp;quot;Estimate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ipeds-many-models/01-r-for-institutional-research_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Claiming that there is more friction to learning R than there is to learning menu-driven tools is like saying learning to microwave TV dinners is easier than learning to cook the same meal from scratch. Both points might be true, but they obscure the ultimate goals of each: R, like cooking, unconstrains you, giving you the freedom to create whatever fills your imagination. And whether it’s running models for 150 majors or making soup for a large dinner party, learning to code and learning to cook can make your work not only more tenable, but more enjoyable, and in the long-run, simpler.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fellow R Instructors: Beware of the Curse of Knowledge!</title>
      <link>/post/curse-of-knowledge/curse-of-knowledge/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/curse-of-knowledge/curse-of-knowledge/</guid>
      <description>


&lt;div id=&#34;the-curse-of-knowledge-in-everyday-life&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Curse of Knowledge in Everyday Life&lt;/h2&gt;
&lt;p&gt;Several years ago my friend Lauren asked me for my recipe for BBQ seitan. I love food-related conversation, so I wasted no time. “Start by sauteing some chopped onion in oil…”, and as quickly as I began, she cut me off. “Hold on,” she interjected. “What kind of oil do you use? How much? How high do you turn the heat?”&lt;/p&gt;
&lt;p&gt;Dissecting the conversation, what happened was that I implicitly made the absurd assumption that knowledge that is in my head must be in hers (i.e., “Use however much of whatever oil you’d like to at whatever heat you normally saute”). In other words, I fell victim to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Curse_of_knowledge&#34;&gt;curse of knowledge&lt;/a&gt;. I’m &lt;strong&gt;not&lt;/strong&gt; an expert cook - just ask my wife who always keeps the salt and pepper shaker within arm’s reach when I prepare a meal - but I did naively explain the recipe to Lauren as if she possessed my idiosyncratic definition of “saute”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-curse-of-knowledge-when-teaching-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Curse of Knowledge When Teaching R&lt;/h2&gt;
&lt;p&gt;Scenarios like this are universal, and most of the time, they are harmless. However, they can be frustrating when people have invested time and money to learn R from you. Even if you yourself are relatively to new to R, it is easy to take for granted all that you know and what it’s like to be a true beginner. Consider the following questions and confusion that a new R user might have when you ask them to do something as seemingly innocuous as running a line of &lt;code&gt;read_csv()&lt;/code&gt; code you’ve provided:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“I just bought a book that says to use &lt;code&gt;read.csv()&lt;/code&gt;, but you use &lt;code&gt;read_csv()&lt;/code&gt;. They are so similar they must do exactly the same thing, right?”&lt;/li&gt;
&lt;li&gt;“Excuse me, but are you saying &lt;em&gt;tibble&lt;/em&gt;? Do you mean &lt;em&gt;table&lt;/em&gt;?”&lt;/li&gt;
&lt;li&gt;“I tried running &lt;code&gt;read_csv()&lt;/code&gt; but I got an error saying the function couldn’t be found. How does &lt;em&gt;that&lt;/em&gt; make sense?”&lt;/li&gt;
&lt;li&gt;“I thought you said colons aren’t allowed in function names, so why did you write &lt;code&gt;readr::read_csv()&lt;/code&gt;?”&lt;/li&gt;
&lt;li&gt;“The code you shared says &lt;code&gt;read_csv(&amp;quot;raw-data/survey-results.csv&amp;quot;)&lt;/code&gt; but I changed the &lt;code&gt;/&lt;/code&gt; to &lt;code&gt;\&lt;/code&gt; because that’s what the folders look like on my computer and now it doesn’t work. WTF, right?!”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not all of your students will tell you when they’re stuck, and because you can’t read their minds, what are you to do? &lt;strong&gt;Ask them!&lt;/strong&gt; Whether it’s a one day workshop or a semester long course, giving frequent, brief, assessments will help you identify areas of confusion and guide your lessons.&lt;/p&gt;
&lt;p&gt;You might be thinking that when you have a large group of people, resolving every question that every student has is unrealistic. That may be true, but it is a shame when a student falls behind because an instructor misses an opportunity for a simple clarification. Consider the following (intentionally confusing) passage from Bransford and Johnson (1972):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The procedure is actually quite simple. First you arrange things into different groups… Of course, one pile may be sufficient depending on how much there is to do. If you have to go somewhere else due to lack of facilities that is the next step, otherwise you are pretty well set. It is important not to overdo any particular endeavor. That is, it is better to do too few things at once than too many. In the short run this may not seem important, but complications from doing too many can easily arise. A mistake can be expensive as well… At first the whole procedure will seem complicated. Soon, however, it will become just another facet of life. It is difficult to foresee any end to the necessity for this task in the immediate future, but then one never can tell. After the procedure is completed one arranges the materials into different groups again. Then they can be put into their appropriate places. Eventually they will be used once more and the whole cycle will have to be repeated. However, that is part of life. (Bransford and Johnson 1972 p. 722)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you’ve never seen this passage before, it probably makes little sense to you and its details are unlikely to stick in your mind. But if before you read it I gave you the passage’s title - &lt;em&gt;Washing Clothes&lt;/em&gt; - it would suddenly become much clearer. There are plenty of “washing clothes” examples in R, and as the instructor, it’s your job to construct an environment that helps you identify them.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Frequent assessments will alert you when you succumb to the curse of knowledge and help you to correct your biases. If you’re teaching R - or for that matter, anything - and you’re not regularly checking in on what your students know and what their misconceptions are, it’s worth asking yourself what your goals are, because maximizing student understanding may not be one of them.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to the Tidyverse</title>
      <link>/talk/colorado-ed/</link>
      <pubDate>Tue, 14 Jan 2020 11:00:00 +0000</pubDate>
      <guid>/talk/colorado-ed/</guid>
      <description>


&lt;p&gt;Part 1 covers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduction to R.&lt;/li&gt;
&lt;li&gt;Visualization with &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Tranformation with &lt;code&gt;dplyr&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Part 2 covers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Modeling with &lt;code&gt;modelr&lt;/code&gt; and &lt;code&gt;broom&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Reporting with &lt;code&gt;rmarkdown&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ggplot2: Going Beyond the Defaults</title>
      <link>/post/ggplotredo1/geographic-growth-midwest/</link>
      <pubDate>Sun, 05 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/ggplotredo1/geographic-growth-midwest/</guid>
      <description>


&lt;p&gt;With ggplot2 - the ubiquitous tool for making plots in R - you can create beautiful data visualizations without doing much to the defaults. By applying the template below (see &lt;a href=&#34;https://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt;), adding a theme (e.g., &lt;code&gt;theme_light()&lt;/code&gt;), and giving your chart custom labels, you can have a publication-ready visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = &amp;lt;DATA&amp;gt;) + 
  &amp;lt;GEOM_FUNCTION&amp;gt;(
     mapping = aes(&amp;lt;MAPPINGS&amp;gt;),
     stat = &amp;lt;STAT&amp;gt;, 
     position = &amp;lt;POSITION&amp;gt;
  ) +
  &amp;lt;COORDINATE_FUNCTION&amp;gt; +
  &amp;lt;FACET_FUNCTION&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But the more I pay attention to how people respond to visualizations, the more I realize how minor improvements can make a major difference. Like cooking, where adding a little bit of salt or giving a dish a few extra minutes in the oven can transform a meal from acceptable to outstanding, relatively small changes to a chart can do the same.&lt;/p&gt;
&lt;p&gt;Take the example below, in which one of &lt;a href=&#34;http://stephanieevergreen.com/before-and-after-business-slides/&#34;&gt;Stephanie Evergreen’s&lt;/a&gt; clients started with the slide on the top, and she helped them create the one below it:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/evergreen-before.png&#34; width=&#34;400&#34; /&gt;&lt;img src=&#34;/img/evergreen-after.png&#34; width=&#34;400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both charts contain the same information, and even if you can’t express why, you just &lt;em&gt;know&lt;/em&gt; the one on the bottom is better. Continuing with the cooking analogy, the charts are like two Mexican restaurants that use the same ingredients, but at one, the guacamole is fresher, the rice is more flavorful, and the tortillas are made in house.&lt;/p&gt;
&lt;p&gt;ggplot2 was designed so users can &lt;a href=&#34;https://r4ds.had.co.nz/data-visualisation.html&#34;&gt;build any plot that they can imagine&lt;/a&gt;, so as attractive as its defaults are, my goal with this series of posts is to venture beyond the minor design adjustments I typically make and learn to tweak charts from ordinary to outstanding. My first goal was to use ggplot2 to reproduce the chart that Dr. Evergreen created for her client, which based on her post, I think she did in Excel.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(scales)
library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;step-0-create-the-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 0: Create the Dataset&lt;/h2&gt;
&lt;p&gt;In addition to creating the data set, I used &lt;code&gt;fct_reorder()&lt;/code&gt; so the bars will appear in order from highest to lowest growth in the chart.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;growth &amp;lt;- tribble(
  ~region,               ~growth, ~group,
  &amp;quot;Large city, Midwest&amp;quot;,     .2,   &amp;quot;Midwest&amp;quot;,
  &amp;quot;Large city, East Coast&amp;quot;, .175,   &amp;quot;Other&amp;quot;,
  &amp;quot;Medium city, South&amp;quot;,      .165,  &amp;quot;Other&amp;quot;,
  &amp;quot;Medium city, Midwest&amp;quot;,    .165,  &amp;quot;Midwest&amp;quot;,
  &amp;quot;Small city, Midwest&amp;quot;,     .14,   &amp;quot;Midwest&amp;quot;, 
  &amp;quot;Large city, West coast&amp;quot;,  .10,   &amp;quot;Other&amp;quot;
) %&amp;gt;% 
  mutate(region = fct_reorder(region, growth)) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-make-the-foundational-chart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Make the Foundational Chart&lt;/h2&gt;
&lt;p&gt;This chart contains all of the information that the final chart contains, but it’s like the decent restaurant you’ll never return to: fine, but unmemorable. In this step, I also narrowed the width of the bars, a subtle alteration that improves the feel of the chart as it gets closer to the finished product.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p &amp;lt;- growth %&amp;gt;% 
  ggplot(aes(x = region, y = growth, fill = group)) + 
  geom_col(width = .7) + 
  coord_flip() + 
  labs(x = NULL, y = NULL, 
       title = &amp;quot;Geographic growth dominated by Midwest.&amp;quot;,
       subtitle = &amp;quot;5 year growth&amp;quot;,
       caption = &amp;quot;Source: Our Smart Source 2015&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ggplotredo1/geographic-growth-midwest_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-update-the-colors-and-remove-the-legend&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Update the Colors and Remove the Legend&lt;/h2&gt;
&lt;p&gt;Even if you like the default ggplot2 colors - which I very much do - putting a dark color next to a muted color does a better job of highlighting a conclusion you might want to draw attention to. And by juxtaposing the green and gray, in combination with the chart’s title, the legend becomes extraneous and can be removed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p &amp;lt;- p +
  scale_fill_manual(values = c(&amp;quot;#4D643D&amp;quot;, &amp;quot;#D7DBDD&amp;quot;)) +
  guides(fill = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ggplotredo1/geographic-growth-midwest_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-white-background-and-minimal-gridlines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: White Background and Minimal Gridlines&lt;/h2&gt;
&lt;p&gt;The white background can be achieved using &lt;code&gt;theme_minimal()&lt;/code&gt;, and removing all of the grid lines other than the major-x ones cleans up the look of the chart.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p &amp;lt;- p +
  theme_minimal()  +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank()))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ggplotredo1/geographic-growth-midwest_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-change-font-and-convert-axis-to&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4: Change Font and Convert Axis to %&lt;/h2&gt;
&lt;p&gt;Taken together, using a lighter color for the axis text and bolding the labels, make the chart look more professional. It’s so easy to rely on ggplot2’s default font choices that it’s also easy to forget that changing them can give your chart a completely different look. In this step, I also expressed the axis as percents, which is more consistent with how people think about growth (and happens to look better).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p &amp;lt;- p +
  theme(plot.title = element_text(face = &amp;quot;bold&amp;quot;),
        plot.subtitle = element_text(face = &amp;quot;bold&amp;quot;),
        plot.caption = element_text(color = &amp;quot;gray53&amp;quot;, hjust = -.15,
                                    face = &amp;quot;bold&amp;quot;),
        axis.text.y = element_text(color = &amp;quot;gray30&amp;quot;, face = &amp;quot;bold&amp;quot;),
        axis.text.x = element_text(color = &amp;quot;gray30&amp;quot;, face = &amp;quot;bold&amp;quot;)) +
  scale_y_continuous(label = percent_format(), limits = c(0, .25))) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ggplotredo1/geographic-growth-midwest_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-add-an-icon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5: Add an Icon&lt;/h2&gt;
&lt;p&gt;The logo - which I added using the cowplot package - is functionally unnecessary but aesthetically powerful. It is the cilantro garnish on top of your beans and rice: Doesn’t add much flavor, but makes you all the more eager to dig in!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggdraw() +
  draw_image(&amp;quot;us-map.png&amp;quot;,
             y = .85,
             x = 0.1,
             width = .15,
             height = .15
           ) +
  draw_plot(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ggplotredo1/geographic-growth-midwest_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;ggplot2 has become so popular that I rarely, if ever, search online for a question about it that hasn’t already been asked and answered. This task was no different, with each question I had (e.g., how do I add an icon?) quickly resolved with a &lt;a href=&#34;https://stackoverflow.com/questions/9917049/inserting-an-image-to-ggplot2&#34;&gt;StackOverflow solution&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Small adjustments make a big difference, and the beauty of ggplot2 is that those adjustments are not only possible, but the knowledge to accomplish them is accessible (&lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt;, &lt;a href=&#34;https://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt;, etc.). Stay tuned for more tricks on how you can make your ggplots more interpretable, compelling, and visually satisfying!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Brief History of NBA Three-Point Attempts: 1998 to 2018 Seasons</title>
      <link>/post/history-3s/history-of-basketball/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/post/history-3s/history-of-basketball/</guid>
      <description>


&lt;p&gt;On October 29, 2018, in a game against the Chicago Bulls, Klay Thompson attempted 24 three-pointers, making 14 of them; both marks set NBA records. A couple months after that his teammate Stephen Curry attempted 14 three pointers in a single &lt;em&gt;half&lt;/em&gt; of a single NBA game, which tied an NBA record. That same game, the Golden State Warriors and Sacramento Kings made a combined 41 threes, a total that had never been reached before. Just this morning when I checked ESPN.com I learned that the previous night James Harden scored 61 points in Madison Square Garden, but maybe more shocking, he did it while attempting 20 three-point shots (he only made five of them).&lt;/p&gt;
&lt;p&gt;For anyone who has been paying attention to the NBA for more than a few years, these numbers look like typos. The NBA began using the three-point line in 1979, and on October 12 of that year - almost 40 years ago - &lt;a href=&#34;https://en.wikipedia.org/wiki/Three-point_field_goal#/media/File:Houston_Rockets_at_Boston_Celtics_1979-10-12_(Official_Scorer%27s_Report-Original)_(Chris_Ford_crop).jpg&#34;&gt;Chris Ford made the first one&lt;/a&gt;. For most of its history the three-point shot has been like that kitchen gadget you’ve always had but rarely used until someone showed you what a powerful tool it could be (immersion blender maybe?).&lt;/p&gt;
&lt;p&gt;In the case of the NBA, that &lt;em&gt;someone&lt;/em&gt; would be Houston Rockets General Manager Daryl Morey, who wisely drew attention to math and the reality that three is worth more than two. More specifically, his insight was that the shots most worth taking are higher percentage attempts near the basket (layups and dunks, ideally), or lower percentage ones that are further from the basket but are worth three points (free throws are also an important part of the equation in so-called Morey-Ball). This means that if a team’s goal is to get the most points out of every position (and that is every team’s goal), mid-range shots should be discouraged: They are harder to make than layups and dunks and still worth only two points.&lt;/p&gt;
&lt;p&gt;This has had a profound effect on how games are played and which players are valued. It wasn’t long ago that big men were expected to be able to post up near the basket and wouldn’t have to think about life outside the three-point arc. In 2019, a few games past the midpoint of the current season, Brook Lopez has already attempted 304 threes, Karl-Anthony Towns attempted 217, Marc Gasol attempted 193, Joel Embid attempted 178. Each of these players is listed as a center and is at least 7’0 feet tall. And the list goes on.&lt;/p&gt;
&lt;p&gt;And it’s not just who is shooting threes, it’s how they’re being shot. James Harden is setting new records for the number of &lt;a href=&#34;https://www.theringer.com/nba/2019/1/23/18193249/james-harden-scoring-history-houston-rockets&#34;&gt;unassisted threes made&lt;/a&gt;, meaning he doesn’t just shoot from deep as a compromise when better shots are not available, but it’s as if any shot that is &lt;em&gt;not&lt;/em&gt; a three is a compromise. Or consider Curry, for whom the three-point line itself is merely a suggestion; he has made &lt;a href=&#34;http://www.espn.com/nba/story/_/id/25771897/steph-curry-unleashing-impossible-range&#34;&gt;45 of 94 threes (47.9%!) launched from between 30 and 35 feet since 2014-2015&lt;/a&gt; (the line is 23 ft 9 inches from the basket, and 22 feet if you are standing in the corner).&lt;/p&gt;
&lt;p&gt;In 2019, you don’t have to watch games closely to notice the abundance of three pointers that are being shot. But still, I wanted to attach some numbers to the obvious, so below, I present a brief quantitative history of NBA three-point attempts.&lt;/p&gt;
&lt;div id=&#34;percentage-of-all-shot-attempts-that-were-threes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Percentage of All Shot Attempts That Were Threes&lt;/h1&gt;
&lt;p&gt;It’s easy to forget how recent the explosion in three-point attempts has been. The chart below, which displays the percentage of all shot attempts that were threes, shows a gradual increase from 1998 to 2007. But from 2008 until 2012, the numbers leveled off, and it seemed peak-three had been reached. Then 2014 to 2018 happened, and the increase during that time period alone (8%) was greater than it was from all of 1998 to 2012 (7%)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/history-3s/history-of-basketball_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although it’s a bit of an aside, despite the increase in the number of attempts, the percentage of three-pointers that have been made has remained relatively steady, as displayed below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/history-3s/history-of-basketball_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-of-threes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distribution of Threes&lt;/h1&gt;
&lt;p&gt;As I mentioned above, three-point shots are no longer limited to certain positions, a point that is reflected in the histogram below. It shows players’ average number of three-point attempts per game along the x-axis, and the number of players falling within that range for the season is represented along the y-axis. Over time, notice how the distribution gets flatter. In 1998, the group who shot between zero and 0.5 threes per game appeared with the most frequency. But by 2018 that bar dropped significantly and the other bars lifted up. And pay attention to right side of the chart beginning in 2016.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/history-3s/history-of-basketball_files/figure-html/unnamed-chunk-4-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-attempters-by-year&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top Attempters by Year&lt;/h1&gt;
&lt;p&gt;Who are these gunners that are changing how basketball is played? The chart below displays the three players who averaged the most threes per game for each season from 1998 to 2018. Even the casual fan will not be surprised by the more recent appearances on the chart: Splash-Brothers Curry and Thompson; and Gordon and Harden, whose GM is the aforementioned Daryl Morey.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/history-3s/history-of-basketball_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;curry-explains-it-all&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Curry Explains it All&lt;/h1&gt;
&lt;p&gt;Daryl Morey’s influence on the game and the state of the evolution of the three-point shot is encapsulated by the shot charts below, which I downloaded using &lt;a href=&#34;https://github.com/toddwschneider/ballr&#34;&gt;Todd Schneider’s ballr package&lt;/a&gt;. The chart on the left is Stephen Curry’s shot chart from his 2009-2010 rookie season, and the shot chart on the right is from his 2017-2018 season. It’s almost as if during that time the in-between game disappeared, leaving only three point shots and shots at the basket. That, folks, is modern basketball.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/stephen-curry-2018-19-shot-chart-heat-map.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The new style of play is not for everyone. Like any trend, this one can’t go on forever, but I do wonder how far it can go until it reaches a breaking point. Maybe the breaking point comes from the league if they decide to move the line &lt;a href=&#34;http://www.espn.com/blog/statsinfo/post/_/id/115055/what-moving-the-3-point-line-back-would-mean-for-warriors-nba&#34;&gt;further back&lt;/a&gt; or establish a &lt;a href=&#34;https://slate.com/culture/2016/06/the-4-point-line-could-be-coming-to-the-nba-heres-where-to-put-it.html&#34;&gt;four-point line&lt;/a&gt;. Or maybe it comes from the next Daryl Morey who discovers an inefficiency in the way the game is currently played. Either way, enjoy the show, because there is nothing quite like watching Stephen Curry casually make game winning shots from close to &lt;a href=&#34;https://www.youtube.com/watch?v=9dheZHuPSAo&#34;&gt;half court&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About / Contact</title>
      <link>/contact/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talks &amp; Workshops</title>
      <link>/list/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/list/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Analysis of Gender Disparity Among Higher Education Chief Administrators</title>
      <link>/post/gender-college-presidents/gender-college-presidents/</link>
      <pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate>
      <guid>/post/gender-college-presidents/gender-college-presidents/</guid>
      <description>


&lt;p&gt;The chief administrator job of a higher education institution is, as the title implies, the pinnacle of careers in academic administration. The chief administrator is a school’s spokesperson and guides its vision, affecting the lives of the thousands of students who pass through those institutions. And for the chief administrators who don’t care about the idealism of educating future generations, I would imagine the &lt;a href=&#34;https://www.chronicle.com/interactives/executive-compensation#id=table_public_2017&#34;&gt;high-six/low-seven figure salaries&lt;/a&gt; that many earn is incentive enough. Despite the prominent roles that these administrators fill, there is a dearth of publicly available data on them.&lt;/p&gt;
&lt;p&gt;Every institution that participates in federal student financial aid is required to submit data to Integrated Postsecondary Education System, or IPEDS. Eventually, the submitted data is made publicly available in the &lt;a href=&#34;https://nces.ed.gov/ipeds/use-the-data&#34;&gt;IPEDS Data Center&lt;/a&gt;. This includes numbers on admissions, student enrollment, degree completions, graduation rates, financial aid, finances, human resources, and libraries. Much of this data has to be reported by gender and race/ethnicity. For example, how many Hispanic female undergraduates began at an institution last fall? What is the graduation rate of male American Indian Alaska Natives? What is the average salary of female instructional staff on a 9-month contract? But for one reason or another, as far as I can tell, the only things that must be reported about an institution’s chief administrator are her or his name and title.&lt;/p&gt;
&lt;p&gt;Names, though, are not completely devoid of meaningful information. If you live in the United States and hear the name Steven, you probably think of a male, and if you hear the name Mary, you probably think of a female. Yes, some names are more ambiguous than others (my own being a good example), and some people’s names might belie the gender they identify with, but there is a degree of reliability that a person’s name offers in determining whether they are female or male. Thus, using first names to make educated guesses about chief administrators’ gender, my goal here was to describe gender representation among this set of individuals.&lt;/p&gt;
&lt;p&gt;The first step was to download the names of the chief administrators for every institution in the IPEDS Data Center (n = 7108) and clean up the data. This required putting years in a consistent format; removing titles preceding first names (e.g., Dr., Ms., Mrs.); extracting first names into their own column; and adding variable labels for college sector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(babynames)
library(scales)

admin &amp;lt;- read_csv(&amp;quot;data/chief-admin-names.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# tidy year 
admin &amp;lt;- admin %&amp;gt;% 
  gather(&amp;quot;year&amp;quot;, &amp;quot;name&amp;quot;, `Name of chief administrator (HD2016)`,
         `Name of chief administrator (HD2015)`:`Name of Chief Administrator (IC90HD)`) 

# extract year from names and put in consistent format
admin &amp;lt;- admin %&amp;gt;% 
  mutate(year =  parse_number(year),
         year = ifelse(year &amp;gt;= 9596 &amp;amp; year &amp;lt;= 9798,
                       str_sub(start = 1, end = 2, year), year),
         year = ifelse(nchar(year) == 2, 
                       paste0(&amp;quot;19&amp;quot;, year), 
                       year),
         year = as.integer(year))

# select and rename variables
admin &amp;lt;- admin %&amp;gt;% 
  select(unit_id = UnitID, 
         sector_code = `Sector of institution (HD2016)`,
         institution_name = `Institution Name`,
         year, name,
         undergrad_enroll_2016 = `Grand total (EF2016  All students  Undergraduate total)`,
         grad_enroll_2016 = `Grand total (EF2016  All students  Graduate and First professional)`)

# titles are in the first position for many names, so need to remove
# those so can extract first posistion from names and have it reflect
# first name
admin &amp;lt;- admin %&amp;gt;% 
  mutate(name = tolower(name),
         name = gsub(&amp;quot;\\.&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;dr\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;mr\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;ms\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;mrs\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;rev\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;reverend\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;very reverend\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;very\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;rabbi\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;msgr\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;dra\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = gsub(&amp;quot;\\&amp;lt;sr\\&amp;gt;&amp;quot;, &amp;quot;&amp;quot;, name),
         name = str_trim(name, side = &amp;quot;both&amp;quot;),
         full_name = tolower(name)) %&amp;gt;% 
  separate(name, into = &amp;quot;first_name&amp;quot;, sep = &amp;quot; &amp;quot;)

# add sector label
labels &amp;lt;- read_csv(&amp;quot;data/sector-value-labels.csv&amp;quot;) %&amp;gt;% 
  select(sector_code = Value, 
         sector_label = ValueLabel)

admin &amp;lt;- left_join(admin, labels, by = &amp;quot;sector_code&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I assigned a gender to each chief administrator based on her or his first name. To do this, I used R’s &lt;code&gt;babynames&lt;/code&gt; package, which contains the number of babies born every year from 1880 to 2015 for each combination of name and sex&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. I took the babynames data and calculated how often each name was given to females vs. males, and then assigned gender based on the higher proportion. For example, in the babynames data, about 72% of all newborns named Jaydin were male, so I assigned the name Jaydin to male. Names like Jaydin, however, were the exception: Most of the time, names went overwhelmingly to one sex or the other, with the vast majority of names in the babynames dataset being associated with only one sex.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read in babynames data and assign a proportion to female and male for each name
baby_names &amp;lt;- babynames %&amp;gt;% 
  select(sex, name, n) %&amp;gt;% 
  mutate(name = tolower(name))

baby_names &amp;lt;- baby_names %&amp;gt;% 
  group_by(sex, name) %&amp;gt;% 
  summarise(total = sum(n)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  group_by(name) %&amp;gt;% 
  mutate(prop = total/sum(total)) %&amp;gt;%
  filter(prop == max(prop)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  select(sex, first_name = name, prop)

# one chief admin officer has name gold and it happens to be one that is .5 prop,
# so removed from dataset. (gold was given as full name, so i&amp;#39;m *guessing* this 
# is actually last name).
baby_names &amp;lt;- baby_names %&amp;gt;% 
  filter(first_name != &amp;quot;gold&amp;quot; | prop != .5)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;historical-trends&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Historical Trends&lt;/h1&gt;
&lt;p&gt;This gave me a “dictionary” containing the probabilistic sex of 97,430 first names, which I then linked to the chief administrator data&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, making it possible to examine historical trends in gender differences among chief administrators. (The babynames data is based on sex, but once I link it to adults’ names (i.e., the administrators), I make the (often wrong) assumption that names reflect gender. Also, of course, with this data, it is not possible to account for gender non-binary administrators).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# join ipeds and babynames, removing rows where there were no matches
admin &amp;lt;- left_join(admin, baby_names, by = &amp;quot;first_name&amp;quot;) %&amp;gt;% 
  mutate(institution_name = gsub(&amp;quot;-&amp;quot;, &amp;quot; &amp;quot;, institution_name)) %&amp;gt;% 
  filter(!is.na(sex))

admin &amp;lt;- admin %&amp;gt;% 
  select(unit_id, institution_name, sector_label, year, undergrad_enroll_2016, 
         grad_enroll_2016, full_name, first_name, sex)

# several variables are for 2016 only, so put those in their own data frame
admin_2016 &amp;lt;- admin %&amp;gt;% 
  filter(year == 2016) %&amp;gt;% 
  select(-year)

admin &amp;lt;- admin %&amp;gt;% 
  select(-undergrad_enroll_2016, -grad_enroll_2016)

# proportion female by year
female_prop_sex &amp;lt;- admin %&amp;gt;% 
  count(year, sex) %&amp;gt;%
  group_by(year) %&amp;gt;% 
  mutate(year_total = sum(n)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(prop_sex = n/year_total) %&amp;gt;% 
  filter(sex == &amp;quot;F&amp;quot;)
  
female_prop_sex %&amp;gt;% 
  ggplot(aes(x = factor(year), y = prop_sex, group = 1)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_y_continuous(label = percent_format()) +
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;Percent Women&amp;quot;,
       title = &amp;quot;Percentage of Chief Administrators Who Are Women&amp;quot;,
       subtitle = &amp;quot;All IPEDS Institutions, Years 1990-2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gender-college-presidents/gender-college-presidents_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are three things to notice about the chart:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The increasing percentage of chief administrators who are women&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The still minority percentage of chief administrators who are women&lt;/li&gt;
&lt;li&gt;The 35% figure I came up with for 2016 is roughly consistent with a &lt;a href=&#34;http://www.acenet.edu/news-room/Documents/Leading-the-Way-to-Parity.pdf&#34;&gt;survey reporting that 30%&lt;/a&gt; of 2016 college presidents were women), and compatible with the trends reported &lt;a href=&#34;https://infogram.com/ready-to-lead-women-in-the-presidency-1h8n6me9o9392xo&#34;&gt;here&lt;/a&gt;, lending some support to the approach I selected.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;institution-type&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Institution Type&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;School&lt;/em&gt;, as it’s used in IPEDS, is a broad term that covers vastly different types of institutions. It includes everything from a cosmetology school that enrolls a handful of students each year to flagship schools with billion-plus endowments and tens of thousands of students. That is to say, chief administrator positions vary in prestige, responsibility, salary, and a host of other intangibles. With that in mind, I calculated the percentage of female chief administrators, by sector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# proportion female by sector
female_by_sector &amp;lt;- admin_2016 %&amp;gt;% 
  count(sector_label, sex) %&amp;gt;% 
  group_by(sector_label) %&amp;gt;% 
  mutate(prop_sex = n/sum(n)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(sex == &amp;quot;F&amp;quot;)

female_by_sector %&amp;gt;%
  ggplot(aes(x = reorder(sector_label, prop_sex), y = prop_sex)) +
  geom_col() +
  coord_flip() +
  scale_y_continuous(label = percent_format()) +
  theme_minimal() +
  labs(x = NULL, y = &amp;quot;Percent Women&amp;quot;,
       title = &amp;quot;Percentage of Chief Aministrators Who Are Women,\nby Sector&amp;quot;,
       subtitle = &amp;quot;All IPEDS Institutions, Year 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gender-college-presidents/gender-college-presidents_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although they are a minority in every sector, women make up a higher percentage of chief administrators at 2-year schools than at 4-year schools. One institution type is not better than another, but they serve different functions (e.g., teaching vs. research), meaning the disparities by sector further exaggerate the existing imbalance. For example, in 2016, 35% of academic chief administrators were women, yet the institutions they led accounted for only 29% of all students enrolled at institutions of higher education.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The overall trend is moving in the right direction, but change is slow: From 1990 to 2016, the average yearly increase in the percentage of women chief administrators was 1%. These are prestigious jobs that aren’t vacated haphazardly, so for the near-term, the disparity is here to stay: If the same rate of change observed from 1990 to 2016 continues, it will take 23 years until gender parity among chief administrators is achieved.&lt;/p&gt;
&lt;p&gt;This is of course not a problem unique to higher education, but a societal one that begins well before women submit job applications. We must be aware of our biases - stop always telling your niece that she is pretty and her brother that he is smart! - however subtle they are, and correct them. You don’t have to be a woman to care about this, after all, “&lt;a href=&#34;https://www.pbs.org/weta/washingtonweek/web-video/hillary-clinton-declares-womens-rights-are-human-rights&#34;&gt;human rights are women’s rights&lt;/a&gt;”.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The data is restricted to combinations of five for more.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I removed rows where there were no matches between first names in the IPEDS data and first names in the babynames data.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;You may have noticed a slight dip from 2006 to 2009. Considering how abrupt it is, I’m skeptical it represents a real trend. My guess is that it is instead reflective of an unrelated change in the underlying data (e.g., different reporting requirements), but I’m not entirely sure.&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Popularity of Various Data Analytic Tools at AIR Forums</title>
      <link>/post/air-forum-text-software-frequency/air-forum-text-software-frequency/</link>
      <pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate>
      <guid>/post/air-forum-text-software-frequency/air-forum-text-software-frequency/</guid>
      <description>


&lt;p&gt;Data: It’s become a cliche to say that it’s everywhere and in quantities that are unimaginable. But data in its raw form, whether in a structured database or on the internet, is of limited use until a human does something to it: Gather it, clean it, visualize it, model it, write about it, and so-on.&lt;/p&gt;
&lt;p&gt;The amount of data that those in institutional research encounter requires powerful tools to work with. And those tools exist. Lots of them. Everything from free, open-source software, to software costing hundreds of thousands dollars offered by companies that won’t stop emailing you despite unsubscribing from their list on a weekly basis.&lt;/p&gt;
&lt;p&gt;On the rare occasion someone asks me which software I’d recommend, I always say R. In my experience, its ability to do everything you’d want and need to do as an institutional researcher is unmatched (cut to five minutes later when that someone regrets having asked me and is looking for ways to exit the conversation). But rather than use this space to drone on about why I think R is amazing, my goal here is to reveal the software preferences of others in the field. (And obviously, I used R to do this!)&lt;/p&gt;
&lt;p&gt;It’s not a perfect approach, but my thinking was I could see how often different tools were mentioned in AIR Forum program books and how that’s changed over time. My first step was to download the program books from the forum website going back six years. Next, I created a function to read each of the programs books into R and &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidy&lt;/a&gt; them so every individual word within each book is contained on its own line in a single data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages
library(pdftools)
library(tidyverse)
library(tidytext)
library(scales)

# function to read in PDFs and get one word per line.
text_prepr &amp;lt;- function(doc, forum_year){
  
  reg &amp;lt;- &amp;quot;([^A-Za-z\\d#@&amp;#39;]|&amp;#39;(?![A-Za-z\\d#@]))&amp;quot;
  
  df &amp;lt;- pdf_text(doc)
  
  df &amp;lt;- data.frame(df)

  df &amp;lt;- df %&amp;gt;%
    rename(text = df) %&amp;gt;%
    unnest_tokens(word, text,  token = &amp;quot;regex&amp;quot;, pattern = reg) %&amp;gt;%
    mutate(year = forum_year)
  
  return(df)
}

# apply function to programs books
all_years &amp;lt;- bind_rows(
  text_prepr(&amp;quot;data/AIR-2018-Forum-Program-Book.pdf&amp;quot;, 2018),
  text_prepr(&amp;quot;data/2017-AIR-Forum-Program-Book.pdf&amp;quot;, 2017),
  text_prepr(&amp;quot;data/2016_AIR-Forum_Program-Book.pdf&amp;quot;, 2016),
  text_prepr(&amp;quot;data/2015-Forum-Program-Book-Web.pdf&amp;quot;, 2015),
  text_prepr(&amp;quot;data/2014ForumProgramBookFinal.pdf&amp;quot;, 2014),
  text_prepr(&amp;quot;data/2013finalprogram.pdf&amp;quot;, 2013)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This resulted in a data frame with a total 447582 rows (one row for each word), a glimpse of which is printed below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(all_years)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            word year
## 1   association 2018
## 2           for 2018
## 3 institutional 2018
## 4      research 2018
## 5          2018 2018
## 6           may 2018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I decided on which words I would ask R to look for. Somewhat arbitrarily and somewhat based on my experience at AIR Forums, I chose the following: &lt;em&gt;Excel&lt;/em&gt;, &lt;em&gt;R&lt;/em&gt;, &lt;em&gt;SAS&lt;/em&gt;, &lt;em&gt;SPSS&lt;/em&gt;, and &lt;em&gt;Tableau&lt;/em&gt;. The code below (1) searches for mentions of those tools in the list of words created above (2) counts the results by year (3) builds a chart of the results. To clarify, the resulting chart displays the number of times each software is mentioned in each of the AIR Forum program books for each of the respective years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# filter for keywords and count by word and year
software &amp;lt;- all_years %&amp;gt;% 
  mutate(word = tolower(word)) %&amp;gt;% 
  filter(word %in% c(&amp;quot;excel&amp;quot;, &amp;quot;r&amp;quot;, &amp;quot;sas&amp;quot;, &amp;quot;spss&amp;quot;, &amp;quot;tableau&amp;quot;)) %&amp;gt;% 
  count(word, year) %&amp;gt;% 
  complete(word, year, fill = list(n = 0))

# make data frame of only 2018, so can include as labels at end of lines
software_2018 &amp;lt;- software %&amp;gt;% 
  filter(year == 2018)

# create chart
ggplot() +
  geom_line(data = software, aes(x = year, y = n, color = word, group = word), size = 2) +
  geom_text(data = software_2018, aes(x = year, y = n, label = word), nudge_y = 2) +
  geom_point(data = software, aes(x = year, y = n, color = word), size = 3) +
  theme_minimal() +
  labs(x = &amp;quot;Forum Year&amp;quot;, y = &amp;quot;# of Mentions&amp;quot;,
       title = &amp;quot;Number of Times Software is Mentioned in AIR Forum Program Book&amp;quot;,
       subtitle = &amp;quot;2013 to 2018 Forums&amp;quot;) +
  guides(color = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/air-forum-text-software-frequency/air-forum-text-software-frequency_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What stands out - and confirms what I’ve noticed at AIR Forums - is the rapid rise of Tableau. In 2014 I had never heard of it. In 2018, not knowing at least something about it seems unavoidable. Tableau is known for visual analytics, so it’s not surprising that its rise in populairty has coincided with an increasing interest in data visualization at AIR Forums.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# filter for keyword and count by word and year
visualize &amp;lt;- all_years %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  mutate(total = n()) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(visualiz = str_detect(word, &amp;quot;visualiz&amp;quot;)) %&amp;gt;%
  group_by(year) %&amp;gt;% 
  summarise(prop = mean(visualiz)) %&amp;gt;% 
  ungroup()


# create chart
ggplot(visualize, aes(x = year, y = prop)) +
  geom_line(size = 2) +
  geom_point(size = 3) +
  theme_minimal() +
  labs(x = &amp;quot;Forum Year&amp;quot;, y = &amp;quot;% of Mentions&amp;quot;,
       title = &amp;#39;Percent of All Words That Were &amp;quot;Visualiz*&amp;quot; in AIR Forum Program Book&amp;#39;,
       subtitle = &amp;quot;2013 to 2018 Forums&amp;quot;) +
  guides(color = FALSE) +
  scale_y_continuous(label = percent_format())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/air-forum-text-software-frequency/air-forum-text-software-frequency_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Returning to the first chart, perhaps it’s my background in psychology - a field which has historically been dominated by SPSS - but I was surprised how little SPSS is mentioned (although, I wouldn’t be surprised to see it decrease in the future). As for R, it seems to have a presence, but I’m curious to see what happens to its popularity in institutional research over the next few years. Judging by my experience at recent Forums and the development of R tools that decrease the barrier to entry, my prediction is that interest will only grow.&lt;/p&gt;
&lt;p&gt;I’m not sure if this need be said, but I will: This approach to evaluating software popularity is fraught with limitations. My goal, however, was not to get a precise estimate. Rather, I was interested in one, getting a general sense of broad trends, and two, sharing how just a little of bit R code can do so much!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Achieving Reproducibility in IR With R</title>
      <link>/talk/2018-air/</link>
      <pubDate>Thu, 31 May 2018 10:45:00 +0000</pubDate>
      <guid>/talk/2018-air/</guid>
      <description>


&lt;p&gt;Although the concept of reproducibility is typically reserved for the sciences, the presenter will argue that by adopting its principles, IR offices would see immeasurable benefits in efficiency, accuracy, and transparency. Reproducible workflows preserve every decision made about data analyses (e.g., removing a student who withdrew) and allow users to quickly and accurately respond to requests for modifications (e.g., group tables by college instead of major). One barrier to reproducibility, however, is that it requires coding. Using examples from the free R programming language, the presenter will show that not only is R an ideal software for reproducibility, but that many of its modern features are designed to get novices quickly doing powerful things. The primary goals of the presentation are for audience members to leave convinced that they can learn R, and that if they do, they will become better at their jobs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Visualization Showcase</title>
      <link>/talk/2017-air/</link>
      <pubDate>Tue, 30 May 2017 15:30:00 +0000</pubDate>
      <guid>/talk/2017-air/</guid>
      <description>


&lt;p&gt;A multitude of tools and promising practices exist for visualizing data for informing and decision making. This session will feature a showcase of tools and techniques for utilizing data to inform and present. Presenters were all provided the same dataset and guiding questions and each used a different tool (Excel, R, Tableau, and SAS Visual Analytics) to create a visual display of the data. Each will present their work with the session moderator highlighting best practice in visualization throughout the presentation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating Data Visualizations Using R: An Introduction for Non-Programmers</title>
      <link>/talk/2016-air/</link>
      <pubDate>Fri, 01 Jul 2016 15:00:00 +0000</pubDate>
      <guid>/talk/2016-air/</guid>
      <description>


&lt;p&gt;Creating graphs is a central part of the IR workload, but it can be frustrating. If you make graphs using Excel, you might spend hours mindlessly pointing-and-clicking, which then has to be repeated if updates to the graphs are required. The presenter will begin by arguing in favor of two points: First, IR professionals would be better off using R (a free and open-source programming language), and second, even those without any programming experience can learn R well enough to reap its majesty. A demonstration will be given on how to create and modify a scatter plot using ggplot2, an R package for building highly customizable visualizations. The primary goal of the presentation is for audience members to leave feeling empowered to learn to use R to improve the quantity, quality, and reproducibility of their work&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
